AnyLogic Case Studies for Prompt Analysis and Action(s)


Total number of chunks: 60

Section 1 of 60
  Metadata   A Digital Twin for Energy Efficient and Sustainable Districts Link:  Tags: Business Processes, Asset Management  Overview Germany’s plan is to completely  switch from coal and nuclear to renewables. The target is to reduce greenhouse gas emissions by 55% in 2030 compared with 1990. Today, a huge amount of solar and wind renewables is already installed, but Germany wants to expand the use of renewables up to 2030. The regional Ministry of the Environment, Climate Protection and the Energy Sector in Baden-Wuerttemberg, The European Institute for Energy Research (EIFER), and partners worked together on improving the resource efficiency and energy transition for highly efficient and sustainable districts in Germany.  Problem The main problem was how to integrate fluctuable renewables into the system. Electricity must immediately cover the daily demand. It could be stored in batteries, but this way was quite expensive for energy needed for the whole district or city. That’s why in this project EIFER followed the approach of using the existing electricity devices in their flexibility for better adapting the demand to fluctuable renewables. This means, for example, shifting loads of electricity devices that would run when there is some sun and wind.  The cost of flexibility would be much lower than installing the additional batteries. Thus, they needed to activate as much flexibility as possible instead of using the batteries.   Decentralized energy management   Decentralized energy management is quite predictable, but currently, it consists of a lot of technologies, renewables, as well as new consumers (e.g., electric vehicles). It changed how the system was managed including generation resources, distributed storage as well as flexible loads, for example, the devices that could shift the energy use in time (e.g., home appliances).  Solution EIFER worked on a real energy demonstrator of a district containing 10 buildings which included around 25 households in which EIFER was setting up a decentralized energy management system. In parallel, they constructed a digital twin that could be compared with the real demonstrator.    Digital twin conceptual components (click to enlarge)   EIFER showed the advantages of this digital twin, a virtual representation of the real system. It accompanied the project through various phases and enriched it throughout its life cycle. The digital twin also served as a data repository for static and dynamic information, such as for different operating scenarios.    Virtual demonstrator   The energy demonstrator was a highly detailed  agent-based simulation model  that mapped and connected the individual plant components of generation, storage, and demand for the electricity and heat sectors.  The Allensbach property was chosen to illustrate the virtual demonstration through a multimethod simulation model. This model replaced 140 real devices for testing. 1-second resolution enabled real-time and hardware-in the-loop testing of the energy management system. It represented thermal and electrical flows at different levels (appliance, household, building, and property) and their interactions.    Agent-based model of a household (click to enlarge)   This example of a household included different parts that were modeled as individual agents. There were electrical flows in yellow and thermal flows in red. There were the controllers that allowed demand flexibilities for shifting the consumption of the heat pump to the proper times with the autonomous algorithm. And this was connected with a grid state indicator that came from the grid connection point. The agents received the information from the grid state indicator.    Platform architecture (click to enlarge)   AnyLogic was the simulation core. The inputs were stored in Excel files and the AnyLogic database. EIFER used  AnyLogic Cloud  for visualization. Outputs could also be exported to Excel to enable non-modelers to analyze the data.  EIFER used a multimethod approach including discrete event modeling and system dynamics due to the complexity of the system. In addition, they used data-driven models. AnyLogic allowed connectivity with different devices. Finally, AnyLogic Cloud was used for experiments and evaluation.  Results This system showed an increase in the self-consumption rate from 55% to 75%. At the same time there was a reduction of the power peak. If we include EVs, there is a huge reduction in this peak because people don’t load them all at the same time and shift loading over time.    Results of 1-year simulations      The increase in the self-consumption rate led to the reduction of the operation costs of the system. This was translated into a decrease of the electricity cost up to 5€ct/kWh to German users. As the tariff was 30€ct/kWh before the energy crises, the savings are almost 20%. Due to the energy crises, the electricity tariffs had gone up a lot, but the savings were expected to go up proportionally.    Home appliances   As for appliances, they usually work when they are switched on. If people want their dishes to be washed by morning, the operation window is a whole night. Dishwashers in various houses will not be run at the same time flattening the electricity curve overall. The simulation investigated how the proportion of locally generated and used energy can be increased by intelligently controlling the generation and consumption of electricity and heat.  In the future, EIFER plans to use AnyLogic Cloud more for such kinds of projects. For further development towards an interconnected digital twin, EIFER wants to add new functions such as a replay of real historical scenarios in the virtual world, predictive control, optimization, and learning algorithms.  The case study was presented by Enrique Kremers, of EIFER, at the AnyLogic Conference 2022.  The slides are available as a PDF.      A Pharmaceutical Company Decides on a Marketing Strategy Using Agent-Based Modeling Link:  Tags: Healthcare, Marketing  Problem           Sterling Simulation consulting company was chosen to provide an agent-based pharmaceutical marketing model for a drug manufacturer. This company owned two competing non-generic drugs on the same market. One drug was well established and tended to be the industry leader, and the other one was recently introduced.   There were several concerns about how to obtain a useful market share for the newer drug, while maintaining or increasing the market share for the company’s drugs as a whole. The company used different forms of promotion, including direct-to-consumer (DTC) marketing strategy, which usually concludes in advertising through TV, pint, and other mass and social media. The pharmaceutical market simulation model was designed to answer the question, “When should the company stop DTC marketing for the new drug in order to maximize total prescription sales?”. A sufficiently accurate pharmaceutical market model could save the pharmaceutical company tens of millions of dollars.   Solution  Pharamceutical Market Simulation  Traditionally, marketing analytics have determined different spending scenarios by using a marketing-mix model to calculate the impact of marketing. However, this approach does not provide a clear picture of how the changes in spending impact the results. Marketing-mix models can give you weights, but they cannot tell you why these weights exist. The company wanted a pharamceutical market simulation model that would be at least as good as a marketing-mix model, which meant the contractors had to find the percentage point of market share over time.           To obtain a better understanding of the mechanics behind marketing-mix models (for example, why does DTC marketing give diminishing returns v. sales rep visits?), one alternative is to use an agent-based modeling approach (ABM) to pharmaceutical market simulation. It provides the secondary benefit of removing assumptions from a pharmaceutical market simulation model, which allows for a more complete understanding.   One thing to note about ABM approaches in general, and this model in particular, is that the data requirements are different from those in marketing-mix models and, in general, higher. In this case, the pharmaceutical company had the data and was able to take full advantage of pharmaceutical market simulation using ABM approach. Agent-based models tend to be more able to deal with assumptions and provide insights, which are more profitable, in this case for pharmaceutical marketing modeling, than simple answers.   The ABM approach was selected, and, for this pharmaceutical marketing modeling project, AnyLogic software was chosen. These are a few of the reasons AnyLogic was used for the pharmaceutical marketing simulation model building:   The company was familiar with AnyLogic and had finished projects in pharmaceutical marketing simulation using this software and its capabilities before.   AnyLogic allows for the greatest flexibility in modeling frameworks which is important in pharmaceutical market simulation.   AnyLogic has the best visualization possibilities for modeling.   The model framework differed substantially from traditional marketing-mix models. Specifically, this pharmaceutical market simulation model considered the entire patient/doctor interaction in order to determine the impact of marketing expenditure. Additionally, the impacts of the new drug’s introduction to the pharmaceutical market was integrated to obtain correct market share information.           The pharmaceutical market simulation model consists of the following elements:   patients;   doctors;   sales representatives; drugs;   payers;   formulary;    The latter two elements were not described in the case because they had little behavior in the model.            The patients in the model were all diagnosed with the specific illness that the pharmaceutical market handles. The disease involved was not life threatening, so this drug category is elective. The behavior of the patient included:   Meeting with their physician every three months. Determining whether they desired a specific drug (the result showed primary DTC market impact). Awareness of different drugs based on advertising and the ability to request it from doctors. Whether they filled their prescription (depended primarily on drug’s price). Whether they stayed on a drug (on average, the first month and second/third month loss was calculated to be around 40% and 20% or less, respectively).  Pharmaceutical Modeling — Simulation Model Logic          The doctors in the pharmaceutical market simulation model had different specializations, related to the illnesses, and had differing numbers of patients, depending on specialization. The behavior of the doctors included:    Handling patient appointments.   Determining what drug to prescribe to a patient (theoretical preference was based on clinical drug performance, practical preference was based on patient reaction on drugs). Whether to provide samples or a script to new patients.   Interacting with sales reps.   You will find the preassigned model of a patient’s behavior during and after an appointment, as well as the description of a patient’s lifecycle below.   Sales representatives were assigned to a pool of doctors. They visited their doctors at a different rate, based on the patient pool of each doctor and historical information. During the visit, the representative tried to change the doctor’s attitude towards a certain drug by adding samples to the doctor’s supply.    The drugs in the pharmaceutical market simulation model were two of the company’s drugs, another specific non-generic drug, and generics as a group.   Outcome   The pharmaceutical market simulation model took a while to calibrate properly. This was because the data was sparse in some areas. The model was calibrated primarily with each drug’s (or drug family’s, in the case of the generics) market share, for both patients and prescriptions per month. Once calibrated, the pharmaceutical market simulation model showed that the ideal time to stop DTC marketing for the new drug would have been six months before the current date. This was made clear by the calibrated model. Since this was unfeasible, the given answer was to stop DTC marketing as soon as feasible.   Another interesting insight was connected with sales rep marketing. It was clarified, that over time, the doctors’ preferences would overwhelma patients’ preferences for drugs. That is why the money invested in sales rep visits, in contrast to DTC marketing, did not show any diminishing returns and always influenced market share. This was not unexpected, given that the availability of samples was directly tied to the visits, and had a broad impact on the willingness of patients to try the drug.   Concerning the budget issue, it was clearly demonstrated, that should the company follow the results of the pharmaceutical market simulation model and eliminate DTC marketing, doing so would save at least $10M a year.    To learn more about this project and pharmaceutical market modeling watch the presentation by Scott Hebert, Vice President of Sterling Simulation        A Simulation for a Meat Cooling Facility's Production in the Nordic Electricity Market Link:  Tags: Business Processes  A power grid is simply one big interconnected machine. Whenever someone turns on a light switch, someone on the other side has to increase production. This is a simple case of supply and demand. However, this balancing act is becoming more uncertain and unpredictable because of the emergence of renewables. So, it requires new solutions such as batteries which can store energy, hydrogen, and even super connected grids linking countries.  However, there is another option – demand response. This refers to changes in the use of electricity by consumers during high periods in order to decrease demand on the power grid and ensure electricity reliability. Therefore, this focuses on process rescheduling instead of new technology investments. It is also more data-driven, which means that AnyLogic simulation could be applied here effectively.  The raw material, which in this case is meat cuttings, is received by the industrial consumer and needs to be stored in a cooled room before it is processed. In the system, there is a maximum and minimum temperature threshold within which the meat can be stored. So, the temperature can be changed depending on the power grid’s needs in order to provide a stable service.  For the electricity supplier, the prices vary depending on the time of the day, for example, the early morning and evening are more expensive, while the consumer has fixed prices. If the consumer accepts flexible prices, they can choose to use more electricity. In this case it means increasing the temperature in the cooling room during the non-peak hours in the grid, and reducing the temperature in the cooling room during

 

Section 2 of 60
peak hours. This will ease the electricity consumption during peak hours.  This analysis of demand response using simulation focused on a single industrial consumer, which was a Danish company in the meat processing industry. With simulation it is possible to compare different scenarios: to vary parameters and see how the modeled system responds.  AnyLogic simulation software was the best option to simulate this demand response effect because of the following advantages it brings to an analysis:    Descriptive simulation approach in order to compare results of different operational conditions and help decision making in a multivariate program.   Discrete-event simulation as timing is crucial for market strategy.   Agent-based modeling, where agent interactions influence information access, which in turn influences market strategy.   Using simulation, the modeler can visualize the results showing the potential of the simulation software in the following ways:    Evaluate the current potential using a simulation experiment   In this experiment, a market-based explicit direct response strategy was chosen. This means participating in the market directly as a consumer without involving the electricity supplier. It is explicit because it is clear and direct without other assumptions. The consumer is then interested in the results which are broken down into two sections:   The Financial impact of demand response which includes the cumulative and daily results.  The Process impact of demand response where the cumulative and real-time results are displayed.   h4 {text-align: center;}   Financial impact of demand response - Cumulative results   Data from the simulation showing how much the consumer is saving or not. If the green line is above the blue line, the consumer is saving    h4 {text-align: center;}   Process impact of demand response - Cumulative results   The histogram on the left shows the overall supply of electricity, and is used to ensure that the consumer supplies the same amount of energy. The second histogram shows deviations in temperatures. The third diagram illustrates the share of changed hours    h4 {text-align: center;}   Process impact of demand response – Real-time results   These histograms show the power and temperature output initially and then on different loads     Evaluate different market options using a CompareRuns experiment    Here the user can compare different scenarios, which is where the power of simulation comes in. Users can run various simulations with alternative direct responses, and market configurations. Then see how their electricity bill evolves with time. It is possible to compare the relative savings contrasted to the base rate. Essentially, finding that various market combinations can give different savings.    The first results on the left show how the electricity bill evolves over time, when using different market strategies. While the results on the right show the relative savings for the different market strategies      Evaluate operational flexibility using a ParameterVariation Experiment   This example showed an increase in the maximum temperature parameter of half a degree. As a result, the consumer can bid more on the electricity market and make a certain amount of savings. There is, however, a saturation effect. This means that the more the consumer increases the temperature, the less marginal benefits they receive. From this, the industrial consumer would understand that increasing the temperature by another half a degree wouldn’t reduce costs.    The histogram on the left shows how the maximum temperature can vary, and using this information the second histogram can be modeled over different days to show the frequency distribution of savings    The results of this demand response showed that consumers could participate in the electricity market using three approaches:   Descriptive approach, where the consumer decides what parameters are acceptable to participate in the electricity market.  Consumer-centric approach, focusing on the results for the industrial consumer and not for the grid operator, yet at the same time continuing to model the logic of all the other market players.  Modular approach, because different business models are tested, shifting the roles and distributing them to different agents.   Finally, a larger aim has been identified to apply this simulation to a number of different consumers in two different contexts – the Danish and the Chinese market. Here the usefulness of the modular approach can be seen because the two markets are quite different in structure, yet the interfaces can be quickly switched from one agent to the other. These can then be implemented in many different industrial processes.    The case study was presented by Nicolas Fatras, PhD student, Center for Energy Informatics, University of Southern Denmark at the AnyLogic 2021 Conference.   The slides are available as a PDF.     A Trust Model for Real-time Health Policy Support Link:  Tags: Healthcare  COVID-19 has been a challenge for many reasons. At the beginning, it was unclear how this new virus would impact the world. In Saskatchewan, they saw frightening evidence from China and Italy about the impact it was having on individuals and the pressure it was putting on the health care system.  The  Saskatchewan Health Authority needed to plan for this virus and decided to integrate a trust model into the decision-making approach within the health policy.  Problem In a traditional complex decision-making approach, experts look at each individual part of a system, and then come together to make the best decisions possible together.  This was not the best approach for COVID-19 as it could take a long time and each expert may prioritize their part over another. The digital health analytics team needed to work together with the modeling team to overcome this problem.  Solution The Saskatchewan Health Authority needed to, in real time, integrate large amounts of data from multiple sources, standardize it, bring it together, and then make decisions on how to prepare for COVID. Everything was changing very quickly, and decisions needed to be made promptly. The Health Authority quickly understood that to successfully apply modeling to the health system decision making, 5 key components were needed:    Map of Full Process  Data  Evidence  Contextual Knowledge  Knowledge Translation   5 key components of applied health system modeling (click to enlarge)   At the beginning of 2020, as COVID-19 started to spread, the University of Saskatchewan formed three parallel lines of action to develop a trust model. These lines fed into the work that   Kreuger Consulting did for the Saskatchewan Health Authority.  In the first line, a model was generated very quickly using system dynamics to make projections for possible situations in Saskatchewan, e.g., severity, kinds of contact tracing, demands on the hospital system, and so on.  In the second line, there was an integration between system dynamics and a machine learning approach called the particle filter. Data such as daily cases, hospital admissions, etc., was fed into this particle filter system dynamics model, and it could automatically give an updated short-term projection, and as the data shifted, so did the model forecasts.  The third line of modeling was an agent-based model, which was a nation-based model. It was built initially for the medium and long-term horizon to answer questions around policies. This was a  GIS model with discrete-event simulation for service delivery, testing and contact tracing, and then there was an agent-based population represented in this GIS environment.  So, none of these models are multimethod models – there is an agent-based model and a separate system dynamics model. AnyLogic allowed those running the models to easily switch between agent and system dynamics approaches.  The primary value of the agent-based model was to be able to ask preventive questions and then communicate this. Data was assumptions that people’s behavior stayed the same into the future, but then there is an intervention, such as vaccination efficacy, and the outcome changes. The advantage of the agent-based model allowed scenario planning to check the robustness of the system to understand if they were entering a danger area or some kind of a limit.  The value of an integration between a particle filter and system dynamics is its ability to show the conditions right now and what they might be in one- or two-weeks’ time.  Results Currently, the agent-based model is being used at the provincial level, where there are 1.2 million people, or 1.2 million agents. In the past it was used at the local level, small communities where local outbreaks occurred at the beginning of the pandemic.  There is a visualization of the trust model below, although that is not as important as what the model actually does. It generates, in real time, a number of statistics, such as the number of tests per day, the number of new admissions, the number of cases in long-term care facilities, or even the number of vaccinations by doses by area. Every time the model is run, one hypothesis is generated and as it is a stochastic model, it should be run multiple times. These different runs are then sent to Kreuger servers, where they run many iterations and scenarios and replications across all those runs.    Understanding the impact at provincial or area specific level – staff cohorting in long term care homes and different school strategies   This trust model was very transparent and captured a true Saskatchewan scenario. As a result, it built trust among the stakeholders. This trust resulted in modeling becoming really significant in the planning for COVID-19. The results are presented multiple times a week showing the modeling as well as the actual results to see how well the model is tracking.   The Saskatchewan Health Authority realized that modeling was very powerful, especially if incorporated into the decision-making approach. This could then result in changes in actions and decisions.   The case study was presented by Dr. Kurt Kreuger of Kreuger Consulting, and Dr. Jenny Basran, Senior Medical Information Officer Saskatchewan Health Authority, at the AnyLogic 2021 Conference.      AI and Simulation for Container Yard Planning Link:  Tags: Ports & Terminals  Problem   Today, around 80% of global trade by volume is carried by sea and handled by intermodal ports and terminals worldwide. With the increase of global transportation, intermodal facilities are being redesigned to expand capacity and fulfill demand. At the same time, it is necessary to keep the facilities safe and efficient, which is why operating companies are looking for ways to test changes before implementation.   Terminal San Giorgio in Genoa, Italy, planned the reorganization of their facility’s layout and operations, aiming to increase its throughput and make the terminal safer. They needed a port simulator – a digital twin of the terminal, which would serve as a testbed for container yard planning and help predict how the changes would affect the current operations. As simulation is a natural technology for creating such digital copies of complex environments, the management contracted this project to container terminal simulation engineers from MEVB Consulting, a Swiss developer of simulation-based decision support systems. Using the container terminal simulation approach, the contractors would be able to mirror both physical and operational activities in an interactive environment and capture detailed dynamic workflows. For the container terminal simulation, the engineers chose AnyLogic modeling software for the following reasons:  It has a built-in modeling library for building complex, yet detailed, material handling models. The library contains ready-to-use elements to speed up and simplify the simulation of cargo handling operations. AnyLogic supports the creation of custom model interfaces, which simplifies model usage for those unfamiliar with AnyLogic. The software allows engineers to connect models with AI platforms and train algorithms using simulation data. The policies learned from training can eventually be deployed to the real system, or used in the model to let the policy make decisions autonomously. The engineers could run several scenarios at a time using AnyLogic Cloud computing capabilities and distributed simulation technology. This way, complex multi-run experiments are executed faster and more efficiently than on a regular computer.  Solution   To create the digital twin, the engineers needed to collect information on how people, containers, and vehicles moved around the terminal. For that, they equipped employees with mobile phones to track their position in the terminal continuously. This way, it was possible to see people entering and leaving certain areas, as well as their movement between them. For containers and trailers, they provided beacons that transmitted their exact locations through GPS to an in-house tracking system.  The container terminal simulation engineers used this information to build an accurate digital twin that was fed with data gathered from the environment. In total, they were able to display in the port simulator the movement of 20,000 interconnected objects, including ships and cargo trains. To speed up simulation, they used AnyLogic Material Handling Library and its ready-to-use elements.   The last step of model building was to connect the container terminal simulation model to Microsoft Project Bonsai. This AI platform allowed for easier deep reinforcement learning based on data from the simulation models. The engineers hoped that the AI brain would make decisions for the managers, or, at least, propose solutions using simulation data at different steps of cargo processing. Testing evacuation scenario   When the container terminal simulation model was ready, the engineers wanted to stress-test the current layout and operations. The engineers split the model into zones, and in some of them, initiated possible evacuation scenarios which could happen in case of fire propagation or an explosion.    To find optimal evacuation paths, they relied on the model’s AI opportunities. The algorithm analyzed all possible paths from the dangerous zone to safe ones, considering the evolution of the emergency and the ongoing processes in the port. A variety of emergency evolution scenarios were run with AnyLogic Cloud to speed up the simulation. As a result, the algorithm proposed optimal evacuation paths to safe areas and assigned them to each agent.  Then, the engineers used the output data to test the evacuation paths in real life. They simulated a dangerous situation in one of the zones and communicated the signal to the evacuation system connected with the container terminal digital twin. In response, the digital twin found the safe zones

 

Section 3 of 60
nearby and communicated their locations to the system. The system then sent a phone alert to people in the evacuation area with a personalized evacuation plan guiding them to safe areas. Area management using AI The container terminal simulation model was also used to improve terminal throughput. When trucks arrive at the parking area to collect or pass on a container, managers assign them a parking spot. Their decisions, however, are usually based on the current situation at the port and do not consider possible breakages, truck latencies, or emergency situations. This might result in inefficient usage of parking space and decreased throughput.    To improve management decisions, the engineers simulated and optimized truck boarding and disembarking operations – one of the cargo processing steps – using AI capabilities. Improvements in this area would help increase terminal throughput by minimizing processing time and making the operations more fluid.  Container terminal simulation with AnyLogic  The engineers proposed that the AI brain could orchestrate truck allocation and make simulation-based decisions autonomously. Gaining input from the digital twin of the terminal, it would rely on aggregated data and predict where to best allocate the trucks with regards to the planned shipments.  Terminal throughput depended heavily on other terminal operations, which is why the engineers expanded the current digital twin with the model of the parking area. In particular, in the model of the truck-related logistics operations, the engineers reflected:  The average time it takes to transport containers from ship to parking lot. The number of places available in the parking lot.  It took some time for the AI brain to learn the historic data from the container terminal simulation model and what policies were implemented for truck allocation. When the learning phase was over, the model was continuously fed with new data related to the shipments, and the brain decided where to allocate the incoming trucks. Results   The engineers created a decision-support system which helped them build a reliable evacuation strategy for emergency situations and improve the safety of the terminal. The system can recalculate paths to safe zones on the fly when accidents occur and communicate the paths to the in-house alert tool, which, in turn, sends direction notifications to employees in the field.   The engineers also showed that AI capabilities coupled with simulation would improve the overall terminal throughput by 20%. The learning AI algorithms can make use of aggregated historic data to improve truck allocation strategies and provide more insights for terminal throughput optimization. When the engineers looked at the decisions made by the algorithms, they were similar to those made by the managers. This proves the fact that AI-based decision making can be later implemented at the terminal and propagated across other cargo processing steps for their further improvement.   Project presentation by Roberto Revetria from MEVB at the AnyLogic Conference 2021:       ATOM: Digital Twin of Siemens Gas Turbine Fleet Operations Link:  Tags: Supply Chains, Business Processes  Overview As the deployment of Internet of Things (IoT) systems grows, the concept of something physical having a virtual avatar is increasingly important. Digital twins represent a virtual avatar of a physical system. These digital representations are built using the domain knowledge of subject matter experts as well as data collected from sensors on-board the system. The Agent-based Turbine Operations & Maintenance (ATOM) model is a digital twin simulation model developed by decisionLab Ltd and Siemens. The digital twin emulates the global maintenance repair and overhaul (MRO) operations of Siemens’ aero-derivative gas turbine division. Driven by live data already available within the supply chain, the model provides the capability to use sophisticated simulation and data-analytics methodologies to optimize the fleet operations of Siemens, enabling better data-driven decision-making to improve productivity and efficiency in customer operations and asset management. Problem Siemens produces a wide range of industrial turbines and recently acquired the Rolls-Royce energy gas turbine and compressor business. Following this, Siemens introduced a new aero-derivative gas turbine (SGT-A65) based on the acquired assets.  With the new turbine not being wholly developed in house, its production and maintenance produced multiple new challenges, including unforeseen in-service performance and support issues. The Excel-based forecasting tools used by Siemens at that time failed to perform efficiently under the new circumstances. The volume of data was too much to manage in Excel and the results were not clear enough to easily identify bottlenecks and quickly find solutions. In short, the company needed a more powerful method for resolving its gas-turbine fleet operation issues. The main needs were to:  Predict business performance and forecast KPIs of interest to inform decision-making; Evaluate investment options — run “what-if” scenarios to quickly understand where best to invest.  Siemens wanted to visualize the whole production and maintenance process, including the supply-chain logistics, which are critical to the system. With the ability to visualize the results of multiple “what-if” scenarios, in order to communicate the business case for several investment options and enable better decision-making, both inside the company and outside, with clients. Solution To meet the challenges, decisionLab and Siemens proposed a digital twin - ATOM. The ATOM digital twin exploits the emergence of digital technologies across Siemens engineering and manufacturing businesses. It uses the vast quantities of data that is available to integrate customers, supply chain, production, and maintenance in order to improve productivity and efficiency in customer operations and asset management. At its core ATOM achieves this by modelling the detailed intricacies of customer operations, maintenance facility operations, engine characteristics, and supply-chain logistics across the whole fleet and operational cycle. Representing the entire system, the digital twin provides great analytical capabilities. Users can examine any aspect of the system and run what-if scenarios to explore all the interdependencies. Such a system would easily identify bottlenecks and enable decision-making that considers the operation of the system as a whole. The development of digital twin requires a highly complex simulation environment, and developers will often use a software development approach. This requires great flexibility from the simulation software to successfully model different levels of business processes and manage varying complexity. For this reason, decisionLab chose AnyLogic as the core simulation tool. In this case, a core part of the model was made from many independent elements and using agent-based modeling it was possible to represent the necessary details. To build the model, the developers captured data relating to the following aspects of Siemens gas turbine fleet operations:  Agent interaction diagram (click to enlarge)  Customer operations (in what conditions, e.g. temperature, customers use the turbines) Maintenance facility operations (both main maintenance facilities were considered) Engine characteristics (different failure modes associated with particular engine components) Supply-chain logistics (as customers are located all over the world)  This is represented in the agent interaction diagram, which defines the complexity of the digital twin environment. In addition to the agent-based modelling approach, the digital twin incorporated a modular architecture, which allowed the system to be divided virtually into its constituent functional layers and provide a system engineering-based approach to model development. This approach allows concurrent users to interact with the model in different ways, and to use different data sets, and also enable the development team to adopt a continuous development and deployment approach without disruption – a reinforcement learning element is planned.   A modular architecture of the digital twin (click to enlarge)   In collaboration with Siemens future phases of development could include the following:  To move from an Excel database to a centralized database containing all Siemens systems and databases, for optimized data storage and processing; To deploy the model in the cloud, so multiple users can access it; To make it possible to use ATOM as a demonstration tool, for work with customers (i.e. to continue improving the visualization part); To add a reinforcement learning capability to optimize the dynamic decision-making process within the simulation environment and present an optimal policy that Siemens might adopt business investment decisions.  Outcome DecisionLab have created a sophisticated digital twin that captures all the functionality required by Siemens. The ATOM-twin simulation model, representing the entire fleet operations of Siemens aero-derivative gas turbines, enables its users to:  capture and forecast system KPIs visualize fleet and maintenance facility operations  identify bottlenecks in the system run both quick ‘what-if’ and detailed scenarios to aid investment decision-making  Although a very complex simulation model, decisionLab delivered a user-friendly and interactive system, usable across the organization. Both upper-management and analysts can use ATOM to easily meet their needs.   Digital twins are the focus of our white paper, An Introduction to Digital Twin Development. It contains further case studies that help demonstrate the development of digital twins and their benefits — download.   The ATOM-twin simulation model (click to enlarge)   Project presentation by Dr. Amrith Surendra, Senior Consultant, and Vitor Lemos, Simulation Modeling Consultant, from decisionLab     Agent-based Optimal Energy Flow of an Integrated Energy System Link:  Tags: Business Processes  Climate change is one of the most pressing issues facing the world today. Countries have various ways to tackle this, but ultimately, all should have the goal of reducing emissions in order to stabilize global warming. This can be achieved through changes in behavior and technology advancements. Problem An integrated energy system (IES) emerged as an option to better operate and optimize the current energy system. In order to achieve this, two challenges had to be overcome.  The first challenge was to combine different energy system analyses. Traditionally this was done separately, and the interaction of different subsystems wasn’t considered. The second challenge was that there was a gap in the agent-based methodology applied at the network level. Most studies using this methodology focused on the local level, such as individual buildings, or even micro-grids.    Schematic diagram of integrated-energy systems    Solution Researchers at UCL created a project with two aims. The first was to further develop previous agent-based modeling for multi-energy networks. The second was to experiment with integrated energy system simulation using real-world case study data.  AnyLogic was chosen as the simulation platform because it can be completely dedicated to agent-based modeling, has a very user-friendly interface, and can easily be integrated with a JAVA package.  The objective of this project was to simulate energy flow. Energy flow describes the states of the energy network.  In a network, energy flows from one node or bus to other nodes. The magnitude and phase angle of voltage at every node, as well as the active and reactive power flowing through the circuit, can be understood through energy flow analysis.  Energy flow analysis is particularly important for network extension planning and determining the best operation of existing systems.  In the model, the classic power system was followed, where there are three node types – slack, control, and load.  For an integrated system, the researchers needed to incorporate multiple energy vectors into such a system so that they could adopt an energy hub. These vectors were electricity, gas, and heat.  This hub is a mixed energy vector system with three features:    Multiple energy inputs and outputs   Conversion   Storage   This multi-energy network is modeled by a group or system of interconnected energy hubs.    Systematic diagram of energy hubs   The model was developed using graph theory, which means trying to find the shortest path from one place to another. In this case, the shortest path from one node to another. In addition, every node had to follow the conservation laws, meaning that every flow into the node must be balanced by the outflow.  An algorithm computed by agents in a decentralized manner was used to calculate the energy flow in the electric system. By aggregating these results, the behavior of a whole network could be calculated.  For the pipeline networks, the calculation result was mimicked using the electric flow method from before.  The model needed to import data from published articles including the type of the node or bus, network configuration, and the associated active and reactive power of each node or bus.    Energy network case study – example of data obtained from published articles    Results In this multi-energy or integrated energy system, researchers modeled electricity, natural gas, and heating of water. Using the inputs of the model and the variables from existing case studies, every active and reactive power and magnitude and phase angle of voltage could be computed.  Using just one node, as an example illustrated below, the apparent power and complex voltage values, as well as the flow potential for gas networks, and district heating networks could be observed.    Model demonstration showing results at one specific node   In the future, the researchers hope to integrate a dynamic energy consumption profile into the system. In addition, agent-based modeling has extremely promising potential in decentralized optimization, and so a future model could explore this in multi-region integrated energy systems.  The case study was presented by Ruiqiu Yao from the UCL Department of Civil, Environmental and Geomatic Engineering, at the AnyLogic 2021 Conference.  The slides are available as a PDF.     Alstom Develops a Rail Network Digital Twin for Railway Yard Design and Predictive Fleet Maintenance Link:  Tags: Rail Logistics  Overview Alstom is a global leader in the transportation sector. The company offers trains, signaling, maintenance services, as well as integrated transport systems. Among Alstom’s products are the TGV and Eurostar high-speed trains. The company has 105 sites with more than 34,000 employees worldwide. Its net income is €475 mln. SimPlan AG is the leading German simulation

 

Section 4 of 60
service provider, specializing in the automotive and logistics fields. The company’s revenue is €14,5 mln. Alstom considers innovation to be crucial in meeting the mobility challenges of the future.  Together, with SimPlan, they agreed on developing a digital decision support system for train fleet maintenance management. This work is a part of the EU OPTIMISED project, financed by the European Commission, as part of the EU H2020 program. OPTIMISED is a large European initiative aiming to develop methods and tools for highly optimized reactive planning across a variety of industrial sectors. Key to the project is simulation and digital twins, built with its help. A digital twin is a virtual replica of a physical system and its operations, as they operate in real life. This type of simulation model can be continuously updated from multiple data sources and change its state to represent its physical counterpart. Problem   Railway yard design toolkit  Alstom maintains the entire Pendolino train fleet on the busy and constrained West Coast Main Line (WCML) in the United Kingdom. With 56 trainsets to be maintained and five maintenance depots, the company has to take many aspects into account when scheduling and managing maintenance:  Daily operating requirements for the routes and timetables regarding the trainsets and capacities needed. Maintenance regimes: frequency and parameters (such as time or mileage) for trainset inspection and maintenance. Corrective maintenance in case of accident or failure. Maintenance capacity: whether a depot has enough resources for maintenance or repair.  Trains serviced earlier than needed cause unnecessary expense for the maintenance company, while late maintenance can result in failure and additional costly repair. So, a comprehensive digital tool was required to help manage maintenance effectively. Solution As there are many parameters to be considered, simulation is needed. But simple simulation with fixed data is insufficient. The reason for this is that the railway situation is very changeable, despite there being a fixed train timetable, and it is very hard to predict train locations, even a few days ahead. Using up-to-date data would enable deeper insight, and this led the developers to build system's digital twin. With daily operational updates, it became possible to represent the system accurately.  AnyLogic transport simulation and planning software enables the use of the most suitable modeling method for a simulation or even the use of several methods together. For this model, the developers chose an agent-based modeling approach, which made it possible to capture the whole railway network and operations:   The fleet Depots and stations Maintenance regimes Diagrams that define fleet schedule  AnyLogic also enabled the developers to handle data from different sources without changing format. System data on fleet, stations, depots, and their constraints are provided in Excel, while the trainset scheduling diagrams are CSV-files and assigned on a daily basis.   Railway simulation tool for rail yard management (click to enlarge)  The maintenance scheduler Alstom usually uses is based on a heuristic scheduling algorithm and the developers embedded it inside AnyLogic railway simulation software. This provides a big advantage because connecting the simulation and the scheduler directly means they can be rerun together for quicker results, whenever needed. The model has an interactive and user-friendly AnyLogic interface. The GIS functionality in AnyLogic makes it possible to display and manage GIS maps in the model. Using this functionality, the developers visualized railway fleet operations using data from OpenRailwayMap. On this map, users can see all fleet operations. Moreover, it is possible to click on any item and get comprehensive information about it. For a trainset, there are:  Statistics on its cumulative working hours. Details of preventive and corrective maintenance, both completed, due, and scheduled, and the depots involved. Total amount of time the train can be out of work for maintenance due to schedule.  Being Java-based, AnyLogic also allowed the developers to create custom Java extensions and a freely distributable standalone application for railroad fleet simulation and optimization — a feature which helped engineers present the model to executives. Numerous further developments of the simulation model are planned. For example, fitting trainsets with detectors to send data to the model and help make it represent reality more closely. The scheduler is also to be upgraded with probabilistic methods and machine-learning features for predictive rail scheduling and further optimization of scheduling policies. Outcome The digital twin represents the operations of the entire WCML fleet. It enables its users to save on unnecessary railway maintenance expenses by finding an optimum solution for the given constraints. The user can:  Understand the system performance within given parameters and find bottlenecks. Explore different ways to service trains more cost effectively (altering train fleet maintenance regimes, scheduling strategies, depot capacities), fast, and safely in a digital environment. Compare scenarios, evaluate KPIs, and make informed decisions.  In case of any emergencies or unplanned events, Alstom can quickly find a new and effective solution by changing the input data. It is also possible to anticipate possible events and find solutions beforehand by running various what-if scenarios.  If some global changes are proposed by the customer (new timetable, additional trains or routes), the maintenance company can check whether they affect maintenance and propose new solutions. Additionally, the model is a good illustrative tool when presenting to the customer.   Functional scope of simulation (click to enlarge)  The rail network digital twin is a valuable railroad simulation and decision-support tool for other stages of fleet maintenance, including:   Taking part in bids and tenders, the company can make reliable estimates with data and simulations to support proposals — the model is a powerful visual tool for communication.  During the design and engineering phase, the maintenance company can be flexible with project changes and consider limitations. The company can make forecasts based on the model outputs and make decisions considering project end-of-life phase.  The investment in a rail network digital twin has proven very useful for decision-making, both in the present and the long term.   Our white paper, An Introduction to Digital Twin Development, contains further case studies that help demonstrate the development of digital twins and their benefits — download.      American Motor Vehicle Market Simulation Link:  Tags: Marketing  Problem           One of the world’s leading motor vehicle producers needed a strategic forecast of their performance in the US market for the next five years. The company wanted to estimate the dynamics of demand on their product (motor vehicles of a particular class) and the expected revenue, taking into account current clients, dealers, competitors, and the used vehicle market. Their main objective was to determine how much product the company would need to produce in the following years. They employed The AnyLogic Company to create a complete model of the US market.   Solution           In the AnyLogic model, the whole country was separated into several regions, which were then separated into districts. All the geographical entities were considered independently. The districts were comprised of the people living in the area, the vehicles that some of these people owned, and the dealers that sold different brands.           The characteristics of the vehicles included brand (a total of nine popular brands were considered), market classification, engine capacity, model, price, and year. The vehicles for sale could also be either new or used. The AnyLogic Company consultants used the data on geographical distribution of used vehicles and their average price in each region, so prices for used entities varied geographically, while the prices for new ones were fixed all over the country. Dealers were considered to sell only new vehicles of a single brand. Each dealer operated within the region, and requested vehicles were always available for purchase.           Customer characteristics included all the essential parameters for a scrupulous marketing analysis. Customers were divided into segments based on their age, gender, and race. Characteristics such as state of employment and income were also included. The customer population was simulated dynamically. Deaths and births, together with other changes, were taken into account. Due to the large scale of the model, 1 agent simulated 100 people. All the input data on customers was taken from real life, including the official US unemployment forecast for the next five years.           Another customer characteristic that was considered was their attitude towards products. The whole model was based around this concept. A prospective customer’s desire to buy a product was influenced by advertisements, contacts with product owners, and visits to dealers (see the picture).           One of the main challenges was “translating” the data to the language of agent behavior. This required a detailed analysis of the facts and determination of the actual points that influenced customers’ decisions, their sensitivity to different factors, and the inertness of their decisions, etc. Thorough validation of the model based on historical facts and statistics was also important in this project.   Outcome            The client used the model created by The AnyLogic Company consulting team to draw up their strategic plan for the next five years and successfully align their marketing policies and production plans with the forecast.           Watch Anatoly Zherebtsov from The AnyLogic Company presenting this project at the AnyLogic Conference 2012 or download his presentation.        An Agent-Based Explanation for Mentally Ill Patient's Living Situation Changes Link:  Tags: Healthcare, Social Processes  Problem           A Severely and Persistently Mentally Ill (SPMI) patient is generally defined as someone with a diagnosis of Schizophrenia, Bipolar Disorder, or Major Depressive Disorder, and this group constitutes about 1.7% of the US population. The housing environment of the SPMI population in the United States has changed drastically over the past 60 years, most notably in the percentage of the population living in prison versus the percentage living in community based care and private residences.           Many of the changes have been successful. The number of SPMI patients living in the community increased, and although SPMI patients can still be institutionalized against their will, the likelihood of this happening is much lower than in the past. Yet a growing minority of people with severe illness are worse off because they are homeless or incarcerated.           In this case study, IBM Global Research and Otsuka Pharmaceuticals used an agent-based approach to model these remarkable swings in SPMI living situations over the second half of the 20th century. They wanted to better understand the situation and possible improvements that could be made.   Solution           The important thing to note about SPMI housing options, is that some types of housing situations (jails, prisons and long-term hospitals) keep SPMI patients longer when they have mental health relapses, while the majority of housing situations seek to evict patients who misbehave in this same way. This fundamental difference was the key rule that the model sought to demonstrate as a major influencing force on the dramatic housing situation changes undergone by SPMIs over the second half of the 20th century.   SPMI Housing Situation Changes          The SPMI housing agent-based model contained components based upon the SPMI population’s real world housing environment.           First, the model included the “housing cycle”. When SPMIs left one housing situation, they were faced with a decision about where to live next. SPMI housing choices were thus a continuous cycle. Upon a patient’s arrival at the housing decision point, the patient was randomly placed into one of the seven housing types with certain probabilities. The housing types included:    Jail or prison  Long term hospital  Community hospital  Assisted living facility Shelter or transitional housing  Private or subsidized residence  Homeless          The housing types had baseline lengths of stay for SPMI patients. Long term hospitals tended to keep SPMI patients for longer than community hospital emergency rooms, etc.           The SPMI patient agents cycled between the decision point and actually residing in the location decided upon. Each patient had a “time to mental health crisis”, representing the various severity levels of mental illness. Time to crisis was uniformly distributed between 5 and 250 days for each SPMI agent in the model and was static for each patient for the entire model run. In this model, a mental health crisis was defined as a symptom flare-up that would make it clear to people interacting with the agent that the agent was not mentally stable. Examples included a psychotic episode, hallucinations, deep depression, etc. In the model animation, agents were closer to the color pure red when their time to crisis was closer to the minimum, and closer to the color pure green when it was closer to the maximum. The model was populated with 1,000 such SPMI patient agents.           The seven housing types responded to a SPMI patient’s mental health crisis differently. While a long-term hospital or a prison typically extended length of stay for a patient having a crisis, most other types of housing sought to evict or discharge such patients.   The housing cycle of SPMI agents          The model also included a ceiling placed by the United States legislation on the percentage of patients who can possibly live in long-term hospitals. The ceiling was gradually lowered each decade, which reflected what happened in real life. With each subsequent lower ceiling on the percentage of agents capable of being housed in long term hospitals, private housing added the majority of the formerly hospitalized agents with a small increase to the jail and prison populations.           The model metrics included the baseline lengths of stay and likelihood of going to each housing type upon reaching the housing decision point. Included were the date, percentage of the total population currently living in each housing situation, and the average time to crisis for the group currently living in each of the seven types of housing.           The model closely matched real world experience about the health of SPMI people living in each of the housing categories. The least

 

Section 5 of 60
healthy SPMI patients tended to pool in long-term hospitals and prisons. As long-term hospitals lost capacity, the worst SPMI agents increasingly moved into jails and prisons.           The reason for this phenomenon of the lowest time to crisis agents pooling in long-term hospitals and prisons had to do with the unique way in which these two housing types treated patients who have a crisis while housed in their custody. The less healthy the person, the stronger the reinforcing effect to stay in a long term hospital or prison and to leave all the other housing options. An ethical problem arises when legislation places a limit on the number of SPMI patients who can live in long term hospital, resulting in prison being the only housing environment which has both the capacity and a response of increased length of stay upon a mental health crisis event.   Outcome           This agent-based model improved the understanding of Severely and Persistently Mentally Ill housing dynamics in multiple important ways.           First, the model showed that even a weak length of stay alteration due to SPMI crises produced a very strong effect. SPMI patients were in a state of flux until arriving in a place that will not let them leave.           Second, increasing patient time to crisis has a significant positive impact on the population’s housing makeup. In a second model run, the researchers increased patient time to crisis by 45 days in the year 2000 and examined what would happen by 2030. As expected, a healthier population (a population with higher time to crisis values) had weaker length of stay reinforcing effects and thus lowered the prison population rate. This insight had implications for policy makers looking to evaluate the return on investment of projects designed to improve the lives of the mentally ill. This agent-based simulation allowed them to understand that proactive governments can indeed lower prison rates by improving mental health in a geographic area.           Finally, the model raised ethical concerns for future mental health policies in the United States. The suggested ways to improve the situation were either increase long term hospital capacity or find a way to increase patient time to crisis. One effort to improve SPMI patient health in the information technology realm is currently underway with a partnership between Otsuka Pharmaceuticals and IBM. This project seeks to use care coordination information technology to help a geographic area’s many health care providers work together to efficiently treat SPMI patients.           More details on the project can be found in the accompanying article or the video presentation by Kyle Johnson from IBM Global Business Services:        Analysis of Management Strategies for the Aircraft Production Ramp-up Link:  Tags: Manufacturing, Business Processes, Asset Management  Problem Ramping up the production of a new aircraft is a complex process, involving continuous evaluation and revision of both the product and production processes. As a result, aircraft lead time can increase dramatically. Additionally, aircraft are low volume products, with each unit often highly customized. These factors, combined with decreasing product lifecycles, pose a major challenge for aircraft production engineers. To meet the challenges of increasingly frequent ramp-ups, the Airbus Group joined the European Union ARUM (Adaptive Production Management) project. The project, aimed mainly at new product ramp-ups in the aircraft and shipbuilding industries, focused on developing tech solutions for risk reduction, decision-making, and planning. Simulation was chosen as a part of the ARUM solution. It allowed the participants to reproduce a real production environment (based on an Airbus Group case study) and provide a benchmark for testing ARUM solutions. AnyLogic simulation software was selected because it can combine agent-based and discrete-event modeling methods. Solution  The simulation model included a part of the Hamburg Airbus A350 assembly line where two different pieces of the fuselage are completed. This part of the assembly line consists of six assembly stations with 30-35 people working at each, and approximately 300 work orders per station. The challenge was to simulate the ramp-up process with general productivity increasing over time, and the whole ramp-up period lasting up to two years.    ARUM solution structure.  The agent-based and discrete-event model consisted of three types of elements:  The flow line, including work stations, each one with its own material and labor resources. The stations were modeled as agents. The products (fuselage sections) going through the assembly line. Each section required 200-600 work orders at a work station. Work orders formed tasks that required specific materials and resources. When a section entered a station, it started going through work processes (modeled using the Process Modeling Library), then moved to the next station, and finally, to the assembly line in a different city, which was not modeled.  The control model included plans that were sometimes affected by disturbances. The controller agent modeled the complex behavior of human managers reacting to disturbance events with control strategies.  The control strategies included open work policy alternatives. This meant that if some of the work could not be done, it could be postponed and completed further along the assembly line, perhaps in another city. In this case, workers from Hamburg would travel to complete the work (the “traveling work” strategy). Alternatively, work on the section could be suspended until the problem was resolved (the “stop and fix” strategy). The disturbances that occurred during the ramp-up included:  Unbalanced workload and resource allocation due to the worker learning curve and the fact that the same line produced several different products. Design non-conformities and changes, as production often began with a partially prepared product. Missing material or material incompatibilities due to late design changes.  Included in the model statistics were: aircraft lead time, the amount of traveling work, and resource utilization rates (labor, materials, and stations). The project resulted in a model that was easy to understand and reuse. Ultimately, it was integrated into the ARUM solution architecture, including the assembly line visualization component.  Simulation model structure.   Outcome The model was run to simulate the effects of the disturbance mitigation strategies currently being applied at the Airbus facility, including the “stop-and-fix” and “traveling work” strategies. Multiple ramp-up scenarios with different sets of production plans were tested and made use of historical assembly disturbance data, including extreme scenarios. Overall, the model will be used to compare plans suggested by the ARUM suite with current management practices. This will allow the development of optimal disturbance mitigation strategies for both aerospace and shipbuilding manufacturing ramp-ups. A powerful tool for accelerated product lifecycles.    AnyLogic Simulates Consumer Choice in Telecommunication Market Link:  Tags: Marketing  Context Telecommunications providers are seeking to capitalise on the growth potential of broadband internet services in Australia.   The choice of technology roll-out by providers must balance technology roll-out cost, technology limitations of broadband speed decreasing with distance from the source, and consumer’s demand for internet services requiring increasing broadband speeds over time.  Modeling Approach  Evans & Peck and Alcatel Australia jointly developed an agent based consumer choice model which allows scenario analysis on the potential impact to providers over the next 10 years. Critical to the consumer choice behaviour in the model were the geographical aspects of the consumer's location in relation to technology sources (eg. telephone exchange or base station) which ultimately setup a choice matrix of technologies and providers for that particular consumer.  A user interface was created in which a map of the geographical area of a single telephone exchange could be overlaid with both the location characteristics of households (density and consumer type) and of technologies' coverage.   Key features of the modelling tool developed include:        Segmentation of the market by multiple customer types     Multiple internet service providers, each offering multiple technologies     Inclusion of cost models (CAPEX/OPEX) and revenue models (ARPU)     Geographical mapping of populations densities and technology coverage areas     Consumer choice algorithm based on availability of technologies, customer profile and ranking of providers     Variation of consumer demand for broadband speed over time based on selection of products (eg. high definition video)    Model developed and published by Evans & Peck Australia    AnyLogic Tackles Eiffel Tower Crowds Link:  Tags: Passenger Terminals    To efficiently manage visitors at the most popular tourist site in Paris, the Eiffel Tower, the operating company (SETE) used AnyLogic simulation software from The AnyLogic Company.     The model designed by the software simulates tourist arrivals and their behavior in open spaces. The program estimated queues in front of elevators, taking into regard elevator operation rules and capacity. The main issues of waiting lines and crowd management were on the Tower’s top floor.     AnyLogic has an Agent Based simulation modeling tool, which means it actually simulates the individual behavior of each person (in this case, visitors at the Eiffel Tower). The project members said the Eiffel Tower had 6.7 million tourists in 2010. 27,000 program agents are in attendance on the days of highest visitor traffic. The Tower model with its interior and visitor flows was created by the French EMSYSS consulting company. AnyLogic Europe also took part in the project.     The AnyLogic Company Business Development Director, Timophey Popkov, reported that the project took 30 business days to be completed. “Within that time we gathered data to create a Tower model, took photo and video footage of the visitors’ behavior, and looked into the project to reconstruct the 3rd and 4th floors of the Tower,” he said. The second stage included the model designs. As a result of various experiments, the engineers came to realize that new designs would not solve the problem of crowd management, and thereby would not be reasonable in terms of investments. “The model determined that the new designs would not change the passability issue,” said Popkov. “Finally, we experimented with the model to try and optimize the existing visitor itineraries.”       Apparel Company Chose Location for New Distribution Center Using Simulation Modeling Link:  Tags: Supply Chains  Problem   Fruit of the Loom (FOTL) is one of the largest US apparel manufacturers and marketers. The company was expanding, and the executives wanted to know if it would be beneficial, in terms of shipping costs, to add a new distribution center (DC) in the US, or to redistribute products to a pre-existing DC. The contractors decided to simulate the whole supply chain in order to visualize DC locations on a GIS map, and the supply networks between them. Focusing on wholesalers, the company considered distributors as customers. No small packages or end-users were involved in this case.   Solution           AnyLogic simulation software was chosen due to its agent-based modeling capability. The team put the description of the supply chain into the model, and defined the following as interacting agents with their own goals and rules, individual behavior, and interaction policies:     Distribution centers (location, number of units, overhead cost, startup cost)  Customers, or wholesalers (location, demand rate, total shipments, distance, shipment type etc.)  Trucks and trains (location, units, owner, destination)  Producers (cotton farmers)  Processors (yarn mills)           With AnyLogic, it was easy to test multiple scenarios and their variations. The input data (number of DC, suppliers, etc.) was different for each variation.           The team found advantages of applying simulation at different stages. It enabled abstraction, simplifying a complex system by focusing on relevant details and estimating them.           When building the model, the team first looked at shipment data, focusing on the locations of high demand customers, as they made up 85% of all the company’s shipments. These distributors used truckloads and rails. The team also considered shipments the customer received per year and the demand per shipment in units per customer. Last year demand indicators were used as model input.           To estimate and visualize the existing transportation routes, the team used AnyLogic GIS capabilities, which linked the agents of the model to their locations. The company’s head DC was located in the south-east US. From there, the cargo was distributed to wholesalers directly or via transit DCs. This could be a reason for additional transportation costs, which is why the team estimated an optimal location for new DC regarding the wholesalers' disposition. For this purpose, the developers conducted supply chain Greenfield analysis. This experiment is available at anyLogistix, the multimethod software for supply network optimization, design, and analysis.    Scenario #1 included the original DC and outlier customers, and did not show shipping cost reductions regarding products distributed from the DC.             In Scenario #2, the original DC was used, rerouting the shipment to other existing DCs, located on the east and west coasts of the US. The scenario resulted in a 42% cost reduction.            In Scenario #3, the network with the original DC was incremented with a new DC in optimal GIS-location. Cost reduction appeared as 32%.   Scenario #4 included the original DC, rerouting products through another already existing DC on the west coast and a new DC in an optimal GIS-location. The scenario's results showed about a 45% cost reduction in the supply chain.           The reports on the model runs for each DC could be exported as an Excel file.           After the experiments, the model was extended with the following agents to adjust for an international supply chain:    Manufacturers   Additional distribution centers   Loading ports   Ports of discharge   Vessels   Additional trucks           Then the team could design the whole international supply chain and determine the optimal location of the DC, taking all agents into account.   Outcome           While decisions made on people’s expertise, without examining the data, may be considered biased, data-driven insights from GIS and AnyLogic, paired with business knowledge, helped develop supply chain recommendations.            For this case, simulation

 

Section 6 of 60
modeling was used as an exploratory research tool to assess the suggestions on placing DCs throughout the country and to prove their economical effectiveness in terms of manufacturing and customer’s supply chains.            The developers highly rated AnyLogic user-friendliness, which allowed them to drag and drop the elements and customize, or expand, the model to any conditions, which would benefit Fruit of the Loom’s international business.    Project presentation by Elizabeth Tyrie, Data Science Manager       Assessing Moscow Metro Station Blueprint with Crowd Simulation Software Link:  Tags: Passenger Terminals  Problem The Moscow metro is Europe’s busiest and most expansive subway system in terms of annual ridership, and it is Russia’s most dynamic means of transportation. Every year new metro stations are built and the existing metro facilities are reconstructed. The latter includes the renovation of station antechambers, the creation of additional entryways, and the installation of new escalators. When planning the subway renovations, the engineers wanted to be sure that the design solutions they chose would comply with current safety regulations and ridership. For instance, at Kiyevskaya station, one of the Moscow’s largest metro hubs, with ridership of 30,000 passengers an hour, the engineers decided to enlarge the existing antechamber to increase the passenger flow, replace the escalator belts, and add an extra escalator in the rundown corridor to the Arbat-Pokrovskaya line. To evaluate the project design, they contracted the specialists of the Institute for Development of Transportation Systems (IDTS). The consultants were asked to develop a pedestrian simulation model in order to:  Conduct crowd dynamics simulation and analysis to identify solutions for the efficient organization of passenger flows at the antechambers of the station, including options with and without interior redesign. Analyze operations at the station with an increased number of escalators. Perform evacuation simulation and evaluate the time required for the evacuation of passengers from station platforms. Provide recommendations for operating process optimization during the subway station reconstruction.  To this end, the consultants created a simulation model of the subway station using AnyLogic pedestrian modeling software.   Solution In the initial phase, the specialists developed a baseline simulation model that reflected passenger flows behavior, from disembarking from suburban electric trains at Kiyevskaya railway station to embarking on the trains in the subway. To make the model as realistic as possible, and to properly reflect the behavior of the passengers, the pedestrian model was calibrated on the basis of an in-situ survey. The consultants paid attention to crowd behavior in the loading and unloading areas of stairways and escalators. With the completed model, the consultants analyzed different topological options for the antechamber entrance design and selected the most efficient options, with and without building redesign.  Current status (click to enlarge)  An option without building redesign (click to enlarge)  An option with building redesign (click to enlarge)   The consultants also simulated the scenarios for the operation of escalators during the reconstruction, during the commissioning of new escalators, and during the evacuation of passengers. They noted that, in the case of three escalators simultaneously lifting only in rush hours, the exit antechamber would be unable to handle the expected passenger flow because the escalator unloading area would become congested with passengers.    Congestion of passengers leaving the escalators   Result The customer was issued the simulation models for further experiments, with the user interface module allowing them to change the following parameters:  Intervals between subway trains; Passenger distribution among the train destinations; Number of services in operation: ticket windows, ticket machines, turnstiles; Passenger service time; Escalator operation mode; Ridership characteristics embarking on and disembarking from trains.  As a result, the consultants offered a solution for the organization of station antechambers operations, both in the standard mode and for the renovation period, and provided recommendations concerning the operation mode of a fifth escalator leading out of the metro station to the Kiyevskaya railway station.    Automated Driving Systems Testing Using Agent-Based Modeling  Link:  Tags: Road Traffic, Defense  Southwest Research Institute (SwRI) has gained worldwide attention by leading NASA missions such as the New Horizons mission to Pluto and the Juno mission to Jupiter. SwRI is also a leader in fuel and energy efficiency, geosciences, turbomachinery, and energy storage. Their contract engineering efforts benefit government, industry, and the public through the application of science and technology.  Problem One of the institute’s research areas is automated driving systems. SwRI has been working in this field since 2006 and has designed systems for a semi-truck, Ford Explorer, many military platforms, and a wide variety of unmanned aerial vehicles (UAVs), commonly known as drones. These automated systems no longer need human drivers to control them on their missions, whether it is reconnaissance, hauling, or simply transportation. But SwRI’s engineers didn’t want to stop there and decided to make autonomous vehicles free, not only from the driver, but also from a control center. According to this idea, vehicles would communicate in a distributed manner with each other, share information about their current location and environment, and make decisions on further actions based on this information themselves. This technology would primarily be used by military forces for the transportation of supplies to the fields of operation, demining, reconnaissance operations, and many other areas where humans can be replaced by machines for their own safety. The implementation of such systems can take a lot of time and money, so the engineers at SwRI decided to use simulation modeling to explore the possibilities of autonomous vehicles. Solution   Picture 1. Collaborative map of the area based on agents’ explorations To assess the performance of automated vehicles and to evaluate algorithms and task sharing between the vehicles, SwRI engineers decided to build an agent-based AnyLogic model of vehicles’ operations in an enclosed area with random obstacles. It was the easiest way to represent multiple interacting virtual vehicles with a variety of capabilities and have them all operate simultaneously.  The vehicles detected obstacles, found, and refueled capsules in the area. Completing these tasks quickly required the vehicles to cooperate and share information about the environment.  All the vehicles had sensors that could detect the environment, gather information about the things around them, and share their knowledge with other agents. Each vehicle was given predefined behavioral features: some vehicles could only search for capsules, some could only check if capsules were full or empty, and others could only refuel them.    Picture 2. Agent’s state chart On the right side of Picture 1, you can see how each individual vehicle explored the area and found obstacles, and on the left side of the picture is the collaborative map of the area based on their explorations. The collaborative map was shared between all the agents and each agent could take advantage of the combined mapping capabilities of the other ones. For refueling capsules in this area, vehicles had to form teams based on their individual capabilities and location. When a searcher vehicle finds a capsule, it signals for the nearest vehicles with the required capabilities. These vehicles create a team with the searcher and assist with classification and refueling to complete the mission. In Picture 2 you can see the agent’s state chart describing this process. Outcome With AnyLogic, SwRI engineers tested how automated vehicles could behave in a cooperative network, and proved that such networks can be built in real life. Consequently, this suggests that researchers can develop algorithms for solving any related problem using modeling in AnyLogic, test these algorithms, and implement them in autonomous vehicles. For example, the creation of a mixture of drone and ground-based robots for scouting or security patrols.   To learn more, watch the project presentation at AnyLogic Conference 2016 or download it.     Automatic Generation of Simulation Models to Improve Business Processes Link:  Tags: Business Processes  Problem  Simulation can be difficult, often requiring a lot of training, therefore   AIG sought to make this process easier. They resolved that someone without any skills should be able to utilize discrete-event simulation and build good models.  Solution  AIG developed a methodology called Process Wind Tunnel (PWT), which is a system that uses a data-driven approach to improve business processes. Discrete-event simulation is an important component of this.  The Process Wind Tunnel includes current state analysis, future state design, and process automation continuous improvement.    The Process Wind Tunnel created by AIG (click to enlarge)   Current state analysis includes an important element called process mining, which looks at event logs that are generated by the business process. This historical data can be analyzed statistically to gain insights into the business process.  Once there is a baseline for this process, it is necessary to improve it. This can be done in the future state design. Here, AnyLogic simulation is used because it has many powerful features including the ability to integrate with other tools through the Java programming capability. AIG uses AnyLogic to build data driven discrete-simulation models. AIG builds the model, does the scenario analysis, looks at new design options and comes up with an optimized or improved design.  In process automation, the system is analyzed end to end to identify areas for targeted automation or even complete automation. They have also started looking at a   digital twin because they collect so much data from various systems.  Building a discrete-event simulation in this environment requires a partnership between a business process domain expert and a highly skilled simulation and analytics specialist. The former would collect historical data, while the latter would build the model in AnyLogic using this information. Once the model was built, the modeler and/or the business domain expert could utilize the simulation model to perform scenario analysis and optimization.    Business process model (click to enlarge)   Simulation modeling requires significant information gathering. The solution offered was to decouple the process of information gathering as it was time consuming. Instead, a predefined model template could be used, which would be dependent on the relevant business process. This template should have information that a businessperson could understand.  There is an example template below, created in an Excel spreadsheet, and this is what would be provided once it was developed and customized for a specific business application. A businessperson who has little or no experience of discrete-event simulation or of modeling in general could input information. From that information a discrete-event simulation model would be generated automatically. Depending on the different business process, the model template could be different.    Example template created in an Excel spreadsheet (click to enlarge)   Once this template has been created, the modeler could choose different ways to build a simulation model. The first way would be to use the built-in Java functions to programmatically construct simulation models within the AnyLogic environment. The second way, which AIG chose, would be to use software modules written in Java outside of the AnyLogic environment and then import them into AnyLogic and let the power of AnyLogic do the further analysis.  So now the businessperson is only dealing with those Excel spreadsheets which have understandable modeling information and from there onwards the process of transforming that into a simulation model is done automatically.    The java application instantiates Java classes, reads the Excel template, and creates the AnyLogic simulation model automatically   Results  Using the methods described here, it is not necessary to build a model over and over again if you have multiple instances of a given process. The full scalability of the model can be enabled, from the business information to the actual modeling. The results can be interpreted by a businessperson and not an advanced AnyLogic modeler.   The case study was presented by Sudhendu Rai of AIG Investments, at the AnyLogic 2021 Conference.  The slides are available as a PDF.     Automotive Company Applies Business Process Simulation Tool to Improve Resilience and Innovation Link:  Tags: Social Processes  Problem A multinational US based automaker was considering altering the company’s strategy and business model in the face of changing realities. A resilience and innovation gap can grow into an existential risk. Statistics show that companies are six times more likely to go bankrupt today than 40 years ago. Today, the car ownership model is gradually being replaced by a model where a vehicle is a service. In view of this, the automotive company declared itself not a manufacturer any more, but a mobility company. Going through extensive changes, the company wanted to become more resilient and innovative, and therefore, reduce risks and increase income. They believed that the core factors in achieving these goals are professionals and their effective cooperation. The company began looking for ways to facilitate effective networking to ensure the seamless flow of ideas and collective participation in thinking and decision-making processes. As a testbed group, the company’s analytics team was chosen. It incorporated analysts and data scientists on a wide range of applications from company infrastructure to smart mobility solutions.  Solution The company’s social specialists used the Adaptive Cycle Theory to explain that there are four phases any company goes through again and again regarding its strategy in different spheres. They used the term “adaptive resilience” to explain that professionals’ networks inside the company should be flexible and adaptive. The social specialists claimed that teams should also have different but complementary workstyles.   The Adaptive Cycle: corresponding professional workstyles and network types in the company (click to enlarge) To build such resilient and diverse teams, the company followed the concept of mirroring. Like sport teams who use video recordings to view their own games, members of the

 

Section 7 of 60
testbed group needed an equivalent business process simulation tool to help reflect their performance from the perspective of communication and collaboration. First, social specialists responsible for the project gathered data about employees’ networks and their features. With the help of questionnaires, specialists learned how people were connected regarding innovative ideas, expertise, and projects. The second information resource was employees’ mailbox data showing people’s interactions. Then the network analysis tools were applied to analyze the existing relationships between the employees.   Business process model architecture (click to enlarge)  The simulation was developed in AnyLogic, which provided the opportunity of multimethod business process simulation. The developers used agent-based modeling to replicate the employees’ network and system dynamic modeling to imitate the adaptive cycle. As a result, the simulation reflected possible networks and collaboration in each phase of the adaptive cycle. AnyLogic business process modeling software also provided the possibility to incorporate Python codes right into the model for in-simulation network analysis. The business process simulation model can be exported and delivered as a standalone application, where the employee can enter the network ID and then get access to the simulation with interactive and user-friendly AnyLogic interface. With the business process simulation model, employees can:  See their network statistics and compare it with their peers and persons of higher positions. They can get insights into their network connectivity and their position in this network, and then compare their own metrics with the highest and the lowest performances.  Learn the robustness of their network to understand how resilient their network is and if it can adapt to different phases of adaptation cycle. Do what-if analysis. The employees can run various scenarios including or excluding people from their network and see how it will influence the network and the users’ position in it. This way they can learn the role of other people in their networks and find the best network solutions.  Employees are not the only ones who can use the business process simulation tool. It also enables social specialists to:  Experiment with the networks in a safe digital environment to understand how to build resilient teams.  Verify ideas about the adaptive diversity and resilience concepts.  Better understand employees’ preferences, strengths, and weaknesses regarding collaboration and teamwork in different adaptive cycle phases.  Outcome With the help of AnyLogic business process simulation software, the company’s social specialists responsible for increasing the company’s innovation and resilience level got some significant results:  The model enabled them to validate the adaptive diversity and adaptive resilience concepts they use when building teams for various challenges. It gave social specialists deeper insights on how to put teams together for better adaptive capacity. It helped social specialists segment employees into different groups according to their workstyles and collaboration preferences and find some tendencies in this regard. It would help the specialists build effective and resilient teams for particular projects.  Regarding employees from the testbed group, it was argued that the best way to improve their performance was to let them develop innovations on their own, offering the right environment. With the business process analysis tool, the company gave their employees the opportunity to analyze their network, their position in it, run digital experiments, and make their own decisions about possible changes.  In order to find the most effective way of mirroring for employees, the company tried different visual learning approaches. All of them provided employees with information about their networks, but only business process simulation gave them the opportunity for experiments. Simulation proved to be the most effective tool so far. After getting access to simulation, the employees changed their collaboration behavior significantly, which has increased the effectiveness and robustness of the networks. The business process simulation proved to be a powerful tool for both social specialists and employees themselves concerning improving professionals’ collaboration. There are plans to expand the project from the testbed group to the whole company. Thus, the business process simulation model, where the core elements are employees and their interactions, could be an effective tool for business companies to improve their performance.    Automotive Production Process Optimization Link:  Tags: Manufacturing    Engineers at a world-renowned German automotive manufacturer sought a modern and optimized production line for the company’s highly successful commercial van. With over 3.4 million deliveries of the commercial vehicle from various production sites, the company wanted an optimization tool with the flexibility to apply at different plants and account for technologies such as autonomous guided vehicles (AGV). The first stage of the project realized an efficiency gain of 5% and identified further significant gains for the next stage.   Problem: Assembly line optimization and efficiency   The company’s logistics engineers needed a tool to assess and optimize the efficiency of automotive plant intralogistics in the context of a large-scale AGV implementation. They wanted to know:    Equipment utilization Workforce pooling (especially for irregular tasks) Time lost (due to traffic, waiting, repacking, etc.) Distances traveled The number of assembly line shortages Stock re-order levels Delivery tour utilization  Above all, for profitability, there should be no production stoppages caused by assembly line shortages.   While considering the efficiency and optimization of the aspects listed above, planning engineers had to ensure they met the challenges of the commercial van production line:    Numerous components – tens of thousands of part numbers assembled in the same plant Component variability – the van’s wing mirror has 130 variants Complex transportation – including forklifts and car-set delivering AGVs Time and space constraints – shared pathways and time critical activities Assembly line policies – quality, numbers at line, ‘one touch – one motion’, etc.  How to optimize a complex assembly line   Traditional analysis techniques based on spreadsheets cannot consider the complexities of a modern automotive assembly line, such as the processes involved and the relationships between them. Furthermore, to truly capture the nature of an assembly line, it is necessary to consider the dynamic nature of transporter vehicle speeds, traffic conditions, access to shared resources, and stochastic demand.   Simulation offers a way to capture assembly line complexities and their dynamic nature. With AnyLogic simulation software, the production process engineers could model their assembly line using the Material Handling Library, connect to external software libraries, and develop custom elements. In addition to the modeling capabilities of AnyLogic, the software’s cloud functionality met the collaborative experimentation requirements of the engineering team – speeding up development by shortening feedback and review cycles.   Solution: Modeling for complex automotive production facilities   To maximize the model’s applicability and future usage, the developers created the simulation model using several guiding principles:    Flexibility Scalability Real data sourcing Rapidly adaptable  These principles ensure the model will work accurately with layout modifications, for different plants, and with new technologies and process developments.   The model layout is configured as a parameter by parsing a DXF file of the shop floor. This means that a production planner can modify a layout using their shop floor design software, such as AutoCAD, and quickly test it in simulation.   Real data sourcing means that every part, location, transporter, and order is uploaded and set as an agent at the start of a simulation. This granularity ensures a very low abstraction level and high level of model realism.   To ensure adaptability and accommodate the rapid changes that take place in modern manufacturing, the developers used a modular design approach. This means that new developments and process modifications can be modeled independently and easily integrated into the overall model for analysis.   Processes in the vehicle production line simulation model (click to enlarge)  Results: Simulation provides detailed analytics   The simulation model of the automotive production facility provides more outputs and more detail than the logistics engineers have ever had before. As an example, multi-cart transporters could visit any of more than 300 possible locations and their behaviors were hard to analyze. Now, their routing is captured. Metrics also show, not only how busy the driver is, but also the level of vehicle utilization, giving insights into efficiency.   Screenshots from the simulation model interface (click to enlarge)  The model developers were able to provide all the outputs the engineers wanted on a level they did not have before – providing more results and more detail in those results.   Decision support for planning and investment   Initially, the simulation model met success by verifying that everything for a multi-million euro AGV project would work as desired, before committing to the order.    After helping confirm the initial AGV project implementation, the model continues to be used for verifying projects before implementation, as well as for other purposes as well. In all, the model helps assess manual process efficiency, supports AGV implementations, and supports work that is leading towards a fully automated shop floor.   How the automotive vehicle production line optimization tool is being used (click to enlarge)  As an example of how the model is performing, just the first step of assessing a workforce of more than 70 people realized an efficiency gain of 5%, and the second step found another 5% gain that can be made through automation.   With regards to the efficiency assessment purpose, it is worth noting that the simulation now provides 90% of the calculations that a planner previously had to do manually, helping save time and providing additional verification.   Overall, with careful planning and clear aims, the German auto-giant’s logistics planners have implemented a tool that supports and helps improve present-day and future operations. Furthermore, the model can be applied to other facilities, increasing the return on investment.   Key to the project were the design decisions around the flexibility and scalability of the model and the capabilities of AnyLogic that enabled their implementation.   A related presentation with a Q&A session at the AnyLogic Conference is available to watch.       BNSF Applies Intermodal Container Terminal Simulation and Planning to Expand Facility Capacity Link:  Tags: Rail Logistics, Ports & Terminals  The BNSF Railway Company is one of the largest freight railroad networks in the United States. The company has 32,500 miles of track in 28 states and three Canadian provinces, and more than 8,000 locomotives. BNSF connects freight shippers and consumers in the global marketplace by delivering different types of goods, including agricultural products, coal, industrial products, and intermodal freight. The latter accounts for roughly half of BNSF's total volume, and this share continues to grow. The company’s intermodal hub network is made up of over 26 facilities in the USA, including the major ones, such as Corwith in Chicago, Hobart in Southern California, and Alliance in Fort Worth, Texas. Expanding the company’s existing facilities, and developing new facilities in intermodal segments, would require making large capital investments. To help evaluate various options of capacity expansion in major facilities, and make data-informed decisions, BNSF developed container terminal simulation models of the Corwith and Hobart facilities. The team chose AnyLogic simulation modeling software because of its flexibility and Java-based architecture, which helped employees, unfamiliar with simulation modeling but familiar with Java, learn how to work with the program.  Case #1: Corwith Intermodal Terminal Simulation  Problem  For the Corwith intermodal facility, the main goal was to increase the annual lift capacity to keep up with the growing demand. The team knew that new tracking cranes were needed, but at the same time, they weren’t sure if new storage should also be added. The situation was also complicated, as the facility was landlocked by businesses and neighborhoods, so the company couldn’t expand the footprint. The team had to perform one of the following:  Densify production tracks by adding production tracks in between the existing ones, and then putting taller cranes above smaller ones. Convert some of the other areas in the facility into production tracks.  Solution and Results Another way to expand the capacity was to add a standard rubber tire gantry crane. These cranes are nimble and can move freely through a container yard. However, applying this type of crane does not allow for creating additional tracks. With widespan cranes, a type which allows for more tracks, the process would not be as quick and efficient.  The third type of crane was a cantilevered rubber tire gantry (CRTG) — a hybrid between the two previous types. It is not as large as the widespan crane, it can span multiple tracks, and it has rubber tires, which enables movement between sets of trucks. To see how this crane would influence the terminal operations, the team simulated its functioning in the model.  Standard Rubber Tire Gantry (RTG)  Cantilevered Rubber Tire Gantry (CRTG)  Widespan   Corwith Intermodal Terminal Simulation and Optimization The container terminal model shows the operational process of the facility. First, the train arrives and then is shoved into the production tracks. Next, a crane unloads a box and then puts it on a truck, which transports the box to another track. After that, the CRTG crane arrives and loads the box to another railcar.  Within the model, engineers tested how the new CRTG cranes operated. To analyze the efficiency of the cranes, the developers included in the model various metrics, such as truck utilization, inventory, inbound and outbound performance, delay time, and others.   The results of the analysis showed that the inclusion of the CRTG crane in the terminal operations would provide the most efficient combination of additional track footage, operational speed, and flexibility. Case #2: Hobart Rail Yard Simulation — Southern California Problem  Hobart is the company’s largest facility, processing over one million

 

Section 8 of 60
units per year. When expanding the facility’s capacity, the team of engineers faced the following challenges:   Landlocked facility was surrounded by commercial enterprises. The facility had a large number of arrivals and departures per day. Truck traffic at the facility was dense, which is why adding more truck volume could make the situation even worse.  To determine possible bottlenecks and evaluate strategies for their mitigation, the company decided to perform container yard planning and optimization using simulation modeling. Solution and Results   Hobart Rail Yard Simulation and Optimization To find out how to expand the facility more efficiently, the team simulated the operations of the widespan crane and a new type of diagonal parking underneath the arm of the crane. Simulation showed that this combination would work well for this specific facility, in terms of space and the additional flexibility that diagonal parking could provide. To simulate traffic congestion in the facility, developers implemented the AnyLogic density map, which helped the team understand where the pinch points in the facility were, and find ways to manage them. The analysis of the model showed how to significantly expand the Hobalt facility capacity and avoid bottlenecks. This necessitated a manifold approach:   Alliance Intermodal Facility Simulation  Capital expansion — the company needed to add some tracks and cranes of several types. Operational improvements — it should improve the facility operational efficiency through various optimization techniques. One of those was to route trucks through the facility based on the real-time congestion conditions.  Customer behavior — the team decided to smooth the volume of the facility over the day and week to add capacity without spending capital dollars.   The company is now looking for the automated technology to expand the capacity of their Alliance Intermodal Facility in Fort Worth, which grows 20% each year. Using AnyLogic simulation modeling, the Research & Analytics team has started to work through the routing logic and possible deadlocks, as well as the operations of facility equipment. They will later include this in a full-scale model of their Alliance facility.  Project presentation by Mike Prince, manager of operations research at BNSF       Better Decision Making with Manufacturing Simulation, Digital Twin Technology, and AI Link:  Tags: Manufacturing  Overview       Engineering Ingegneria Informatica is an international specialist in the field of digital system integration. The company has more than 11,000 employees in more than 50 offices around the world. One of its flagship projects is an ecosystem of platforms that enable other technologies to interact with each other – exchanging value, digitizing processes, developing digital services, and creating value for users, especially through the leverage of eight.   Lagor is an Italian company producing ferromagnetic cores for power transformers. Lagor was the first in Europe to offer an electrical steel cutting service, and it remains the leader in the transformer core market. However, while ramping up production and expanding the business, the company faced issues scaling the manufacturing processes. Lagor contracted Engineering to resolve the bottlenecks and blockages in the production system for manufacturing optimization.   Problem   Power transformer cores are made of many layers of coils, can weigh up to eight tons, and require different production cycles depending on their size and client-specific requirements. Power transformer core production starts with the layering of cores on top of each other to reach a desired thickness. The materials remain on a steel pallet during the entire production process. These pallets move between the different workstations using roller or shuttle conveyors.  Manufacturing simulation showcase  Manufacturing simulation limits   All cores go through processing, and some continue through a painting station and a curing station, depending on the production cycle. At the end, all cores undergo testing. Steel pallets are never removed from the line, even if they are empty. Initially, manufacturing line scheduling was conducted manually and for short periods. This method resulted in recurrent issues where objects on the production line created bottlenecks. The most practical way to resolve major blockages was to unload the cores by crane and reset the whole line. With the aim of making the process more efficient, Lagor approached Engineering company. Together, they would streamline production line operations and deliver better management of shop floor movements. Solution With the help of AnyLogic manufacturing simulation capabilities, the consultants created a model to act as a digital twin of the production system. With the digital twin technology, they would be able to feed real-time data, direct from the field of operation, into a manufacturing simulation model to better understand the problems and predict the future performance of the production facility. They used an agent-based approach to model the project’s unique features, including the diversity of core types, related production cycles, and a variable production plan.  Production process sequencing simulation optimized with reinforcement learning   The digital twin technology helped reproduce various production process elements such as:  Steel pallets – critical resources, where the cores are housed on the production line. Regulations state that pallets must never be unloaded from the line. Power transformer cores, which move between workstations depending on the specifics of their production cycle. Conveyors – pallet-carrying components, which can be static (rolling) or moving (shuttle).  Learnt policy applied for the optimized production process   After designing the digital twin model, the consultants connected data from the production facility to the supervisory control and data acquisition system (SCADA) to get an updated line status. They also created a line manager – a virtual agent, which seeks the optimal route for each situation in order to avoid unnecessary movements, anticipate possible criticalities, resolve conflicts, and at the same time, respect delivery dates. To govern the line and achieve better results, the line manager used embedded heuristics-based algorithms. The digital twin of the shop floor, working with the real data, helped reproduce the production and decision-making processes, investigate the production plan, and verify that the selected plan was achievable while respecting delivery dates. With the new digital twin simulation tool, Lagor engineers could successfully rearrange production sequences in a risk-free environment using a “what-if” approach.  Simulating the entire manufacturing production cycle optimized with reinforcement learning   The system, as such, could already be implemented in production and help avoid issues and reduce costs. Nevertheless, the system had limits. Despite careful manufacturing capacity planning and sequence scheduling featuring highly engineered heuristics, there were still occasional bottlenecks. The consultants wanted to get rid of the complicated heuristics and develop a new method of determining the best movement sequence.   The flexibility and customizability of AnyLogic process simulation software enabled the engineers to create a reinforcement learning solution. By using Pathmind deep reinforcement learning and an AnyLogic manufacturing simulation model, they were able to train agents that could determine the movements of cores on the production line and direct the cores to their destination. Simulation models are perfect gyms for AI algorithms, as they represent a realistic environment where algorithm-linked agents can be trained.   In this case, at the beginning, the learning agent does not know the link between a core’s position and the available actions, so it makes random decisions, which are sometimes physically invalid (red arrows in the video below) and do not result in a state change. During the process of learning, the agent stores all the interactions in its memory and, by exploring new actions, discovers better moves. Every time it reaches the target, the layout is randomized, and the simulation is restarted. A rich experience interacting with the environment enables the agent to eventually infer the best decisions for any given situation. After training, the agent can perform its task in an efficient and effective manner. Result Successfully applied deep reinforcement learning resulted in a policy that could effectively manage production line movements and efficiently avoid bottlenecks. The consultants were able to reproduce the entire production process and train the algorithms in a way to avoid the least possible bottlenecks, leading to better production planning optimization and financial savings. Key to the success of the project was AnyLogic digital twin simulation technology, for seamless data integration between the real system and its digital replica, and the possibilities that the software provided for connecting with machine learning technologies. Find out more about integrating AnyLogic models and AI. Watch the video of Luigi Manca, presenting this case study at The AnyLogic Conference, or download the presentation.       Bullwhip Effect in Semiconductor Supply Chain Link:  Tags: Supply Chains         The supply chain management at Infineon, a large semiconductor manufacturer, wanted to investigate the bullwhip effect in their market in order to decrease expenses and better forecast market behavior. They used AnyLogic software to build a model of a supply chain – from raw materials to the market.   Problem        The bullwhip effect refers to a trend of larger and larger swings in inventory in response to changes in demand, as one looks at firms further back in the supply chain for a product. The demand fluctuates much more at the point of semiconductor production in the supply chain than at the point of the final product. The semiconductor industry is very sensitive to outer problems. The Infineon Supply Chain Innovations team wanted to explore the following:    What the bullwhip effect looks like in their supply chain and to what extremes it exists. What connection there is between market demand fluctuation and the fluctuation in demand they received from direct customers.   Solution        The modelers created agents for each of the major players in the supply chain and gave them behaviors based on the well known "Beer Distribution Game". Goods were going from the raw material supplier to the semiconductor manufacturer (Infineon), then to the tier 1 and tier 2 suppliers, the OEM (the final manufacturer), and the market. The information and the orders went backwards. The modelers used the real GDP and semiconductor market data as input signals. Finally, they recreated a simplified internal structure of Infineon. The Infineon agent was separated into two parts:    Planning and control – a branch where capacity decisions were taken and where forecasts and orders were made. Base system – a branch where material flowed and orders were executed.        The agents, Infineon, and the market were then all linked together using Discrete Event simulation method to combine a hybrid model with a highly realistic structure.        All agents outside the semiconductor manufacturer (Infineon) were modeled identically:    Agents produced generic output (information flow was delayed in the supply chain) Agents had two states, anxious and careless, determined by inventory reach:   Agents over-ordered when anxious (+ 20 % of demand)  Agents under-ordered when careless (-50 % of demand)   Outcome        The modelers reproduced the typical behaviors of the agents in a supply chain with the bullwhip effect.        The model:     Helped to analyze particular situations emerging in the market, the consequences of a bullwhip effect, and the amplification of demand along the supply chain. Was used for internal company trainings for bullwhip effect illustration. Was planned for use in communication with customers for cooperative work on reduction of the bullwhip effect.       AnyLogic software was utilized by the Supply Chain Innovations team at Infineon. The specialists were not familiar with simulation software or programming before this project. All the necessary knowledge was obtained from the available AnyLogic tutorials. They chose AnyLogic because it allowed them to combine Agent Based and Discrete Event modeling approaches. The Infineon team thought this was the main advantage of the software, along with its ease of use.        Watch Hans Ehm from Infineon presenting this project at the AnyLogic Conference 2012 or download his presentation:       CSX Solves Railroad Operation Challenges with and without the AnyLogic Rail Library Link:  Tags: Transportation, Rail Logistics, Asset Management  CSX is a railroad company that operates about 21,000 route miles (34,000 km), including one of the three Class I railroads, which serves most of the East Coast of the United States, and reaches nearly two-thirds of the country’s population. The Network Planning division’s role is critical to the company’s success. This division identifies where to add capacity to accommodate future growth, ensures infrastructure can support and sustain a high level of service, and tries to improve the efficiency of capital spending.  Network Planning uses a multistep approach to manage the network capability. They utilize analytical tools to monitor current service levels, identify appearing problems, and determine the root cause of these disruptions (if the problem is operational or infrastructural).  In addition, they analyze what possible solutions can be applied to the problem, including investment decisions, and which of these decisions will provide the best financial return. To get the right answers, the use of traditional analytical tools is insufficient. That is why, for these purposes, CSX employs simulation modeling technologies. They use AnyLogic software for many different purposes because it allows them to create models of various systems, at the required abstraction level, with a quick turnaround time.  AnyLogic allows the railroad industry users to simulate line-of-road, terminal, and yard problems. The following three projects, completed by CSX in 2014, covered a variety of tasks that were solved using AnyLogic software.  MGA Line Investment Planning  Problem  MGA Rail Line Simulation   A rail line that is jointly owned by CSX and their competitors was expected to see a large growth in demand from several coal mines. The high competition between the two companies meant that if one of them could not fulfill the demand, the other one would do so. CSX needed to identify the best operational/capital strategy to handle the increased business. They wanted to know the

 

Section 9 of 60
answers to these specific questions:   Did they have enough staging capacity on the line to stage empty unit coal trains to respond quickly to the new demand?   Where were the best locations on the line to add the additional staging capacity if needed?  They utilized AnyLogic simulation modeling to find the answers.  Solution  The created supply-chain network model simulated the demand of empty trains from five coal mines, as well as the fulfillment of the demand, and staging of empty trains. The trains were modeled as agents moving across the network. By varying values of relevant parameters, users could infer the impacts of different factors to the train throughput (i.e. staging capacity, as well as loading speeds at the coal mines).  The model calculated the company’s achieved throughput, and the business lost by CSX, due to the lack of available trains.  Outcome The model provided a way for decision makers to gain insight into the system to help identify the maximum possible throughput. The simulation showed that the company did not have enough staging capacity to serve the increased demand, and it helped distinguish the highest priority capital investment projects to implement.  Nashville Locomotive Shop Redesign  Problem  Locomotive Shop Simulation Model  The CSX’s Nashville locomotive shop needed to be expanded in order to meet the higher level railroad network redesign. The facility included a quality maintenance shop and a roundhouse. The company’s mechanical department needed to select the best layout design from eight alternatives. The objective was to identify the layout that maximized the throughput of locomotive processing.  Solution  This project utilized the special AnyLogic Rail Library to build a model of the locomotive shop and test the different designs.  In the model, 72% of the incoming locomotives went to the roundhouse, while 22% went to the maintenance shop. The remaining 6% could go to either of them, depending on the problem they had after further inspection. Service times in both shops differed.  Locomotives moved at five miles per hour in the system. There was one common queue, with nine spots, for both shops. A locomotive was pulled into the system if there was a spot available in the roundhouse, the maintenance shop, or the common queue. The numbers of spots available in both shops and in the queue were parameters that could be varied by the user.  Outcome The model was used by the mechanical department to test their assumptions by experimenting with the system, and as a decision support tool to determine which layout configuration was best. The model helped the specialists drive the conversation among the stakeholders and base their solution on the reliable data.  Network Performance Emulator  Rail Network Emulator  The company faced greater than expected demand growth, coupled with hard winter weather and resource constraints, which led to congestion on the northern tier of the CSX network. When they analyzed this problematic situation afterwards, the Network Planning team was trying to determine what happened on the network and could avoid these issues in the future.  As the research continued, they found out that it would be easier to understand the processes if they replaced the traditional analytical methods with a visual emulator. So, they decided to reproduce, or replay, the past system behavior in AnyLogic with the use of animation on a GIS map to better understand density, flow, and congestion processes in the network and improve decision making. All of the train movement data was imported to AnyLogic from the databases, predefining the behavior of the trains in the model. The emulator included the animated train movement with statistics and indicators making the data visually understandable.  The model was presented to the C-level officers and the customers and helped drastically raise the understanding of the issue among the stakeholders.  Watch the video of Jeremiah Dirnberger from CSX presenting these case studies at the AnyLogic Conference 2014:      Call Center Optimization and Investment Planning Using Simulation Modeling Link:  Tags: Business Processes, Asset Management            The world’s largest companies use data analytics to keep up with the changing business world. But how does data science relate to simulation modeling, and what are the cases for the implementation of this interaction, primarily concerning value for the business? The United Services Automobile Association (USAA), a Fortune 500 group of companies, has answered these questions with real-life solutions improving their investment policies and call center performance.   Getting business value through modeling            To make the right decision and benefit from it, executives need to answer the following questions, specific to their business:    What are the company’s options in terms of money investment?  Where is the breaking point/capacity limit of the business? What are the likely outcomes/business impacts if certain actions/decisions are considered? Is the business agile enough to handle sudden shifts?          Methodologies such as data-mining or machine learning do not respond to these questions. USAA analysists found the answer in AnyLogic simulation modeling. It goes beyond analytical modeling, and combines business processes with assumptions that analysts make. Simulation modeling is used to visualize system behavior, processes inside the system, and their aftermaths, and prescribe a solution. This approach explains why the system will act in a certain manner and explores a wide range of outcomes.   Case #1: Call Center Optimization Problem           USAA owns large call centers with highly complex infrastructures and sophisticated process management. The USAA representatives wanted to create a call center simulation model in order to optimize the headcount and the scheduling and routing of calls, by using aggregated data. These steps were aimed at improving call center overall performance and customer satisfaction rates, as well as lowering the abandonment rate.           Call center simulation models are quite widespread. However, because of deficiencies in utilized approaches, these models were neglected. Some of the deficiencies are listed below:    Low granularity in terms of time and groups of people Missing effects of abandonment behavior and multi-skilled call center representatives Very simplistic routing strategy Inability to take into account call and sales rep attributes, and attribute-based routing  The developers accounted for these pequliarities in the new model. Solution and Outcome            The AnyLogic call center simulation model represented the incorporation of calls, call center representatives and their skills, routing and abandonments in detail. The insights which were gained from call center simulation and optimization acted as a basis for some process improvement ideas. For example, the customer service index rose significantly due to reduced wait time, while the abandonment rate dropped down, which increased the revenue. Due to the improvement of the call center business processes, it became possible to cut hiring and training costs.            The company has been using this call center simulation model for several years, and is still using it, applying modifications to reflect the changing environment. Representing the stand-alone contact center, the model can be expanded in the future into the entire call center eco-system.   Case #2: Investment Planning Problem:           Companies are trying to plan investments, while facing the problems of prioritization and placing them on an annual roadmap. USAA challenged these issues with the AnyLogic simulation tool and created a model on how the investments could be prioritized.   Solution          In the model, the capability roadmap visualized possible investment plans, while interdependencies between them exposed costs, benefits, and possible risks of each investment plan.   When a multiple portfolio of investments was created, modelers simulated the operations of the company with these investments and players, and analyzed what the expenses, revenue, and profitability in the long-term period would look like, and which resources might be under stress. Outcome            The AnyLogic simulation modeling and investment planning with AnyLogic software helped reduce operational risks and find out where these risks might surface. It also enabled USAA to see the benefits of each investment plan and see the prospects of each plan in a 12-15 year period. This strategy provided the company with a roadmap to follow, and facilitated performing proactive mitigation strategies.      Capacity Optimization for World Cup Railway Stations Link:  Tags: Rail Logistics, Passenger Terminals  Problem  Railway simulation software (click to enlarge)  The 2018 FIFA World Cup is in Russia, and in preparation for the event the extensive rail network of the enormous country had to prepare for increased traffic. A railway operator in Russia, conducted an event preparedness assessment involving 31 railway stations across the eleven host cities of: Moscow, St. Petersburg, Kazan, Sochi, Saransk, Samara, Rostov-on-Don, Volgograd, Kaliningrad, Nizhny Novgorod, and Yekaterinburg. The project was contracted out to IRTS, a company specializing in the development of transport systems, and consisted of four tasks:  Analyze the capacity of railway stations under normal and tournament conditions. Identify station bottlenecks during increased passenger traffic. Develop recommendations for optimizing passenger services at stations during the tournament. Optimize evacuation routes in railway station buildings.  Railway simulation animation   Solution To solve these problems, IRTS used AnyLogic simulation modeling. A decision that allowed them to develop a custom Station Modeling Library for modeling the stations and common station elements. This library reduces the time needed to create computer models of the network's assets by providing ready objects such as platforms, waiting areas, and services.  Drawing the pedestrian area   IRTS created models for stations in each host city and, in the first instance, simulated the normal operation of suburban and long-distance trains. Added to this, the station models were individually calibrated to reflect current station operations and customer behavior characteristics. The next stage of modeling required loading the train schedules that would be used during the tournament and also the expected passenger numbers. With this data loaded, the bottlenecks at the stations were identified and, through testing, a set of measures was developed to enable the optimal operation of the facilities during special events. Finally, with the detailed models and plans of operation, IRTS analyzed the evacuation plans for the station buildings. The pedestrian modeling considered different behavior characteristics, such as people with or without luggage, people in cafes, waiting rooms, and platforms, as well as varying congestion during the day. The peak load for the stations was taken as the base level for the evacuation scenarios and the time for evacuating the buildings was analyzed. Part of the functionality of AnyLogic enabled stand-alone models to be created and downloaded. These are fully interactive, with a user interface, so the customer can conduct experiments without the need for AnyLogic itself. Outcome IRTS provided a successful solution that is both efficient and flexible. Operational planning for railway stations in use during the FIFA World Cup 2018 was completed by creating a system with a great number of variable parameters: timetable adjustments, train type changes, carriage numbers, passenger flows, and more. For special events to emergency planning, it is a system that will be used long after the FIFA World Cup has come to a close. In the future, the station models will continue to be developed by the railway operator. The AnyLogic models allow editing and configuration direct from the simulation window, for easier and wider use, without the need to enter developer mode. The commercial potential of developments can be analyzed, the impact of changes on the surrounding area modeled, and the safety of the station users protected, all helping ensure the continued efficient running of the rail network.    Changing Facility Layout Design to Increase Throughput in a Steel Service Center Link:  Tags: Supply Chains, Manufacturing  Overview  TBS Consulting helped to increase steel service center throughput by 1.5x at NLMK’s MSC facility in Manage, Belgium. The throughput increase resulted from using a simulation model to analyze facility layout and modifications. This case study details how the companies worked together on the solution.    TBS Consulting Ltd  has been a partner of AnyLogic since 2013. As specialists in the FMCG, steel, oil, and gas industries, they provide related management, logistics, and IT consulting. TBS have delivered more than 100 projects and regularly use simulation modeling in their solutions. MSC selected TBS to develop a specification and implement a model as part of their steel service center modernization project.   MSC provides a wide range of strip steel services in Europe. They focus on hot-rolled steels, pickled steels, cold-rolled steels, and galvanized steels for furniture, heavy & light metallic frames, and the automotive industries. Most of the steel is supplied by road with just-in-time deliveries. The center processes over 200,000 tons per year.  Problem  MSC planned to implement a set of modernization activities to increase throughput at their facility by 1.5x. Changing the layout of the facility could increase throughput, and engineers identified several considerations for analysis:   The installation or relocation of production lines Crane additions Scale locations Conveyor installations The opening of additional gates   Many interrelated factors might affect throughput performance, but any reconfiguration should fit the facility’s constraints and provide the desired throughput.   The facility’s territory is limited by a railway, a canal, and private property, leaving little space to fit a new production line. Limited space is also a problem for trucks arriving for finished goods, which must take special precautions to avoid collisions.   The loading of coils onto the production line requires one of two existing powerful cranes. And, in the place where these cranes move, space is required for the unloading of the finished steel coils from the line.   An additional production line would take a lot of extra space in the facility and would decrease storage capacity for finished goods before the shipment. As a result there could be an impact on export activities.   Any solution should also consider seasonal variations in client demand and the supply of coils, as well as high

 

Section 10 of 60
levels of variability in daily truck arrivals.  Solution   In the first phase of the project, TBS and MSC agreed on a detailed technical specification for a model that covered the whole production cycle. Activities captured by the model ranged from the arrival of coils at the facility, through production and storage, to the shipping of finished products. The model would provide an overall site view, as well as detailed operational data, such as for operators, cranes, and production lines.   The technical specification was very important because it formed the foundation of the project development, determining the inputs, outputs, and statistical distributions for use in the model, as well as the agreed user requirements.    The model accepted static, historical, and interactive data inputs (click to enlarge)  Following the specification, the project went through three further stages of development:   Construction of an as-is model, representing the facility as it was Experiments and analysis of plans Delivery of a tool for scenario analysis and day-to-day planning  The as-is model represented the facility as it was, and its accuracy was verified by running simulations with historical data. The model could then be used to test various scenarios and compare their results. Finally, by implementing easy user input, the tool could be used for planning, such as for organizing day-to-day operations or analyzing year-long scheduling.  The simulation model supported analysis and helped build insight through allowing various experiment types. For example, the 2D and 3D visualizations let engineers verify that the model was accounting for considerations that may have not been captured in the model logic.  In the case of crane control, the standard overhead crane included in  Material Handling Library does not account for operator movement – an operator controls an overhead crane from a mobile remote control. AnyLogic’s customizability meant the designers could easily specify that a crane’s operator must follow certain paths to comply with safety regulations, while the crane load would move according to its own operational path.  Accounting for differences in the crane operator and crane paths gave time constraint information critical in helping identify bottlenecks.    Charts showing resource utilization and throughput in the steel service center (click to enlarge)  Experiments without visualization, however, permitted faster experiments, such a mass runs using parameter variation.  Overall, the model collected detailed data on each operation, such as  coil or sheet movements, truck arrival and departure, coil processing, and operator actions.  Performance indicators included data for total weight produced, total weight shipped, resource utilization, and more. All the information from simulation runs could also be exported for further analysis in Excel.  TBS implemented the model for MSC using AnyLogic simulation software. Simulation modeling provides an important method of analysis that is easily verified, communicated, and understood. It gives all stakeholders in a project clear insights into complex systems.   Results The model helps to identify most critical resources and affiliated risks and can check the effectiveness of decisions (e.g. different placement policy or adding a new crane). Also, it enables finding bottlenecks for truck operations.  Simulation modeling helped the MSC management team to select optimal configurations for the facility and confirm their productivity. Their analysis showed that the installation of the new CTL production line will significantly improve the ecological factors in line with NLMK’s general strategy.  Analysis of crane and gate efficiency showed that one crane was causing a bottleneck, with an average utilization of over 70%, and periods when trucks were left waiting.  Accounting for differences in the crane operator and crane paths gave time constraint information critical in helping identify bottlenecks.    Chart illustrating crane and gate utilization. Results identified a bottleneck with gate number K00 (click to enlarge)  Due to seasonal variations, steel service center bottlenecks were shown to cause a small percentage of incoming trucks to wait so long that they would miss their next scheduled appointment. Although only a small number, these delays would have knock-on effects that could cause further disruption.    Example of truck data analysis (click to enlarge)  After the initial results, management is now looking to use the model for daily operations planning, as well as optimizing arrival plans and coil storage to decrease the number of extra reshuffling operations.  In the future, the intention is to fine-tune the storage policies, work schedules, and equipment selection. And finally, the model is available for experiments and analysis in the case of extreme conditions.  You can learn more about the project in the presentation video from the AnyLogic Conference 2021.      Chelyabinsk Metallurgical Plant Uses a Simulation Model of Electric-Furnace Melting Shop Link:  Tags: Manufacturing    The model simulates the redesigned ground environment and production processes of the shop floor to be renovated. This is the first production simulation modeling project experienced by Chelyabinsk Metallurgical Plant (CHMK), part of the global mining and metal manufacturing company Mechel group. The shop floor model was created by Russian simulation software developers The AnyLogic Company.     The simulation modeling technology can be utilized by mining production companies to find the exact solutions for challenges which previously had only provisional, not foreseeable, results. Experimenting with the simulation model significantly reduced the expenses in the plant modernization process. It also helped to define the production capacity of new equipment in different conditions and for various kinds of steel and production modes.     The electric-furnace melting shop model is represented as a Java application that simulates operations on the shop floor within different time slots (from 12 hours to 12 months). The model includes the new shop environment, equipment of different configurations, and the required options of the production’s processes. The results of the experiments are presented in Excel format reports on production capacity, equipment outage, etc.  The simulation model provided the solution which enabled the enterprise to increase the output of high-quality rolled products and rails.       Choosing the Best Concept for a Passenger Rail Car Link:  Tags: Passenger Terminals  Problem The largest CIS rail car building facility, Tver Carriage Works, needed to choose the best passenger rail car concept for a city train. The plant needed to compare passenger entrance/exit times during peak loads for concepts that had different door amounts and widths, and then choose the best variant. They contracted ITS Consulting to perform this task. The consultants found pedestrian simulation modeling to be the best possible instrument to provide a solution. Solution     Three-door rail car seen from side and from the top   Five rail car concepts were provided as input data. To choose the best one, the consultants had to compare them using the following criteria:   Full car filling time Evacuation time Entrance/exit times at stations Average exit path length  Car population density   To simulate passenger movements, the modelers employed AnyLogic Pedestrian Library, together with 2D and 3D animation, which allowed for visualization of the process.  To make the model realistic, they divided passengers into several categories (regular passenger, elderly passenger, and passenger with luggage), each of which had its own specific behavior, including a “give up a seat” scenario.  Passengers in the model entered and exited cars in a stochastic order, similar to the real world conditions of public transportation. The modelers used average 50-runs statistics for analysis.   Result Thanks to the density map functionality, the model showed that using 3-door concepts resulted in a more effective use of space between doors in comparison to the other variants.  Pedestrian simulation modeling showed that the concept with three 1400 millimeter-wide doors provided minimal entrance/exit and evacuation time. Increased free space inside 3-door cars allowed for more effective and comfortable transportation during peak loads, while providing enough seats for passengers during non-peak periods.  The results were used for the design of the new Tver Carriage Works’ EG2Tv electric multiple unit.     Choosing the Right Warehouse Layout for a Leading FMCG Retailer Link:  Tags: Warehouse Operations  Problem One of the biggest FMCG retailers in Eastern Europe supplies to several hundred shops, across multiple regions, through a single distribution center. The company planned to change the arrangement of pallet racks and conveyors in the center. To evaluate the capacity of the new layout and to measure the effectiveness of the warehouse, they decided to have the AnyLogic Company build a simulation model of the warehouse.      Warehouse layout simulated in AnyLogic  Solution The warehouse model simulated all of the main processes happening in the center, including:   Vehicle loading and unloading Incoming and outgoing article tracking  Article placement in storage positions Article movement from buffer storage areas to order packaging areas Order packaging Packaged order sorting by pallet  The model used the following as the input data: staff numbers and work schedules, plans for receiving and sending goods, operations’ timing, and probabilities of various events, including equipment breakdowns. Using this data, the simulation evaluated the quality of service, equipment and staff workload by type and position, and also calculated the warehouse used capacity and operational costs. The model was notable due to its size and high detail because the distribution center, with an area of more than 50 thousand square meters, serves several hundred shops of the company across multiple regions.  Simulated warehouse business processes    The model took into account the individual characteristics of the thousands of articles and dozens of thousands of storage bins, all the cartons in the warehouse, the actions with each article, and every incoming and outgoing order.  Result The simulation model gave the client the ability to test the new pallet rack and conveyor arrangement at different load levels. It also analyzed the correlation between the numbers of staff and equipment and the warehouse capacity. This made it possible to use the model in operational management for staffing needs based on the plans for warehouse loads at certain periods of time.  Additionally, the client uses the model for personnel training. The animation allows the management to introduce new employees to the business processes in the warehouse.     Coal Loading Port Optimization Link:  Tags: Ports & Terminals   TAKRAF GmbH is a global German industrial manufacturer and supplier that specializes in planning and constructing facilities for the mining industry, including coal loading ports.    SimPlan AG is a leading German simulation service provider with expertise in material flow simulation and the creation of decision support software tools.   Problem        Demand at a coal loading port was forecast to grow 33% and exceed the facility’s operating capacity. To meet this expected increase in demand, the port’s operators first sought the expertise of TAKRAF GmbH and SimPlan AG to determine the required works.           It was difficult to forecast and plan extensions because operations at the port were influenced by numerous uncertainties. Coal is delivered from different mines and transferred to many different ships for delivery. Weather conditions, train delays, and differing types of coal also meant it was difficult for the port to modernize using traditional methods.           The aims of the port optimization project were:    to determine the current maximum capacity of the facility; to identify the main bottlenecks that limit the capacity; to test and evaluate different extension scenarios in a risk-free digital environment.          The main issue the port operating company needed to understand was how to reorganize the heap storage area to increase its capacity. Therefore, the modeling of this area and how to make the storage logic efficient were the key elements of the simulation project.   Solution        For the port optimization project, SimPlan engineers created a model of the coal loading port using different modeling techniques. They chose AnyLogic because of its multimethod modeling environment and special industry libraries.        One of these libraries, the AnyLogic Rail Library, enables the easy modelling and visualization of large and complex rail yards and rail transportation. The developers used the library to model port’s railyard and tipping stations. Furthermore, for the coal conveyor modeling, they used elements of the Fluid Library, to simulate the logistics of moving bulk materials and fluids. Finally, the modelling of stacker-reclaimers and ship loaders made use of the agent-based simulation capabilities of AnyLogic.       SimPlan’s engineers managed to capture all the processes in the port – from delivery to shipping:     coal is delivered from different mines (one type of coal in each wagon) and loaded from wagons to conveyors by tipping stations; stacker-reclaimers take the coal from conveyors and stack it at the dedicated heap area by type; after a ship has docked, a stacker-reclaimer takes coal from the heap area and loads it onto the conveyors that transport coal to ships; ship loaders load coal into ship hatches (one type of coal in each hatch, the sequence of hatches is predefined); ships leave the port area when loading and documentation are completed.  Storage of different types of coal (different colours) in heaps reflected in the port optimization model        A storage control system was used to determine and allocate areas for storing coal at the port. Using simulation modeling, the engineers developed logic which would suggest an efficient coal storage solution to increase storage and reclaim capacity. The main storing strategy focused on building as large a heap of each coal type as possible.    Result        The simulation model for port optimization replicated all the stages of coal handling in the coal loading port. The decision support tool created by SimPlan helped the company management determine the maximum capacity of the facility in its current state. Initially, there was an assumption that the capacity of the port was not being used effectively.            After analysis with the tool and its simulation model, the company were able to identify strategies to enhance the port’s capacity. As a result, they

 

Section 11 of 60
chose to:    make heaps of high demand coal types accessible to several stacker-reclaimers simultaneously; arrange the heaps so that the loading of ships doesn’t obstruct the work of the tipping stations; presort wagons and plan to directly load ships in order to free more storage space.          Based on the simulation modeling results from the decision support tool, the management decided to also develop a port extension strategy that would enable them to reach the port’s target throughput. This included adding a new tipping station of lower capacity and new storage area.           The port optimization project highlighted the efficiency of using AnyLogic models to increase the capacity and throughput of facilities such as ports. The model could also be modified for similar sites with different layouts.      Cold Store Warehouse Operations Optimization  Link:  Tags: Warehouse Operations    Phoenix Analytics is a Turkish consulting company and provider of simulation solutions for various industries. They were involved in developing a warehouse optimization project for one of the biggest ice-cream and frozen desserts manufacturers in Europe and Asia.    Problem   Phoenix Analytics engineers needed to create a simulation model of a cold store to help decision-makers for an ice cream manufacturer understand how to more effectively use resources.    The warehouse stores frozen products that require specific storage area temperature conditions. Temperatures in the storage area are affected by storage related activities, such as putaway and picking. For these areas of activity, three types of warehouse operators offered opportunities for optimization:    Runner (click to enlarge)   Forklifts Collectors Runners  Engineers also had to follow several restrictions and rules while creating a model:    First in First Out (FIFO) product storage All rack depths to accommodate the same product types Waiting time limits for pallets in the loading area Limits on the number of pallets in the loading area Limits on the number of pallets in the container area  Solution   Resource classification (click to enlarge)  Engineers used AnyLogic as warehouse simulation software. They developed a model that allowed flexible usage of various scenarios and the changing of cold store parameters depending on the season, rack load, staff shifts, etc.   Forklifts and collectors were classified as inbound and outbound resources. The priority for inbound and outbound dock usage could be easily changed to see the consequences and the impact of each setting.   In the model, the developers implemented a heat map color indication system for the warehouse racks. The heat map could be set up to present different information. Set to present for product type, the heat map is useful for analyzing the location of each product type for inbound and outbound operations. Setting the heat map to show the number of pickings makes it possible to monitor the racks with the most or least demand.   Considering numerous parameters, the warehouse optimization model determined the shortest travel distance for the forklifts and the collectors and showed where best to store pallets. It also showed the current state of fulfillment, the current state of resource utilization, and other statistics that help stakeholders in decision-making.  Storing and picking pallets using runners (click to enlarge)  For a simulation run, engineers could use real data from different periods of operation to analyze performance.   Result   Summary of simulation results (click to enlarge)  The resulting statistics from a simulation experiment with the model include whether a scenario is successful or not for different combinations of resources as well as resource utilization. It is possible to see when service levels and KPI will not be met and to evaluate how different combinations of resources might perform.    The configuration that used the least resources while achieving the desired performance levels was chosen by the clients for their warehouse optimization needs.   The case study was presented by Ali Yoğuran, of Phoenix Analytics, at the AnyLogic Conference 2021.   The slides are available as a PDF >>      Construction Simulation Model Tackling Increased Constraints on a Complex Earthmoving Project Link:  Tags: Road Traffic, Asset Management  Overview Consolidated Contractors Company (CCC) is the largest construction company in the Middle East and ranks #18 internationally. CCC has offices and projects in over 40 countries, and a workforce of more than 130,000 employees. Its portfolio includes oil and gas plants, refineries and petrochemical facilities, pipelines, power and desalination plants, light industries, water and sewage treatment plants, airports and seaports, heavy civil works, dams, reservoirs and distribution systems, road networks and skyscrapers. CCT International (CCT) is the primary technology provider for CCC since 1998. CCT also develops and markets solutions for the construction industry with clients all over the world. CCT products include 3D BIM/project control environments, document and content management solutions, mobile construction management systems, asset management and fleet management systems, QA/QC systems, and construction process simulation and optimization tools. CCT has offices in Beirut, Dubai, Cairo, Athens, and Cyprus with a headcount of around 65+ engineers, software developers, and industry experts. Problem Just after contract signature of a site preparation project (earthmoving scope of approximately four million cubic meters), the client and local authorities placed new, more restrictive constraints on the operation. These constraints included:  Trucks were now allowed to run at a maximum of 10kph within the construction project site instead of the original 20kph. Trucks from/to dump location were instructed to follow a specific route full of traffic lights, intersections, roundabouts and security gates. Accordingly, the original assumption of an average of 40kmph truck speed on route from and to dump site could not be maintained. Truck sizes/loads were brought down from a maximum allowable of 32m3 to 15m3.  Number of truck trips restricted to a maximum of 100/hr. Only one of the original four site access points was granted after contract signature thus restricting all traffic on site to one gate. Only one work shift (10hrs/day) was allowed for dumping at the dump site in contrast to the original two-shift (20hrs/day) schedule.   The newly placed constraints essentially meant that the project schedule will be severely impacted and with it the total cost to complete. The number and complexity of the constraints made it quite difficult to evaluate manually the impact of the constraints on time to complete and equipment requirements. As a result of the newly added complexity the simulation team was asked to help in quantifying the impact of the new constraints and substantiating a claim by CCC for an extension of time.   Truck Route Model Outputs Showing Route Visualization and                  Average Speeds  Solution CCC already had a well-established construction simulation model made in AnyLogiс software. This model was used for basic construction optimization: to forecast equipment and time requirements for earthworks operations. Using the earthworks simulator requires multiple parameter inputs, including the assumed average speed the trucks will travel on their haul routes and back. With a new truck route imposed on the project, it was very difficult to manually estimate the average speed the trucks would run at.  As such, a more complex construction simulation model was built in AnyLogic software to mimic the trucks traversing the route segments, both while loaded and empty. Using optimization techniques in AnyLogic was helpful for risk-free experimentation and for effective construction management on this project. In the construction simulation model, each route segment was modeled with a stochastic distribution for the total time to traverse the segment. Then the trucks were made to run 10,000 times each way in the simulator to arrive at an average speed for each route and loaded/empty combination.  Why use simulation in construction? AnyLogic was specifically selected for construction modeling because this software allowed CCT to:  Very quickly build a construction optimization model representing the route and its segments Add a map of the routes and superimpose an animation of the trucks traversing the routes to make it easier to explain the work visually to stakeholders.  The average speeds deduced from the truck route simulator were then fed into the earthworks construction simulation model along with the remainder of the new constraints. These included:  Truckload sizes and number of work shifts (working hours) per day to produce multiple scenarios showing the original forecast time and equipment requirements to complete the operations. The current forecast time and equipment requirements to complete the operations (impact quantification in time and resource requirements). Proposed mitigation scenario.  Outcome The results of the two-step process of using the truck route simulator to summarize the route and then feeding it into the earthworks simulator enabled CCT to quantify very quickly the impact of the new more restrictive constraints and to build mitigation scenarios to aid in the claim for extension of time. Both of the construction simulation models were created in AnyLogic software, giving them the flexibility to consider all the new constraints. AnyLogic optimization techniques have proven to be good assistants for construction management. Based on the construction simulation model results, the client agreed to extend the total duration of the earthworks operation by an additional 50% on top of the original schedule duration, and to allow two work shifts per day. This essentially saved the project an estimated additional cost equivalent to 18% of the original total contract value.    Container Terminal Throughput Evaluation with AnyLogic Link:  Tags: Ports & Terminals  Context            Port authorities aim to maximise container (TEU) throughput whilst minimising rehandling.           Major upgrades to the terminal can improve TEU throughput, but improvements may be obtained with less expenditure through analysis of work methods - in particular the layout and height of TEU stacks on the berth.           Key decision to be made: What is the optimal stack configuration that allows increased throughput without increasing the rehandling of TEUs?   Modeling Approach            Evans & Peck utilised a discrete event simulation modelling approach to demonstrate the trade-off between increased TEU throughput and increased rehandling from increased stack heights.           Through the use of an object-orientated approach, individual container tracking is possible. The overall functionality of the port is assessed by measuring utilisation of the berth and the staddle carrier fleet.           The same decision modelling method can be applied to analyse various layout and configuration options for intermodal container terminals (ie. rail to road).   Model developed and published by Evans & Peck Australia.     Container Yard Planning and Management System Built with AnyLogic Simulation Link:  Tags: Ports & Terminals  Problem The volume of cargo traffic in Russian ports quadrupled during the period from 2000 to 2016. The growth of cargo traffic led to the need to build new modern terminals. Another challenge, in this regard, was the choice between the creation of new terminals and the modernization of the existing ones. This choice would depend largely on the quality of pre-design solutions, which would also allow a reduction in facility design costs, as well as avoid significant material costs for further operation. The managing company of Novorossiysk Sea Port planned to expand the existing trading terminal, so that, in the future, its capacity would increase to 1.2 million tons per year. For project implementation, the managing company hired an engineering company, Morstroytechnologiya, to devise maritime container yard planning.  Simulation model for maritime container yard planning (click to enlarge)   When developing the container terminal simulation models, it is necessary to determine the required technical characteristics of the cargo terminal, including capacity and throughput of warehouses and cargo fronts, as well as the terminal’s specific characteristics (for example, the average number of storage tiers for a container terminal). These parameters are determined according to the volume and annual cargo traffic structure. Previously, analytical methods were used at the maritime container terminal planning stage, but these methods have significant limitations. First, they embrace only the basic probability distributions and define only average indicators at the output. Probability distributions do not include the option of working with the ship arrival schedule, and they do not allow for the consideration of random factors like equipment failure, local violation of the ship arrival schedule, delays within the time frame, and bad weather conditions. Secondly, they do not offer tools to assess resource distribution among transport operations and, therefore, to determine the optimal number of vehicles. Thus, the irregularities in the ship arrival schedule and consignment volume and quantities lead to uneven cargo distribution inside the warehouse. If the warehouses are stocked up, the overall time of goods collection increases because it imposes additional requirements on the transport and logistics inside the terminal. Container terminal simulation modeling allows users to consider these shortcomings and make more accurate and feasible calculations. Therefore, to create container terminal simulation model, Morstroytechnologiya sought advice from consultants. First of all, the consultants needed to coordinate the high-quality service of cargo traffic with the efficient use of terminal capacities. To do this, they had to:  Determine the technical characteristics of objects inside the terminal. Determine the optimal local logistics within the terminal. Include in the model the structure and content of the cargo traffic. Check the efficiency of the proposed solutions.  For these purposes, they applied AnyLogic simulation modeling. Solution When creating the container port simulation model, the following structural elements of the terminal were specified:  Sea Cargo Front (MGF) Railway Cargo Front (ZHF) Motor Vehicle Front (AGF) Warehouse Ports of entry ZHF buffer zone  The consultants set the following parameters for the model:  The schedules for external vehicles movement on cargo fronts. The container cargo traffic structure on shipping lines, divided into imports/exports and by type of containers. The intraport logistics

 

Section 12 of 60
parameters - the model allowed them to specify the permitted lengths of queues and to consider various warehouse layouts, the amount of storage equipment, work intensity, storage area, and height.  The container terminal simulation in the AnyLogic environment contained another advantage, which was the additional option of entering data using MS Excel tables (Access). This granted them the possibility to work not only with simple numerical parameters (vehicles’ movement speed), but also with complex data structures (ship arrival schedule, cargo volume distribution). When working with the maritime container yard planning model, the specialists could observe a change in a number of dynamic parameters:  Export and import cargo traffic volume Container storage tiers Time of transport stay at the terminal Degree of technological equipment usage Mainline transport service waiting time Main and intraport transport queues’ lengths  The simulation results were presented in three formats: graphic, text, and chart, which was another advantage. The use of temporal diagrams and histograms helped to better visualize the results. The results can also be saved to external files.  Parameters of container terminal simulation model (click to enlarge)  Container terminal simulation model outputs (click to enlarge)   Results After running the model and observing a series of experiments, the specialists succeeded in:  Detecting and identifying the shortcomings of the current terminal layout (in particular, the "dipping" container effect at multi-tiered storage). Proposing the algorithms for elimination of these shortcomings (for example, the optimal storage height for various storage systems). Clarifying some technical parameters of the terminal. Getting an insight into the process behavior during random and non-random changes in the cargo traffic.  Simulation that acts as a container port simulator and best matches real processes allows users to make well-grounded decisions about internal objects placement within the terminal. Running a model helps them choose the most optimal configuration that will significantly increase the terminal's pass-through capacity, and, therefore, lead to an increase in turnover and improvement of service quality. Since container terminal simulation met the expectations of the customer and made it possible to solve a number of tasks on facility optimization, the specialists of Morstroytekhnologiya decided to design a universal model of the sea terminal. It can be used as a technological base for creating simulations of other types of terminals (liquid, bulk, etc.). The structure and degree of detailing may vary depending on the needs of a particular project.    Creating Optimal Store Layouts and Improving Labor Scheduling Link:  Tags: Business Processes  Domino’s is a well-known brand and is the number one pizza company in Australia and New Zealand. Across the Asia Pacific and Europe, Domino’s Pizza Enterprises (DPE) is the largest master franchisee of the Domino’s brand.  Problem Domino’s has been growing very quickly and opening a lot of new stores. When opening a new store, a number of factors must be considered, for example, the right location, the layout of the store, and later labor scheduling.  So, there are a number of steps or activities that they need to do, including:   Working with the operations team to determine the kind of products they offer and the processes involved in providing those items. Then modifying the model taking those factors into account.  Spending a lot of time in the actual store under evaluation, watching and interacting with employees. If they are not able to travel, they use video footage.  Using time and motion studies to determine the different preparation steps and their durations.  Accessing a lot of historical data in order to understand the problem exactly.  Engaging with different construction and equipment specialists.   All of this helps to make the model more accurate.  Solution A digital twin environment needed to be established in order to test new concepts, store processes, and ideal layouts before they were implemented in the actual physical layouts of a store.  Simulation to improve store layout The first step when starting modeling is taking an actual store floor plan and converting it into an equivalent AnyLogic representation.  Domino’s identified three components to focus on in order to develop a simulation model to improve their stores:   Physical store layout – the dimensions and shape of the floor space as well as the location of the store equipment. This needs to be converted into an AnyLogic representation and then run as a simulation.  Process flows – this is essentially all the underlying logic that determines how the agents will act in the simulation environment.  Order and staffing data – all the information on orders and personnel, as well as their roles that aid in delivering those orders.    Typical Domino’s store layout: floor plan to simulation with a focus on critical workstations that are important during peak times   The focus is on modeling for peak times and how to improve store efficiency during those times. The objectives are to design a layout that works best for any given store and to increase that store’s efficiency in relation to the layout.  An initial and important question was which floor space works best – square or rectangular? Through simulation it was discovered that square functions better because it allows for more design possibilities and more range of movement for the staff. A rectangular floor, on the other hand, requires more labor, higher order times, and results in lower customer satisfaction. Therefore, the idea was to focus on more compact and proportional floor spaces.  The next step was to optimize the layout of the store. The goal was to ensure that all staff members are moving in a more streamlined way. In essence, the objective is to minimize the number of steps necessary to complete an order while ensuring that the workers don’t obstruct one another.    Streamlined movement in a modeled store   In the illustration above on the left, the driver and makeline staff intersect and get in each other’s way and during peak times this can significantly reduce efficiency. The arrows illustrate the routes taken and they are unnecessarily long.  The alternate arrangement on the right was developed using AnyLogic simulation with no intersection of driver and makeline staff. The number of steps of both were reduced and the product could be delivered much quicker. This is an objective in all stores.  Some of the designs that have been created through simulation have already been turned into actual stores as can be seen below. This illustration shows a store in Australia that was damaged by flooding and needed to be rebuilt. A simulation was then created, a store plan developed, and now the store is currently being built.    A simulation turned into a reality in Australia   Simulation for staff scheduling A store, of course, also needs staff and determining the best labor scheduling for a particular design layout was the last task. Both understaffing and overstaffing have their own particular problems, but through AnyLogic simulation, the right number of staff on a given shift, for a particular layout, could be determined.  In one instance, at a new store in Australia, the builders came up with one layout (the baseline) and the model developers created two alternative plans. These designs were compared to show the efficiency. The outcomes of this analysis are shown in the illustration below.  Adding drivers reduces delivery time, but there comes a point when doing so offers no further advantage. The ideal spot is between 6 and 7 drivers for all 3 layouts. The decision maker can then select which layout they want to go with and where they wish to operate on this staffing curve.  A key insight from this store simulation was that Domino’s was able to identify efficiencies and reduce dispatch times for their products by 4.5%.    Store efficiency vs store layout from a store in Australia   Results Some of the results have already been illustrated above, but in summary, the following can be gained from this case study:   The makeline, oven, and cold room should be arranged close together in a triangular formation to allow the makeline staff to easily move around efficiently.  The production of food in front of customers, known as "food theatre," may be the root cause of sub-optimal layouts.  Smaller, more compact floor plans can overcome poor design.  For better layouts, proportional floor space is paramount (square is better than rectangular).   What-if analysis is continuously being worked on and is taking on a lot more significance. What would happen if a process changed, and would the store layout, for instance, need to alter to accommodate such changes?   The case study was presented by David Federer and Dr. Shelvin Chand, Domino’s Pizza, at the AnyLogic Conference 2022.  The slides are available as a PDF.     Customer-Centric Transportation Network Modelling Link:  Tags: Transportation, Rail Logistics  Problem The sphere of public transportation services in Australia is undergoing a transformation in response to a change in demographics that requires inter-modal integration and major infrastructure investment by the Federal government. To better address new challenges, public transport companies need to understand the behavior of their networks from the customer’s point of view. Widespread use of smart cards at public transport allows them to collect the information needed to conduct such research. The public transportation company employed PwC Australia to develop a solution that could provide a customer-centric view of their railway infrastructure and help the company understand the current incident effects on rail network operations and how to improve the situation. Specifically, the company wanted to:  Understand potential number of customers impacted by an incident (e.g. train derailment, motor breakdown, medical emergency).  Take a high-level network view of incidents in order to understand network behavior when they occur.  Provide customers with more accurate predictions of incident-related delays depending on where they are on the network.  Support operational and maintenance decisions concerning incident responses, including planning of predicted response times, allocation of resources, and prioritization of incidents.  Identify specific incidents upon which to perform root cause analysis, e.g. why certain incidents always occur more at a particular location or on a particular type of rolling stock type.  PwC consultants decided to build a model of the transportation network that would simulate train movements, incidents, and customers at stations and in trains.  Solution   Model Animation and Graphs  To build the model, the consultants chose AnyLogic software due to its ability to combine various simulation methods in one model, which was needed to successfully model both train movements (discrete event modeling) and customer behavior (agent-based modeling). The second reason was its scalability: in AnyLogic, it is easy to extend an existing model to adapt it to network development plans and see how the system would work in a new setting.  The input data for the model was obtained from various sources, including the transportation company, government, and publicly available sources, and comprised of:   Network layout (signals, track geometry, stations, and platforms).  Train data (train set types and capacity per carriage).   Timetable (route, train type, number of carriages).  Operating rules for recovering the network and in hot weather, including speed restrictions.   Incident data (types of incidents).  Passenger data (smart card data and existing usage statistics).  Train Graph  Train movement logic was reproduced using AnyLogic Rail Library. Also, it utilized some custom library components created in AnyLogic by the PwC specialists taking into account special aspects of this project. First, the model provided the network view of customers at stations: it showed the number of customers currently waiting at each station on the network (including their direction of travel), and the number of passengers on each train.  What is more important, the model was designed to allow the company to analyze network’s incident recovery behavior and time. If an incident occurs at a railway, it may cause a long-lasting delay in the timetable, especially during rush hours. It may take several hours for the network to fully recover from the incident and for all the trains to start running on schedule after the initial problem is solved. That is why it was essential for the model output to include a Network Incident Graph that clearly showed the length of the effect of each incident on the whole network and enabled the users to test and compare different incident mitigation policies.  The main metric collected was Lost Customer Minutes (LCM), calculated as sum of delay minutes for all individual journeys within a particular train or a network segment. It was important to review LCM in the context of situations when these minutes were lost (e.g. minutes lost during a rush hour and a weekend had different values). The output included the train graph, which is a conventional way of representing train movement in a network (see picture). Moreover, the consultants animated the model using a GIS map to visually present the processes occurring in the system. The train graph and network animation showed:   Train location on the network.  Whether trains were running to the timetable.  Whether trains were able to make return journeys.  Result The model allowed the users to obtain a passenger-centric lost customer minutes calculation, which is more precise than traditional train-centric methods that either seriously over- or underestimate LCM. This passenger-centric approach was possible due to the use of agent-based simulation. The clients were able to measure impact of incidents on the network behavior to test and form policies for more efficient incident mitigation (e.g. setting up emergency teams dispatched at certain locations for quick medical help to minimize delays related to passengers’ health conditions). It also helped plan incident response prioritization policies according to the number of passengers affected. With the help of the simulation model, the users could evaluate their investment and business decisions according to their estimated impact on lost customer minutes. Also, having LCM as the one customer-focused delay metric allowed the transportation company to create customer-focused targets and KPIs within its structure. Future consultants’ works include extending the model with other forms of transportation and future network elements. They also plan to simulate physical movements of passengers as pedestrians at

 

Section 13 of 60
stations to investigate problems of platform crowding.   Video of the project presentation by Artem Parakhine at the AnyLogic Conference 2014     Demand and Supply Planning for a Large Fast Food Chain Link:  Tags: Business Processes  Problem      HAVI is a $5 billion global company and McDonald’s long-time supply chain and packaging partner. They provide services for supply chain management, packaging, logistics, recycling and waste. When McDonald’s wanted to build on the success of the All-Day Breakfast launch by expanding all-day availability to more menu items at 14,000 restaurants, they encountered a number of challenges (menu complexity, new equipment needs, space constraints).   All-Day Breakfast launch plan (click to enlarge)      McDonald’s business objective was to equip and staff its kitchens to obtain the best financial yield possible for the menu expansion. Working with HAVI, a simulation model was created that reflected the enormous complexity of the supply chain and operations across the 14,000 restaurants. The value of the model was to enable more informed decision-making for equipment purchases and staffing.   Solution      HAVI employs an iterative hypothesis-driven process for its simulation and analytics, balancing data with human experience.      To meet McDonald’s requirements, the model considered:    Regional preferences Menu complexity Cooking space An increase in the variety of items being cooked simultaneously Equipment and staffing     Using AnyLogic, these requirements could be met and simulated, along with the spatial constraints and the wide variety of equipment and labor configurations. Decision variables of the simulation model included:    Equipment type Equipment space requirements Labor needs Demand rate Batch size Product mix Store layout Drive-thru demand  Layout of a modeled kitchen (click to enlarge)      On the output side, it was vital to measure the customer experience. As a result, factors such as service time, product freshness, and waste, among other service metrics, were also included in the model.      Finally, according to the rigor of HAVI’s analytics process, the model was subject to validation and calibration, including trials in McDonald’s test kitchen. The resulting model captured the necessary metrics and provided simulation comparable to the real world. In short, the AnyLogic simulation assisted in the decision-making process, providing McDonald’s with the best financial yield for the desired menu expansion.      The power of agent-based modeling in AnyLogic allowed the nature of the system to be captured as it is in the real world. The characteristics and parameters of equipment, labor, and the environment they operate in, can be modeled as necessary and custom objects developed for re-use.      HAVI chose to use AnyLogic simulation due to its support for multiple modeling methods, with agent-based, discrete-event, and system dynamics working together inside one system for the most holistic and powerful results.   Outcome      The AnyLogic model delivered results for a variety of demand profiles and restaurant configurations. This enabled HAVI to provide tailored recommendations.      These recommendations covered equipment needs and cost estimates for meeting customer service level thresholds in various scenarios. The benefits of which produced proposed equipment cost avoidance and optimized cost tradeoffs for labor and equipment.      Without AnyLogic simulation modeling, time and cost constraints associated with exhaustive physical tests would have prevented tailored recommendations.   Project presentation by Nate DeJong, HAVI     Developing Cement Supply Simulation Model to Optimize Scheduling and Vessel Capacity Utilization Link:  Tags: Supply Chains, Transportation, Ports & Terminals  Overview  Cementos Argos is a cement and ready-mix market leader in Colombia and the fourth largest cement producer in the United States. It has over 8,000 employees and $3 billion in revenue.  It operates across 16 countries and includes 13 cement plants, nine grinding facilities, 340 concrete plants, and 33 ports and terminals. Argos provides cement for both the industrial and retail segments. The specialists create value for the customers through innovative products and solutions.  Problem Argos has two bulk carriers and 31 terminals. Each of the terminals has different silo storage capacity, draft and port restrictions, and terminal operations that have been considered in the model.  Argos engineers would model cement supply in Excel, using updated reports for each terminal: Daily Sales, Inventory Report, and Inventory Planning. Moreover, they did the updating and planning every week, or on demand.  Considering the system constraints, the specialists would manually build up a supply plan. These constraints were known by experience rather than modeled in any system. Modeling in Excel gave the visibility of only 15 days from the operations planning to execution.  To improve the visibility and the accuracy of the overall cement supply planning process, Argos opted for simulation modeling in AnyLogic.  For the simulation modeling project, Cementos Argos had three main objectives:   Avoid cement shortages in each market.  Use 100% vessel capacity.  Optimize discharge times to avoid rotation delays or demurrage.   Solution Using AnyLogic, Argos developed and analyzed a simulation model for the supply of bulk cement to the Eastern Caribbean terminals. The model was then exported as a standalone  Java application.   AnyLogic agent-based simulation model    (click to enlarge) Argos’ bulk carriers moved cement from a plant in Cartagena to terminals in Aruba, Dominica, Antigua, St. Maarten, and St. Thomas. These locations were mapped with the GIS Map shape in the simulation model.  On the model settings screen, users could enter the information about terminals, including variables such as initial inventories and projected demand in each market segment. The model also considered such parameters as storage capacity, packing rate, and working hours.  For vessels, model input data contained the following variables: assigned routes, proposed volume split, speed ranges, and departure delays. It also considered vessel parameters including: capacity, and loading and unloading rates.  Alternatively, the simulation model could read the input data from, and export it to, the same Excel file.  The developers used AnyLogic  Fluid Library to simulate vessel loading and unloading as well as cement storing, packing, and dispatching from the terminals. They also used AnyLogic  schedule elements to simulate the terminals’ working hours.    The terminals’ schedule (click to enlarge) The Cementos Argos engineers used an  agent-based simulation approach   and  statecharts  to model the vessels’ behavior. To simulate stochastic parameters (vessel speed, cement demand, etc.) they applied built-in  probability  or  custom distributions . Users could also specify the simulation starting date and simulation time (in hours).    The statecharts of the vessels’ behavior (click to enlarge) The engineers could view the simulation results on charts and in tables.  For the cement terminal, the model outputs included:   Inventory shortages and accumulated time without inventory Final inventory projected for each product  Monthly sales per terminal Daily demand per product  For the vessel, the output data contained:   Arrival and departure times Permanence time Cement inventory report Speed profile Draft monitoring   Model outputs for vessels and terminals (click to enlarge) Results Simulating cement supply operations, Argos extended their visibility from 15 to 30-35 days. The company also created and analyzed different cement supply scenarios using AnyLogic. The model developers could easily test the scenarios, make necessary adjustments, and eventually choose the best option.  Thanks to simulation modeling, vessel monitoring and sales and production updates were carried out monthly, rather than weekly, which saved a lot of engineers’ time.  To measure the model’s performance, Argos engineers defined KPIs and compared the results to the objectives.     Argos KPIs  The results with the simulation KPIs:   No inventory breakdown at the terminals during certain times, causing steady income.  Accomplished vessel capacity utilization – 99%. Fulfillment of the supply plan – 84% (the objective was 80%). This KPI was measured in metric tons (a metric ton is equal to 1,000 kilograms) of cement.   Benefits of the simulation model Using AnyLogic, Argos optimized its cement supply scheduling and vessel capacity utilization. Also, simulation helped to review the system bottlenecks and identify possible risks such as inventory shortages, shipment delays, and so on. This improved product availability in the real system.  Moreover, modeling reduced labor costs and overtime. As a result, in 2021, Argos sold higher volumes than in 2020 with the same unit cost.  Next steps Argos engineers were going to add autonomous destination definition for vessels in the model. This way the vessels could choose optimal next destination themselves.  The engineers would also like to combine heuristics with the reinforcement learning technology to increase vessel (agent) choice efficiency.   Watch the video about this case study presented by Cementos Argos at the  AnyLogic Conference 2021.       Developing a Digital Twin for Manufacturing in the Cell Therapy Industry Link:  Tags: Healthcare  Stem cell therapy promotes the use of stem cells in tissue restoration. These stem cells are grown in a lab and manipulated to become particular cell types, including heart muscle cells, blood cells, or nerve cells. Stem cells can either repair or replace cells damaged by disease or chemotherapy, or they can work with the immune system to combat certain cancers and blood-related illnesses like leukemia.  Problem  The cell therapy manufacturing process has several unique challenges, including a supply chain that is highly personalized for individual patients.  The procedures in the manufacturing process are very clear and must be carefully followed. The patient initially requests compatible CBUs (cord blood units), and after a suitable batch has been located, it is sent to the manufacturing facility. The CBUs are immediately placed in liquid nitrogen, and all necessary materials for production must be present before the process can begin.  Since the process cannot be interrupted from day zero until the harvest, free production slots must be located in the most efficient manner. The product is frozen after the final day of production (harvest) and then prepared for transportation to the hospital and the patient. During manufacturing, work can only be done on one batch at a time in the clean room, which leads to potential bottlenecks.  The producer of stem cells was preparing to launch their new manufacturing operations. So, they contracted two consultants, Logico and Opyflow, to help validate the manufacturing site capacity and maximize clean room utilization.  Solution The two consultants worked together to develop a solution using a digital twin for the manufacturing process. Since demand in this industry is very stochastic, time dependent, and has many variables, simulation was chosen over a simple analytical model.    Digital twin for manufacturing: model scope   The developers decided to use AnyLogic to create the digital twin because AnyLogic provides both agent-based and discrete-event simulation modeling. In addition, AnyLogic provided the capability to define and modify extremely complicated processes using Java programming to create and customize the Libraries. Importantly for this project, optimization could also be performed. And finally, a UI that featured 3D visualizations and KPI dashboards could be developed.   User interface for parameter input, KPI dashboard, and the 3D model (click to enlarge)  The required parameters that were needed for the digital twin were available to the developers from the physical asset. These included engineering, operations, planning, and quality. Additional parameters can be found within each of these, as shown in the diagram below.   Methodology and model architecture   The customer received a stand-alone model with adjustable parameters. In the simulation model, the process starts when raw materials arrive at the manufacturing facility. All auditing and operational procedures connected with these raw materials also take place at this moment.   Also, within the model, the staff arrive and go from an unclean area to a sterile area. All work is then completed in what is known as the clean room.  In each clean room, work is done on only one batch at any given moment. So, there could be the capacity to have a larger workforce, but the room can only be used for one batch at a time. Therefore, there is a need to maximize the clean room's potential and remove bottlenecks. A multi-step optimization process was implemented for the purpose of identifying bottlenecks. The consultants used this approach to identify a bottleneck, correct the parameter causing the bottleneck, and then "lock" it so that it couldn’t be changed. They then repeated this process, and step by step they reduced the bottlenecks. This process could continue until all potential bottlenecks have been corrected.  Results According to the findings, there was potential for a 30% increase in capacity, which would result in 30% more sales. The client ultimately makes more profit.  Additionally, the client was better prepared for the launch because numerous issues were found and planning problems that hadn't been thought of were identified and resolved. An example of this was when, after identifying bottlenecks, extra equipment was ordered to clear them.  Finally, the digital twin for manufacturing and the FMEA (failure mode and effects analysis) were used to produce the paperwork that had to be submitted to the FDA. This was required to obtain a biological license.  Looking further ahead in this cell therapy manufacturing process, the potential for model expansion is limitless, but the first step will be to create a model that incorporates the entire supply chain. The case study was presented by Yossi Benagou of Logico, and Dov Amor of OPYflow, at the AnyLogic Conference 2022. The slides are available as a PDF.     Developing a Project Management Digital Twin for a Gas Turbine Manufacturer Link:  Tags: Manufacturing, Business Processes  Problem        One of the largest turbine manufacturers in the world had a very promising five-year portfolio of gas turbines to produce and was planning an optimistic 30% net margin. The company’s portfolio consisted of over 100 programs, made of 1000’s of projects, with each project composed of a number of phases. Relying on its good past performance, the company built their strategic competitive edge around their

 

Section 14 of 60
reliability, enabling them to offer penalties for late delivery and bonuses for early performance.        About a year into that five-year portfolio, the management realized that some of the projects required to complete programs were facing significant delays. They were hoping the buffers they added at the project and program level would be enough to absorb these unexpected delays. Considering these delays, and the fact that the sales team kept promising standard lead times on new programs even though there was a growing backlog, the company was concerned about whether they would be able to meet client and shareholder commitments.        It is not only VUCCA - the Volatility, Uncertainty, Complexity, Constraints and Ambiguity - faced by managers in a project environment that makes it so hard to make good decisions and accurate predictions. It is also the fact that there is non-linearity. Small changes can have Big operational and financial performance impacts and vice versa. Dynamic Simulation models like this Project Portfolio digital twin we developed can help managers overcome this challenge.   Dr Alan Barnard                    Goldratt Research Lab        Today, making reliable commitments to clients and shareholders is becoming more and more difficult due to higher levels of VUCCA (volatility, uncertainty, complexity, constraints, and ambiguity). Also, due to the fact that project-based companies meet the criteria of complex adaptative systems, often small changes can cause big positive or negative impacts. Traditional portfolio and project planning software cannot consider all the dynamic interdependencies, resource constraints, and variability, and as such, could not be used to answer the following critical questions the leadership team faced:    Considering the current delays, how long will the company’s five-year pipeline of programs likely take to complete if they continue to use traditional project planning and execution practices? Considering the likely completion delays, would the company still be able to make a profit on this portfolio? How much better could they perform if they were willing to implement project planning and execution of best practices based on Theory of Constraints’ Critical Chain Project Management (CCPM)? Considering the above, by how much would project completion and cost commitments to clients and profitability commitments to shareholders have to be changed?       The company and their CCPM software provider and implementation partner called upon the assistance of Goldratt Research Labs (GRL), a leading source of Theory of Constraints-based research and innovation services. GRL suggested that a digital twin of the company’s portfolio of programs and projects could be developed for simulating manufacturing project management strategy. That included testing to quantify the operational and financial impact of implementing a new CCPM and Agile Planning & Execution rules to limit Work-in-Process (WIP) and enable correct execution priorities to reduce delays caused by multitasking and unsynchronized execution.   The difference between traditional manufacturing project management and critical chain manufacturing project management       The other question the manufacturing digital twin could answer was what the most appropriate WIP control mechanism would be, and what the appropriate WIP limits would be for such a mechanism. Should they control WIP at the Program level, the Project Level, or maybe a hybrid level was necessary that would control WIP at the Program level and release more projects if capacity constrained resources ran out of work. Even the most experienced CCPM and Agile experts could not answer these questions.        GRL’s aim in this project was two-fold. Firstly, develop a digital twin that would represent this company accurately enough to provide the management team with a decision support tool to help answer their key questions. Secondly, to use the opportunity to further develop and test the self-configurable digital twin platform that they have been developing to simulate any portfolio of programs, projects, and phases for any other client with similar challenges.        The scope of this digital twin was designed for manufacturing project management to find out:    What would happen if the company continued following the current (traditional) project management rules (in terms of operational and financial impact). What would happen if the company adopted the new CCPM rules. What would be the best WIP control mechanism and what should be the “optimum” WIP limits. How to prioritize and allocate resources to improve flow and profitability.  Solution        GRL has been using AnyLogic production simulation software for many years to build manufacturing simulation models that can predict the likely operational and financial impact of using traditional supply chain and project management practices versus the TOC best practices. These models are applied to show how much better companies could perform if they implemented TOC’s best practices, and what the fastest, simplest, lowest cost, and lowest risk way of doing this would be.        In this project, they decided to build the digital twin using AnyLogic manufacturing simulation software because of the following reasons:    Multi-method modeling enables more precise replication of the complex real-world environment when it comes to manufacturing project management simulation. The production simulation model can be built as a self-configurable entity and serve as a digital twin once connected to an input data source. This makes the model more scalable and ensures for better usability now and in the future. Exporting the model as a standalone application, or to the AnyLogic Cloud, ensures for easier model sharing and testing, as well as communicating manufacturing optimization results to clients. With access to AnyLogic Cloud simulation capabilities, users can perform complex multi-run experiments faster and more efficiently.  Manufacturing project management simulation tool demo       GRL had already created a manufacturing project management optimization platform in AnyLogic, allowing their developers to create a completely self-configurable digital twin of any environment. They had tested it in other business and industry fields and now adopted it for the objectives of this specific project.        In this model, both agent-based and discrete event simulation methods were used. The input of the model was the full work breakdown structure of the total portfolio, exported directly from the client’s project management system to Excel. There were also additional Excel sheets describing locations for the resources, resource availability, late completion penalties and early completion bonuses, etc. The manufacturing project management simulation model also offered users the ability to select whether to run scenarios using traditional project planning and execution practices or best practices of Theory of Constraints’ CCPM rules.        The baseline scenario was designed to see how long the simulated project management portfolio would likely take if the company continued to use traditional rules including:    Standard lead times regardless of the backlog. Keeping resources busy by starting everything as soon as possible. Getting resources to multi-task (task switch) to show progress on as many projects as possible. Not waiting until a project has a full kit to start.       The manufacturing project management simulation model was designed to offer users the ability to run it in one of three ways:    A Single Run experiment mode where users can monitor the dynamics at play, while programs, projects, and phases were executed. Charts show the progress of each project and program in terms of buffer consuming versus critical chain completion. A Gantt chart view was available to see the magnitude and causes of cumulative delays at the phase, project, and program level. The user could also see the resource utilization and financial impact of delays in real-time. A Sensitivity Analyses mode where users could determine the sensitivity on portfolio duration and profitability with step-changes in WIP limits on programs or projects. A Scenario Comparison mode where three of the most important scenarios could be compared against the baseline.       Simulation outputs include default AnyLogic manufacturing simulation software reports where users could compare different characteristics in different scenarios, detailed Excel reports, and logs for a deep dive into specific project or phase execution.   Results        GRL presented operational and financial results of the four scenarios to the company’s management team. The first baseline scenario showed the consequences of the company following traditional manufacturing project management to complete the planned five-year portfolio of programs. It was clear that even the high net profit they used when quoting, to absorb their past strategy of spending, resources at delayed projects and programs would be insufficient to enable them to finish projects and programs on time and profitably. In fact, the manufacturing project management simulation showed, to the horror of the management team, that the company would likely be late by over two years resulting in a loss of $181 million.   Results of manufacturing project management simulation       The other three scenarios were focused on how much better the company could do in both program on-time, as well as overall profitability performance, if they implemented three different WIP control mechanisms as part of their CCPM implementation.    Scenario 2 showed the impact of controlling WIP at the project level. Scenario 3 showed the impact of controlling WIP at the program level. Scenario 4 showed the impact of controlling WIP with a hybrid rule that controlled WIP at the program level, but which could launch additional projects if capacity constrained resources were being starved of work.       The four scenarios created in our Project Portfolio digital twin simulation, enabled the company’s leadership and project management teams to identify which change in project planning and execution rules would have the largest positive impact on project portfolio performance and profitability.   Jaco-Ben Vosloo                    Goldratt Research Lab        The digital twin showed that, for the company, WIP control at the project level appeared to be the best option. Although the company was going to finish the whole portfolio nine months later, it would still have a net profit of $104 million. Moreover, since not all programs and projects had penalties and bonuses, it was possible to get even better financial results by running scenarios to identify the optimal prioritizing rules in order to ensure those programs and projects with the biggest late completion penalties and early completion bonuses were given the highest priority.        GRL’s manufacturing project management simulation also showed that they were able to create, using the AnyLogic production simulation platform, a reusable and self-configurable project management digital twin that can model any project environment. It is easily configurable, can be integrated with the company’s project management system, can consider task duration variability and other random events to accurately predict the likely outcomes using traditional vs. CCPM rules, and can also determine the impact of changes in resource allocation and prioritization rules on the company’s operational and financial performance.    Watch the video of Dr. Alan Barnard and Jaco-Ben Vosloo, presenting this case study at The AnyLogic Conference, or download his presentation.       Digital Twin of a Manufacturing Line: Helping Maintenance Decision-Making Link:  Tags: Manufacturing  Overview: How and why a digital twin was created and tested for an automotive production line CNH Industrial is a global leader in capital goods. It is financially controlled by the Italian investment company Exor and is comprised of 12 brands, including Case, New Holland, and Iveco. Through its brands, CNHI designs, produces, and sells a wide range of agricultural, industrial, and commercial vehicles and powertrains. It employs more than 63,000 people in 66 manufacturing plants and 53 research and development centers in 180 countries. The company is listed on the New York Stock Exchange and is a constituent of the Italian stock market index. Fair Dynamics operates primarily out of Milan and provides a wide range of consulting services in a variety of industries, including banking, manufacturing, and public services. The company has recently been acquired by Engineering Ingegneria Informatica S.p.A., a provider of software and IT services, both in Italy and internationally. In 2017, the consolidated revenue was more than €1bn. Fair Dynamics applies innovative technologies to solve industrial problems and improve efficiency. Their key approach is modeling and simulation, for which, the company has been using the AnyLogic Platform since 2010 and is also its Italian distributor. Problem Manufacturing processes are becoming increasingly digital. It is considered that now we are entering a fourth industrial era (Industry 4.0) and the transition towards smart factories has begun. Within smart factories, cyber-physical systems monitor physical processes, create a virtual copy of the physical world, and make decentralized decisions. Digital twins are core to the operation of these systems. With digitalization already underway, many companies are trying out new technologies like artificial intelligence and cloud computing with the aim of gradually shifting to a smart factory and benefitting from the new phenomena.   CNH Industrial identified maintenance processes as a promising area to start applying new Industry 4.0 technologies. In the automotive and related industries, downtime costs can be large. For global companies like CNHI the cost of a single minute of downtime could be more than $160k and these figures increase year by year. As such, improving maintenance in order to reduce downtime can deliver significant success. By identifying the most critical areas, even a very small percentage improvement could save a lot of money. Thus, CNH Industrial wanted to test a digital tool for evaluating and selecting different maintenance policies and agreed with Fair Dynamics on a pilot project. They decided to focus on a single manufacturing line dealing with Iveco Daily van chassis welding (the Mascherone line of the Suzzara plant, Italy). A digital twin, a representation of the line in a virtual environment, was to be created. The simulation would enable CNHI management to see the benefits of possible maintenance policies in various scenarios and make informed maintenance decisions. The choice of the Suzzara plant for a digital twin was not random. CNH Industrial

 

Section 15 of 60
applies the principles of World Class Manufacturing (WCM), an innovative program for continuous improvement. At that time, CHNi had only one WCM Gold Level award and the Iveco plant in Suzzara was very close to a second one. CNHI wanted to see how the new technology could help attain it.  Solution  Welding line (click to enlarge)  The digital twin project focused on a specific manufacturing line, Iveco van chassis welding. This line can be described as a conveyer which runs through a number of stations. Fair Dynamics were asked to focus their attention on the automatic welding stations (orange blocks in the picture). When a van stops at one of these stations, the robots work in unison to complete the welding. The welding guns have an Achilles heel – the Lamellar pack (an electrical conductor which must flex during operation). The movement gradually leads to the damage of a pack’s copper layers. When the damage becomes critical, and sufficiently changes the conductivity, it can result in the melting of the Lamellar pack itself. While normally this component can be replaced in few minutes, it can take hours if the Lamellar pack has been damaged. A digital twin that monitors and forecasts the health of this component could provide significant downtime reduction. Fair Dynamics built an agent-based digital twin with the following agents:  Vans — There are different types of van agents in accordance with the types of vans to be produced. Each type requires different handling (different operations, stations, and robots could be involved) and this affects component degradation. Stations — Each station agent is characterized by the number of robots it contains and is regulated by particular rules. Robots — Each robot is fitted with a sensor which sends a signal about the robot’s actual condition to the simulation model. Each robot agent, in turn, is provided with a specific PHM (Prognostic & Health Management) model predicting the robot degradation in accordance with the signals received.   By building the digital twin this way, Fair Dynamics could introduce three basic maintenance policies for testing and use:  Scheduled maintenance (components are replaced according to a schedule). Condition-based maintenance (components are replaced according to warning signals). Predictive maintenance (components are replaced on a schedule based on information from their state and use).  Within the project, AnyLogic software proved useful for digital twin creation. Apart from enabling agent-based modeling, it enabled the customization that made it possible for Fair Dynamics to include the prognostic ELF (machine learning) model. The integration of modeling and machine learning techniques has great potential in such systems.  Through the use of AnyLogic, the digital twin could connect to external data sources. The production sequence, welding point per van type, robot life cycle curve, and other data were imported from external sources and automatically read by an agent at runtime. Moreover, the system could be exported and delivered as a standalone application to multiple machines, easing data constraints and the demands on the IT department. Outcome With the help of the digital twin, CNHI management and specialists can get detailed and demonstrative information about the economic and production consequences for different maintenance policy configurations. This is done by running various what-if scenarios where the user can vary different core parameters (e.g. maintenance policy, production plan, working schedule, etc.). It is also possible to change the characteristics of the line or a robot, if needed. The system can handle both the near and far future and, moreover, using the digital twin for simulation provides an easy-to-use tool to analyze and compare scenarios — enabling a quick understanding of how changes could impact maintenance cost. The digital twin provides a wide variety of data, including total production, maintenance time, total cost for spare parts, and the work cost of maintenance. In short, the digital twin is a detailed and comprehensive tool for establishing efficient production line operations.   The AnyLogic white paper, An Introduction to Digital Twin Development, contains further case studies and outlines digital twin benefits and development — download.    Project presentation by Luigi Manca, Project Delivery COO, Fair Dynamics Consulting Unit:      Disaster Response Planning Using Agent-Based Simulation Link:  Tags: Defense  Overview Battelle is the world’s largest, non-profit, independent R&D organization, and is a worldwide leader in the development, commercialization, and transfer of technology. They manage or co-manage laboratories for the U.S. Department of Energy, the U.S. Department of Homeland Security, and an international nuclear laboratory in the United Kingdom. Problem In an effort to find practical operational solutions for a fast and effective response to an unexpected crisis or natural disaster, Battelle needed to test the effectiveness of a 48 hour shelter-in-place order for an Improvised Nuclear Device scenario (IND). The intended goal was to reduce radiation dosages received during an uncoordinated mass evacuation, by comparing immediate evacuation and shelter-in-place order. Modeling a disaster, whether natural or man-made, represents many unique challenges. There are distinctive environments and physical consequences, and numerous scenario possibilities and threat vectors. In addition, disaster response strategies are rarely implemented as planned, and there are unknown human reactions. Solution Simulation was chosen for the disaster response planning because it had the capability to evaluate the space of potential scenarios. Deterministic models had limitations incorporating factors, like fundamentally unpredictable human responses and the need to compare alternatives versus looking for exact answers. AnyLogic software was a natural choice for Battelle, as the software was already being utilized in a broad range of projects within the organization, including:  Healthcare – Provider Resource Management, Clinical Workflow Modeling, Infection Control  Economic Development and Industry Cluster Forecasting Vehicle Fleet Logistics and Maintenance  National Security and Disaster Response  In addition, AnyLogic’s agent-based capabilities allowed Battelle to capture the most important dynamics of a disaster event. Emergence, or emergent behavior, is a key principal in modeling human behavior. Also, a model can sometimes exhibit unexpected outcomes. Both of these issues can only be captured using agent-based modeling.   Disaster Response Planning Simulation Model Framework  The comprehensive model framework included an environment of road networks, vehicles, drivers, and disaster events. The road network was built with road layouts from GIS databases, local highway agency data (speed limits, lane capacity), and agents as node points for greater control. Changes to the network, such as the flooding of roads or destruction of bridges, were incorporated into dynamic events as the disaster unfolded. The physical limitations of vehicles were governed by parameter data provided by the US Census, Bureau of Transportation. Data from past disaster response studies was used to represent driver agent behaviors, taking into account the changes in irrational drivers in normal circumstances versus during a mass evacuation. The model also incorporated dynamic route finding (several interlinked agent state sets that were dynamically tracked and updated). In addition, all behavior states were linked to physical vehicle movement parameters to initiate vehicle stoppages as drivers became incapacitated. Agent behavior variables from initial values were calibrated, and evacuation data was used from past disasters to set accuracy targets, since calibration and validation were critical steps in proving the validity of the simulation model. If no historical data was available, Battelle used data from other major transportation events, sensitivity analysis based on other disaster events, and survey data.   Disaster Response Planning Simulation Model   Dynamic contours were used to track regions of disaster consequences, often derived from other simulation models, to compartmentalize processing requirements. Contours updated in real time based on predicted weather patterns, land cover, etc., and multiple interlinked contour sets could be adapted to represent almost any disaster scenario (for example, flooding levels, fire spread, damage path, contamination/fallout spread). In the IND scenario, two main contour sets were used; blast radius levels (fireball and overpressure force contours) and fallout distribution (radiation levels in air and deposition on ground from various radioactive particle types). Results The disaster response planning simulation model built using AnyLogic software compared immediate evacuation versus shelter-in-place order and showed that shelter-in-place order significantly reduced radiation dosage received, as well as cases of severe radiation poisoning for large INDs.  The model also produced downstream outputs to test different disaster response strategies and find the best response strategy among several likely options. Battelle was able to incorporate emergency responder agents, multiple intervention scenarios, and interchangeable model components (different locations for same disaster scenario, or different scenario for same location), to achieve the goal of finding practical operational solutions for fast and effective responses to various unexpected crisises or natural disasters.    Distribution Network Planning & Inventory Optimization Supported by Logistics Simulation Modeling Software Link:  Tags: Supply Chains  Problem Diageo is a British multinational alcoholic beverages company. Diageo Russia is one of Russia’s top five wholesale alcoholic beverage distributors, which is traditionally a low margin business where the bottom line is sensitive to the customer service level and high logistics costs.  Diageo sought assistance from the consulting companies Logistics Field Audit (business and supply chain management consulting) and Amalgama (simulation modeling consulting) when they experienced an increase in sales volume but failed to realize larger profits due to logistics costs per unit.  Other mounting concerns for Diageo were customer service level and cost of goods sold, combined with an increased inventory and future plans of development including a new warehouse in Russia and expansion to Urals and Siberia. To translate these initiatives into action, predictive analytics and logistics planning were necessary. The consultants were also assigned to manage Diageo’s big data with simulation-based decision support to show and prove ways to decrease logistics costs and implement logistics network optimization. Logistics planning and optimization with AnyLogic software provided the ability to achieve these goals.   Logistics Optimization Model  Solution The logistics optimization model includes three existing and one prospective factory, three border crossing points, three existing and five perspective warehouses, two customs offices and up to 300 demand points grouped into 45 service groups. In addition, the logistics optimization model contains a replenishment algorithm, an order aggregation algorithm, load bearing algorithms, and delays at border crossing points. Demand and sales- forecasting accuracy for all 280 Diageo products in 6 types of warehouses were also built into the model. Logic built into the logistics optimization model includes a replenishment algorithm for a segment of the logistics network that begins at the consolidation warehouse Due to Russia’s large size and relatively slow transportation capacities, it allows for five days of lead time to the central distribution center, and five days of lead time to the original distribution center. The replenishment algorithm takes into account a requirement diagram (planned sales diagram), current stock, lead time, and minimum order size, then generates the requirements for replenishment, identifies the coverage gaps, (time periods when stock will be lower than the lowest threshold), and takes action to prevent any gaps in coverage. The capability to observe the model behavior dynamically, a result of the logistics simulation using AnyLogic software, was extremely useful in this case.  Validation of the logistics optimization model was imperative and began by comparing data from the previous year’s SAP ERP system data and gave results with less than a 5% differential.   Logistics Network Optimization Model Results  Outcome The initial value for Diageo included an increase in sales forecasting accuracy from 60% to 80%, with a pay-off period of less than 2 years. This increase will allow Diageo to reduce their target stock level by 40% which will reduce logistic costs per unit by 7%, even with sales growth plans. The research also showed there was no need for additional warehouse space, as the stock required to maintain the target service level was unreasonably high. After running the simulation, the logistics optimization model provides the stock level forecast for each product (15 days ahead), a complete cost for each and every delivered product unit and proves to the client what the target state of the supply chain should be. View Andrey A. Malykhanov’s entire presentation about logistics simulation and optimization using AnyLogic software delivered at the AnyLogic Conference 2013:     Domain-specific Mining Simulation Software Tool for the World’s Second Largest Nickel Producer Link:  Tags: Mining   Problem       Mid-term and operational planning in mining is an area where simulation has become a de-facto standard. Simulation copes well with the planning of mining operations: with its hundreds of interacting machines, overlapping maintenance activities, complex layouts of mine fields, and limited capacity of bunkers and conveyors.       As soon as you create a simulation that considers thousands of parameters and diverse types of equipment, you need a lot of data from various sources to run it. It is a challenge to input, maintain, and update all this data. A mine layout changes daily, as soon as you advance through the stopes, and it becomes too complicated for mine planners to update the data and apply simulation in routine operations. That is why the world’s leading mining companies frequently ask for integrated simulation-based tools, with user-friendly interfaces, to support their mid-term and day-to-day operations planning. This was the way the world’s 2nd largest nickel producer followed.       The process inside stope works is cyclic: drilling, then blasting, then finally cleaning, then drilling again. The main constraint here is that all the blasts are done at the same time throughout the mine, two times a day. This

 

Section 16 of 60
is because making a blast requires the evacuation of the whole mine for evident safety reasons. If the drilling and cleaning teams have not completed, the blast is skipped until the next day and production capacity is lost.        A good example of everyday challenges is choosing the optimal depth of blast holes. Deeper blast holes give the advantage of using jumbo drills more efficiently, but shorter blast holes lead to less risk of skipping the blast.        The trade-off between deeper and shorter blast holes has no global solution, since:    Cycle times can vary. One drill rig can be shared between several stopes. Cleaning speed depends on haul length and position of ore passes. Drilling performance can differ from stope to stope due to geological reasons.      Simulation experiments needs to be run for at least every week of the mining process, including for drill and blast. That is why a domain-specific tool that simplifies the simulation experiment is required.   Solution  Amalgama developed a domain-specific mining simulation solution to be regularly used by mine planners. The tool is an application, featuring the following functionalities:    Regional preferences Import of data from ERP, EAM, and specialized GIS systems. Domain-specific user interface for data entry with interactive map of mine field. Interactive dynamic simulation.  Reports for viewing simulation results in charts and tables. Export of modeling results to external IT systems.      Technically, the tool creates, among other things, a drill and blast simulation model using AnyLogic software and a UI that supports a mine manager’s needs. Commercially, such usage typical of an OEM application, where the AnyLogic engine is used within another vertical application.       The dashboard of the mining support software shows how the production capacity depends on the blast depth for each mine stope in each situation.       Blast hole length optimization is obviously not the only challenge that the mining simulation tool is used for. The tool is also used to support weekly planning and to find bottlenecks in the underground transportation system. The simulation demonstrates that the rescheduling of activities and the rerouting of underground transport helps increase the throughput of the mine and ensure the stable flow of ore to the processing plant.   Outcome       An underground mine is a complex system with many constraints and interdependencies. Simulation copes well with the complexity. The AnyLogic model replicates interactions between movable equipment units, as well as the operation of conveyors with continuous flows of ore.       Mining models require a lot of input data that is hard to maintain by mine planners. To simplify the usage of simulation on a regular basis, the new software tool was created. The tool consists of a domain-specific graphical user interface for mine planners and mining simulation model created in AnyLogic software.   This tool opens the way to efficient simulation usage in mine operations to support a wide range of decisions, from daily operations scheduling to the return on investments calculation.  Project presentation by Andrey Malykhanov, Amalgama     Drilling Rig Digital Twin and Well Construction Optimization Link:  Tags: Oil & Gas  Overview        Transocean is a Swiss offshore drilling company that provides rig-based well construction services worldwide. The company is one of the largest contractors in its sector and has offices in 20 countries. It operates a fleet of versatile, mobile, offshore units comprised of midwater, deep water, ultra-deep water, and harsh environment floaters. The company decided to build a digital twin of its well construction processes to enable prediction and advanced planning capabilities with the aim of optimizing operations across its fleet.   Well construction process flowchart   Problem   Oil and gas drilling rigs are very complex systems that contain thousands of processes which must operate 24 hours a day in remote environments and harsh weather conditions. New drilling rig construction can require more than a billion dollars of investment. The process has lots of interdependencies and there are many different failures that can occur. As a result, the operational and financial characteristics of a system during setup and production are often difficult to predict.       The well construction process consists of several stages. Some advanced simulations of the drilling process itself already exist, but Transocean wanted to have a simulation of the whole well construction process, including the stages of non-drilling: tripping, cementing, casing, etc. The result would be a digital twin of the well construction process (read more about digital twin technology – Introduction to Digital Twin Development).   Simulating the whole construction process would help the company plan oil well optimization more precisely. Transocean would be able to:  Choose the best rig for a certain type of contract or environment. Get bonuses for early work completion and escape maluses for delays. Be confident in their predictions when giving commitments. Test and analyze potential benefits of any equipment, process, or policy changes in a safe virtual environment by running various what-if scenarios. Optimize and improve performance with in-depth analysis of real time operational data. Reduce downtime and invisible lost time by indicating possible problems in advance.  The well optimization model’s architecture  Simulated processes that are dependent and interdependent on different factors  The model interface  Solution   Transocean decided to begin with a proof-of-concept project and to continue development if it showed satisfactory results. The company chose AnyLogic as the platform for their simulation for several reasons, including AnyLogic’s multimethod simulation modeling capability. In this project, system dynamics modeling, agent-based modeling, and discrete event modeling were combined. The engineers used the flowchart-based approach for the model with Delay blocks for high-level processes.   Vast amounts of historical data from places such as well plans, machine operations, and the time logs of Transocean’s global fleet of drilling rigs (which record data every 15 minutes) were loaded into the model. This enabled the simulation of very complex dependent and interdependent processes.   The AnyLogic platform eased the creation of the well construction optimization model with multiple levels of fidelity that could combine precise data and statistical distribution. This means engineers can work at a high-level and then dive deep down into the model’s lower levels to investigate more. For example, it is possible to test new equipment by adding data about its characteristics to the model and running what-if scenarios. The results show the equipment’s performance in the overall system and its rate of return.   The well optimization model has an attractive and user-friendly interface in AnyLogic. Users can choose a well plant, set different parameters, and adjust any of the associated machines. As a result, it is possible to see the actual processes going on in a drilling rig in real-time and obtain predictions for the construction process. It is possible to run different what-if scenarios to test new equipment or changes in the supply chain according to the real or predicted situation on site. The results are shown in Gantt charts, logs, and 2D and 3D models.   Transocean created two 3D well optimization models. The polygonal model was made for verification purposes and implemented in AnyLogic, which quickly and easily allowed the building of an accurate and clear visualization. The second model was made in Unreal Engine for marketing purposes, so that customers could see and understand what certain processes look like. The AnyLogic platform API enabled seamless integration of the AnyLogic model with the Unreal Engine visualization.   Oil and gas simulation - well optimization model in 3D   Outcome       The well optimization simulation model was a proof-of-concept project, and the comparison of the real-world well construction cases and the virtual predictions showed a very good correspondence. Moreover, the model is already able to test equipment, process, or policy changes and compare the capital expense and reduction in operating expenses due to these changes. It shows how the company can save working days (each of which could cost Transocean several millions of dollars) and has boosted income by enabling more wells to be drilled per year. Furthermore, it has also helped improve safety by showing management when it is possible to remove people from dangerous zones without causing problems.   Now Transocean is planning to bring in more expertise and enhance the model with regards to:  Adding more interruption events to the model (failures to function or operate, equipment failures, weather events, dynamic downhole conditions, etc.). Improving overall architecture with simplified logic and making it well and drill agnostic.  This project proved the efficiency of AnyLogic software in the offshore oil and gas domain. You can download Jason Baker and Abe Hudson presentation from The AnyLogic Conference.    Electric Vehicle Route Optimization: Plan Delivery with Simulation Software Link:  Tags: Supply Chains, Transportation  Problem  Electricite de France (EDF) is the largest public electricity company in France. One of its activities is electric charging infrastructure development and energy-efficient vehicle popularization. Within this area, the company launched a project to create a tool for the operative management of delivery by electric trucks in Paris and its suburbs.       The company needed to optimize routes to ensure that all orders were delivered on time. To this end, EDF opted for simulation modeling software and decided to integrate a separate electric battery charging optimization algorithm into the developed model.   Solution       The EDF specialists built a delivery model in AnyLogic simulation software. To reflect the real system more precisely, they considered several constraints:    An electric truck shouldn’t run out of power during delivery. Electric trucks can be charged only at the charging station (depot), and the process takes about eight hours. The power of the charging station is limited and distributed among chargers (born 1, born 2, etc.) in a particular way. The amount of energy an electric truck consumes depends on whether it is loaded or empty. The maximum route length is 100 km.       For the delivery route optimization model, the development team used the agent-based simulation method to describe the electric trucks’ operation. For each truck, they defined characteristics (vehicle dimensions, speed, load weight, etc.) and states (waiting, loading, unloading, on route, etc.) that changed over time when the model was run.        Furthermore, in the simulation there were two types of electric trucks with differed battery capacity. The team also set charging station parameters such as GPS position and individual chargers’ power.       After that, with AnyLogic software capabilities, the developers located the remaining logistics network facilities on a GIS map. They also set up 2D and 3D visualization of the charging station and electric trucks. With this, the user could easily switch from the map view of Paris and the suburbs, which enabled observing the whole system work, to a more detailed object operation view.       The outputs of the delivery route optimization model were routes displayed on a GIS map with statistics on the charging station, on each charger, on the trucks’ states, and on the order state.        In the simulation, the EDF team could vary traffic density, change some of the preset parameter values, turn chargers on and off, and observe how these changes affected the operation of the entire logistics network.   Results       While working on the project, the EDF team had a delivery route optimization simulation model developed, visualized the logistics network, and collected statistics using AnyLogic software.        The model helped them calculate the optimal number and ratio of electric trucks with different types of batteries in the fleet. They also identified the number of chargers and the amount of power they should have to support uninterrupted and timely delivery of goods by electric vehicles.       When the model was finished, the EDF team integrated a separate battery charging optimization algorithm in it. As soon as the algorithm was calibrated, the company was going to develop a tool for real-time operational management of delivery by electric trucks.       Emergency Evacuation Planning: Minimizing Gridlock and Improving Public Safety Link:  Tags: Road Traffic, Defense  Problem      Intelligent transportation systems monitor and analyze how vehicles, roadways, and environmental factors affect traffic flow. Most of us are familiar with the frustration of a traffic jam. A typical rush hour impedes the mobility of individual vehicles and significantly slows the overall flow of traffic. This phenomenon is compounded by events of mass mobilization, such as during an evacuation due to a hurricane or other event. When this occurs, traffic can reach a state of gridlock. ITS researchers sought to understand how public safety could be improved during such events by incorporating communication among a percentage of the vehicle population.   Solution    Road Network Model with Radiation Plume   and Affected Roads     One of the oldest and largest independent, non-profit, applied research and development organizations in the US, approached this problem using AnyLogic to explore if having a percentage of vehicles connected via a smartphone or a dedicated short range communication (DSRC) radio could improve the coordination of vehicles during large-scale evacuations.       The vehicle agents incorporate parameters that represent the likelihood of being equipped with a communication device, and the likelihood that they will follow the vehicle in front of them when they are in a congested state. This second parameter approximates the behavior of human drivers who follow other drivers in the assumption that they have knowledge of a better route.          Researchers then ran scenarios based on these two parameters and compared total accumulated radiation exposure and time in congestion.      The AnyLogic modeling tool combines agent-based and system dynamic modeling techniques and has a powerful, intuitive graphical interface for constructing this type of model. Researchers modeled an evacuation scenario based on a radioactive spill in an urban area. The model included a simplified traffic system based on the highways of San Antonio, Texas, and three agents representing a vehicle, a roadway network, and an event notification.   Vehicle Agent State

 

Section 17 of 60
Chart and System Dynamics Architecture for Congestion  Results     The results of this research quantified the performance of the traffic system, as measured by average and total radiation exposure to the vehicle population, as well as overall congestion as reported by each vehicle agent, for a number of different scenarios. The results illustrated the safety impact when even a small number of vehicles receive targeted, timely information regarding a potential danger on their current route.          The simulations also show a benefit from the vehicle following behavior, which is a secondary beneficial effect for a traffic system comprised of human drivers (the inclusion of autonomous vehicles in a traffic system may negate some of these types of effects; however, AVs will also likely be connected via communication devices).      Evaluating Container ETA Data Flow Introduction in the Port of Hamburg Link:  Tags: Transportation, Ports & Terminals  Problem A typical maritime container shipment involves a deep-sea transportation operator, a port terminal operator, and an intermodal operator. Containers are offloaded and stored at terminals, and then they are usually sent by ground transportation to their further destinations. This hinterland part of the supply chain can often become a bottleneck because if a deep-sea vessel gets delayed, it can complicate further shipment processes.  A delay of a ship means a delay of all the containers it carries. If further delivery is by train, the containers will likely miss those trains and need to be shipped on the following ones. This dramatically complicates the shipment planning process. Consequently, containers are often delivered late, terminal storages are overwhelmed, and train capacity is seriously underused, resulting in a loss of money for all parties involved. The situation gets even worse if the arriving ship is really large, because offloading such vessels may take more than ten hours. Nobody knows whether a certain container will be offloaded at the beginning or end of this long process, and this makes the planning even more complicated.  To avoid such situations, intermodal operators try to rebook other containers for the available train capacity, but they have to do it in advance (for example, in the port of Hamburg, Germany, at least one day prior to train departure). To do it more efficiently, the operator would need early provision of information on the arriving containers (time of arrival at the terminal and destination).  One assumption is that the problem could be solved if all of the parties involved knew the estimated time of arrival (ETA) of each container, which would mean knowing the time each container arrived on the quay. Today, only vessel ETA is available, which does not help much, because it makes a difference to the train operator if a container is offloaded first or last. Availability of container ETA would enable rebooking of loading-ready containers for hinterland transportation on trains to increase the capacity utilization.  Researchers from the Technical University of Darmstadt, Germany, were working to identify whether the introduction of container ETA would improve the situation in the port of Hamburg, one of world’s top 20 ports in terms of container traffic.  Solution The researchers decided to reproduce two equal, three month time periods of system operation in a simulation model and compare the performance with and without the availability of container ETA. They decided to test and compare three policies that the intermodal operator could implement:   Standard case: serving only containers from one deep-sea carrier.  Policy 1: serving containers from multiple carriers and primarily sending by train the ones with the most critical shipment deadlines.   Policy 2: forming a pool of train loading-ready containers being stored at the terminal, and rebooking of container train shipments in case of arrival delay.  Port of Hamburg Model Animation  TU Darmstadt partnered with HHLA (port of Hamburg operator), TFG Transfracht (intermodal operator), and Hapag-Lloyd (deep-sea carrier) to gather data from all of the parties involved in the transportation chain to test the work of the system with container ETA. The partners provided the following input data for the research:   The terminal side included transshipment, restacking, dwell, and train loading times, block storage capacity, restacking rate, and more.   The intermodal operator data included the number of trains, capacity, and utilization per train. The deep-sea carrier provided the number of containers transported, and delay and unloading times.  Train capacity utilization before (red) and after (blue) container                   ETA introduction in the three policy cases  The data was used to build distribution functions in the model.  The model reproduced the transport chain by utilizing the system dynamics approach. It simulated the continuous flow of containers from the arriving ships, through the terminal, to the trains. Discrete and dynamic events simulated containers arriving at the terminal from the vessel and loaded on trains. The modelers also added extra features to the system dynamics model, such as functions and programming to add decision rules for the intermodal operator. The model included an interface where users could graphically set up various modes and parameters, like turning on/off container ETA use, choosing intermodal operator policies, setting up train parameters (containers per train, arrival rate, etc.), and more.  To make the results stronger and more comparable, the modelers ran Monte Carlo simulation experiments, which contained multiple simulation runs for every input volume, each with randomly generated parameters. Outcome The results of the simulations for the different policies showed that the introduction of container ETA information would increase the capacity utilization of the intermodal operator’s trains (see picture). The ETA availability would have the best effect on the system if the intermodal operator used the second policy: form a pool of loading-ready containers and send containers from this pool in case of a delay.  For the situation with large vessels, the application of ETA improved the capacity utilization, but did not allow it to achieve the acceptable level of 75 percent. Large vessels have a strong impact on transshipment processes, and the ETA introduction was not meant to improve this process.  Also, simulation showed that the use of the suggested policies alone, without container ETA introduction, would not change the situation.  The results were presented to the partners at workshops. For some maritime transport chain industry practitioners, the idea of container ETA introduction did not seem intuitive. Simulation proved to many of them that it would improve the situation.  The simulation results showed that container ETA availability would allow port authorities to shorten container dwell time in terminals, and allow intermodal operators to increase train capacity utilization, and, ultimately, container shipment time.  In addition, the port of Hamburg users focused on the export direction of containers. Here ETA of containers could improve the capacity utilization of container vessels.  Further research plans include testing the container ETA suggestion in Asian and American port environments, and then evaluating the economic influence of ETA introduction on each individual user. The researchers also plan to introduce agent-based modeling elements to the system dynamics model to track the movements of each container.     Evaluating Healthcare Policies to Reduce Rates of Cesarean Delivery Link:  Tags: Healthcare, Social Processes  Problem Cesarean delivery is a method of childbirth in which a surgeon cuts through a pregnant woman’s abdomen and uterus to deliver the baby. The more natural method of childbirth is vaginal delivery, in which the baby leaves the mother’s uterus through her vaginal canal. Ideally, cesarean delivery would only be used when vaginal delivery would endanger the life or health of the child or mother. It involves major abdominal surgery, which is accompanied by much greater risks for both mother and child than vaginal delivery. Cesarean delivery also costs about 50 percent more. Over the last 40 years, the United States rate of cesarean delivery has increased dramatically, from 4.5% in 1965 to 32.8% in 2012. Many of these cesarean deliveries, some researchers say 50 percent of them, are unnecessary. In 2009, childbirth related hospitalizations accounted for 7.6 percent of all inpatient costs, totaling $27.6 billion.   Model Animation on the Region's Map  The challenge of reducing the cesarean delivery rate has been recognized by numerous researchers for years. For the first time, in research conducted for the Washington State, Alan Mills, FSA MAAA ND, a research actuary, and his colleagues reproduced this part of the United States healthcare system in a simulation model to allow the stakeholders, including health agencies, insurers, clinicians, and legislators, to test their assumptions on the model to find the right solutions.  Solution The researchers approached the problem on an individual state level as states differ in terms of birthing environment, including healthcare infrastructure and Medicaid conditions. At this stage of the project, they looked at four states: Washington State, Illinois, New York, and West Virginia, because they wanted a broad range of cesarean delivery rates.  They decided to choose the agent-based modeling method because it allowed the modelers to reflect the behavior of many heterogeneous agents acting independently. Moreover, agent-based modeling allowed them to reflect the network effects and feedback, when women and obstetricians make their decisions based on what their friends and colleagues do and previous experiences.    General Agent Behaviour in the Model    The model included the following types of agents:  Women  Obstetricians  Certified nurse midwives (CNMs)  Licensed midwives (LMs)  Hospitals  Health insurers  Medicaid  The researchers parametrized the agents with real world data imported from Excel. For the private information, they generated synthetic data based on the real data.  It was important to simulate the key behaviors of the agents correctly to reflect the real decision making process. The simulated behaviors included women choosing birth attendants (obstetricians, CNMs, or LMs), birth attendants choosing hospitals of association, delivery attendance (vaginal or cesarean), the process of submitting a claim to insurers/Medicaid, and payment.  All of the behaviors were based on the same template that had ten main components (see picture). For instance, women chose their birth attendants based on their childbirth month (input data), motivation (desired birth attendant type), and attributes (geographical location). The model had an interactive interface so that the users could play with parameters (motivations, geographical distributions, etc.), run experiments, and see how different policies would affect system behavior. The results window included financial and population outcomes so the users could easily see and compare the effectiveness of various strategies. The model also included visualization on the map, where all of the agents were animated, and the problem was given a geographical perspective.  Results With AnyLogic, the researchers created a game oriented, simulation based, learning environment for the stakeholders to better investigate the problem and test possible strategies. The training sessions that used the simulation model significantly increased the stakeholders’ understanding of the problem and could lead to the development of better policies to decrease the unnecessary cesarean delivery rate.  The primary results of the experiments showed that the previously suggested healthcare provider payment reform would not work alone, and the stakeholders would need a multifaceted strategy. Also, if retooling the workforce was chosen as part of the strategy, it would take a lot of time.  This was the first simulation model in the world addressing the high cesarean delivery rate issue on a regional level.  Watch the video of Alan Mills, FSA MAAA ND, presenting this case study:       Evaluating Hospital Inpatient Care Capacity Link:  Tags: Healthcare  Problem When creating a modern hospital, healthcare specialists face a never-ending list of important questions to ask and important decisions to make. Stockholm County, Sweden was in the process of building a new, highly specialized hospital. The Health Administration of the county questioned whether they would get an acceptable level of care production with the current investments and reasoning concerning various operational and strategic issues. To find the answers, they used simulation modeling in AnyLogic.  In cases when simulation modeling is used to address more sophisticated issues, the first model developed and used is seldom the last one. Using simulation correctly almost always raises both the discussion and understanding of the issue, which often leads to new questions asked and a need to expand or change the scope of the model. Whereas the first version of this model focused on the total capacity and possible amount of care production, the second version allowed the users to experiment with various patient allocation scenarios for the inpatient care. The second version also enabled them to divide patients into various clinical groups with unique traits and provided a decision-support tool to balance the way these needs were addressed by different inpatient care resources.  Solution To best solve real-world problems with simulation, the person in charge of modeling also has to have a strong understanding of strategic and/or operational challenges in the industry (in this case – healthcare), and not purely modeling aptitude. This ensures that he/she is able to grasp the essence of the problem, interpret, and sometimes reformulate it, to build a better model that adds value. In this project, it was competence in the fields of strategy, management, and production that allowed the modeler to evolve his vision of the problem, and change the simulation objective to find answers to the questions that the stakeholders really needed to ask.   First version of the hospital model  Before the simulation project, the effort had a tendency to focus on the various organizational parts of the hospital (emergency department, inpatient care, operation wards, etc.), but not on the hospital as a whole. This is a dangerous route to take, since a hospital is a system by itself and the interdependencies must be taken into account. The model was designed to reflect this systematic approach.  In the first model, each organizational part was described using discrete event simulation in a very simplified form, in

 

Section 18 of 60
terms of key processes and primary resources, and then these pieces were tied together to model various possible patient flows. After this, it was possible to experiment with an infinite number of hypothetical scenarios, by experimenting with Demand (incoming patients), Resource levels (number of beds, rooms, etc.), Times with variations (expected inpatient care time, expected operating time, etc.), and Strategies (how patients were expected to navigate through the hospital).   Second version of the hospital model The second version of the model separated the inpatients into clinical groups. This was done to better take into account the very different traits and needs in different cases, as well as to better see whether a discussed allocation of the patient categories, given the different alternatives, was well balanced or even possible. The model was also given a different appearance, so that it was visually clear that this was a new model, even though most of the logic was the same.  Both versions of the model focused on the physical resources rather than human and personnel resources. This approach was chosen because, in dimensioning decisions related to building and investments, it is more important to consider what capacity limits are set by the physical resources. Another reason was, that to best model human resources, one has to take into account scheduling, planning strategies, and various competence categories and roles, which are relevant in a micro model, but often not appropriate to consider in a meso abstraction level case, like this one. Outcome The statistics provided by the model included achieved care production and resource utilization rate. This helped to gain an understanding of whether the hospital capacity was acceptable.  Thanks to the model, care production could be estimated in terms of operations carried out, inpatient care delivered, etc., given the scenario and circumstances under consideration. By doing this, possible risks/challenges could be pinpointed and discussed while looking at resource utilization. Major conclusions made with the model confirmed what many stakeholders had claimed for a long time, that the resource level for inpatient care was far lower than needed. But through the model and argumentation of the simulation project manager, the decision makers were encouraged to take action to address the problem.  In complex issues, often with conflicting interests, major benefits of using dynamic models are more often qualitative than quantitative (even though figures also play an important role). The reasons for this are the following:  Models gather all the relevant dimensions, issues, parameters, indicators, etc., in one place (the model), and make it visual. This often serves as a catalyst to raising the understanding and level of discussion. Models allow for an infinite number of scenarios to be considered, so that all of the parties with conflicting interests can test their assumptions (and in the end understand opposing opinions better).  Healthcare is an area where decisions are heavily influenced by politics. Moreover, major competence and culture in this area usually include a high level of medical knowledge and evidence-based thinking, rather than operations management and system thinking. That is why it is necessary to make issues like the ones covered in this project easily understandable. Simulation can help decision makers in the area of healthcare push discussions in a better direction and, as a result, make better decisions.  This model has also acted as a template for other models in cases where the issue was to analyze operations at existing hospitals. This model’s logic can be seen as generic, enabling planners to look at a hospital as a system and to evaluate whether the system is in balance or not.    Evaluating Introduction of Warehouse Automation Systems Link:  Tags: Warehouse Operations  Challenge Symbotic is a warehouse automation solutions provider. Its systems are based on mobile robots that can travel freely throughout a dense storage structure, accessing products in all locations and handling them at a very high throughput rate. The main advantages of the Symbotic System are its ability to operate in three dimensions, and their sequencing and palletizing algorithms that build stable, store-friendly pallets at maximum throughput. The company needed a tool to help their customers learn the impact of warehouse reorganization and compare capital investments against expected operational savings before the actual introduction of automation systems. This tool had to be easily adjusted to the case of each specific client. Symbotic specialists decided to use AnyLogic simulation software for this purpose because it could precisely estimate costs and visualize processes inside warehouses in 3D, as well as create models that could be easily reconfigured for multiple projects.  Solution The models that Symbotic engineers created for the company’s clients simulated the environment and operations in their warehouses with a high level of detail. More specifically, this included:  Scheduling and assignment of dock doors, product flow between the dock doors and several different warehouse locations on both inbound and outbound.  Tracking and combining order-specific product flows from different streams on the outbound.  Operations of labor and resources like loaders, unloaders, forklift trucks, de-palletizing and palletizing cells, all simulated as agents.   Warehouse Model 3D Animation  Each agent type had its own properties, like speed, reliability, operation times for equipment, dimensions, cases per layer, and layers per pallet for each SKU. The input data was taken from real life information. It was especially important for the modelers to simulate various interactions between the automation system and human operators. This included receiving incoming deliveries, replenishing stock, both automated and non-automated parts of the warehouse fulfilling customer orders in an optimized sequence, and combining them at the dock-doors in the exact manner it would happen in the projected system. Models also took into account system reaction in case of equipment breakdowns, shift schedules and lunch-breaks for the human operators. The models featured warehouse 3D animation and graphical display of key metrics to provide strong presentation instruments for salespeople and allow the clients to see their future reorganized warehouses in action. Results The models were tested on the historic order data from each client, usually for a six month period. To compare warehouse operations, with and without the automation solutions introduced, Symbotic engineers gathered the following statistics in the model:   Throughput capacity (cases per hour handled)  Number of human operators required and associated costs  Number of warehouse resources required (e.g. dock doors) and their utilization  Time needed to fulfill daily outbound shipment orders, especially during the peak periods  These outputs were used by the clients to evaluate warehouse design alternatives and justify capital investments.  AnyLogic allowed Symbotic to design their simulation models in a way that made it possible to easily change warehouse layout, operating procedures, SKU properties, etc., so that model elements could be reused in multiple projects with relatively small effort. Also, 3D animation allowed the company to utilize these simulations as a powerful selling tool.   Project presentation by Dr. Larry M. Sweet, CTO at Symbotic LLC:      Exploration of Satisficing Behaviors in a Complex Financial Economy Link:  Tags: Business Processes  Problem Economic analysis and systems modeling together are not used very often in economics. As a result, there is a gap in economic analysis and simulation.  Economics is composed of numerous theories, but in this case only three were identified.  The first states that expectations are based on history. These expectations are projections of consumer demand which drives expansion and contraction in the financial cycle.  The second says that decision making is driven by satisficing behaviors – the choice of the first satisfactory outcome, rather than the optimal one.  Finally, prospect theory, which attempts to understand how problems are formulated and options constructed to make better decisions.  Solution The solution was to develop an agent-based model of economic and financial cycles using the three previously mentioned theories.  This could be done by developing a behavioral agent-based model to better understand the role of decision making under uncertainty in a complex financial system. Then, simulating multiple financial system outcomes by varying the range of conditions and seeing tangible changes in economic settings.   AnyLogic was chosen as the right tool for the researcher because it could visualize outcomes of a system and behaviors, connect to data sources, or create and manage data, deal with highly complex agents with decision capabilities, and so on.  The design and the theory needed to be connected to build the model. The simulation could not cover all aspects and so limitations or environment considerations were built into the design. These included:   Internal and external system activity, data, and randomness – behavioral agents determined their own destiny and there was no “real world” data.  Time and history.  Financial relationships.  Financial leakages in the model – not a closed loop system, so all aspects of the model could be tested.  Agent population and lifecycle – a fixed population size with infinite lifespan.  Agent communication, connectivity, and spatiality – “small world” adopted.  Agent relationships, trust, and reputation.  State and Strength labels adopted across all behavioral agents.   Each agent in the model had different attributes and was assigned to different tiers as shown in the diagram below.   Input data for the simulation (click to enlarge)   Tier 1 are the behavioral agents in the simulation and the primary drivers of the system.  Tier 2 agents are non-behavioral agents used as a store of value.  Tier 3 agents are transactional and can be created and destroyed.   The Economic Agent (EA)  was the center of the model. It came from the decision-making process and so it had a decision engine. It received inputs and could define outputs. As a result of these outputs, it attained either a positive, negative, or neutral state, which could be further strengthened e.g., very strong positive.  Defining the state resulted in an intent to act. This intent could result in future projections of price, for example. The EA would also use historic information to evaluate decisions in the future using its reasoning ability.    How state is determined in the economic agent (click to enlarge)   The Asset Market (AM) received a buy or sell order from the EA and so acted as a clearing house to ensure that the system continued to operate.  The Asset Agent (AA) acted as a portfolio and continuously updated throughout the cycling, holding the three key states of positive, negative, or neutral.    Overview of asset market and asset agent (click to enlarge)   The Financial Agent (FA) looked at economic and financial policy. After looking at the attributes of the EA, it determined the asset portfolio equity and the financial state. It also considered the overall aggregate of the system.  The Loan Agent (LA) completed the transactional mechanics which included holding the values of such important factors as interest rates, credit defaults, and so on.    Overview of financial agent and loan agent (click to enlarge)   The controlling Central Agent (CA) facilitated a number of macroeconomic functions, which included production, employment, and more. This CA sent messages with this information to the other agents in the system.     The Asset Market and CA were designed to shape emotions. Thus, they sent signals irrespective of the actual situation in order to start shaping the agent population and determine how they were going to behave.  The simulation performed by the researcher focused on 26 individual conditions and within these, there were a total of 78 options and so a huge range of behaviors were covered. As a result, the permutations were extensive.  Five themes were covered in the research:   Population participation.  Perfect information.  Strong policy.  Weak policy.  A baseline generic example to compare.   Results The results of the simulation enabled the researcher to reach some conclusions regarding decision making strategies and behaviors in a complex financial economy. These included:   “Rational expectations” is not a viable economic method.  There is a policy influence, and the stricter the policy, the more stable the market.  Liquidity drove the market, and more liquidity resulted in more volatility.  System crashes were averted by cash being generated in the past.  Human behaviors and psychological reactions drive the market more than fundamentals.    The case study was presented by Dr. Dennis Feher, Sydney University, at the AnyLogic 2021 Conference.  The slides are available as a PDF.     Foam Concrete Manufacturing Processes Analysis and Optimization with Simulation Software Link:  Tags: Manufacturing  Problem The SET-Holding’s newly constructed foam concrete factory in the Oryol Region could not reach its designed capacity. The plant management blamed the poor performance on the human factor: the manufacturing technique was innovative and challenging for the factory employees. SET-Holding recruited consultants to analyze the situation and increase the production capacity. The consultants decided to build the factory’s simulation model using AnyLogic software. Solution  Production optimization simulation model built with AnyLogic software (click to enlarge)   The simulation model in details imitated foam concrete manufacturing processes including mix preparation, mix pouring, cutting, separation, autoclaving, unloading, movement of trolleys, cranes, workers, and pallets. The required parameters were loaded into the model from an external database that stores the operations duration measurements results: for each operation there is a range of results and possibilities to select from the given values. The model also demonstrated:  Raw material bunkers’ fullness Working and idle time of the mills Storage Autoclaves’ condition  The plant equipment’s state is presented by separate graphs, which helps the user visually estimate the ratio of the equipment’s working time to idle time. The simulation model allowed them to track various enterprise performance parameters: percentage of resources utilized (cutting lines and molds employment, queue for separation length, and the number of pallets in use), daily statistics on the finished mass compressions number (separated and disassembled), the condition of all molds to be poured, and the

 

Section 19 of 60
number of idle molds. Furthermore, with the graphs, the productivity trends and level of molds’ usage could be monitored. Outcome  Manufacturing process simulation model built with AnyLogic software (click to enlarge)   The designed model allowed the factory management to review the enterprise activities in detail. The model precisely simulated all manufacturing processes, and therefore it was a perfect fit for problems analysis, decision making, and predicting changes. The manufacturing simulation model enabled the management to see the workshop performance operational picture and foresee the course of events in case of various breakdowns and abnormal situations. Due to its accuracy, the model serves as a reference for the workshop activities. In particular, the model can be used to determine standards in manufacturing practices. In comparison with such benchmarks, it would be handy to record irregularities and determine their causes. The visual presentation makes the model demonstrative and easier for perception than usual tables and graphs. As a result, the consultants managed to find solutions that significantly improved the factory’s productivity. Due to the opportunity to experiment with the simulation model in a safe digital environment instead experimenting with the real factory, all the necessary changes were implemented quickly and without disrupting production.  Here you can see the model of the autoclaved aerated concrete factory, which is similar to the one that is described in this case study:     Forecasting the Future Development of Russia with a Population Dynamics Simulation Model Link:  Tags: Social Processes  The Moscow State University supercomputer, Lomonosov, predicted the social and economic development of Russia for the next 50 years. The project goal was to acquire the experience of building such population dynamics simulation models in Russia. The project team was comprised of two specialists from the Central Economics and Mathematics Institute (CEMI) and three specialists from Moscow State University (MSU).    The population dynamics model included such factors as population number change in particular regions and in the whole country, GDP dynamics, amount of investments in industry, and country and innovation industry EVA change.   200 supercomputer processors were used in the evaluation. The whole process took only 1 1/2 minutes. The population dynamics model was also tried on 1000 processors, and then it took only 16 seconds. In addition to Lomonosov, the evaluation was also carried out on supercomputers MVS-100K (Joint Supercomputer Center of Russian Academy of Sciences) and Chebishev (MSU). Regular computers are unable to calculate such models (top productivity of Lomonosov after its modernization is 510 TFLOPS).   Population dynamics simulation showed that in 50 years, the north territories’ population will be almost completely disbanded and the Siberian and Far East populations will significantly decrease. South regions are expected to meet population growth. According to the population dynamics simulation model, GDP on the whole is expected to grow. Yet, the specialists who took part in the project warn not to take the results as an inevitable forecast, as the population dynamics model did not take into account such possible influential events as war or a huge epidemic.   The Russian population dynamics model was developed with the use of AnyLogic simulation software. To create this model, CEMI specialists used about 100 million agents. The data used in the population dynamics simulation was taken from the Federal Agency of State Statistics and from Russian monitoring of the economic situation and the health of the population.   Getting the final results, the scientists say, wasn’t the only objective. They also wanted to get experience of model conversion, of transferring a model from a regular PC to a supercomputer, because there is a lack of such experience in Russia. One difficulty was the parallelization of the Agent Based population dynamics simulation model parts for the different computational nodes of the system. Another complication was the technical problems involving model transfer to supercomputer program code. These were the tasks of the MSU specialists.   Large scale social and economic models were also created by AnyLogic users in the USA, Germany, and Sweden. For example, RTI International, a research institute in the USA, used this software to simulate HIV/AIDS proliferation via drug addicts. Also, the United States Census Bureau worked with AnyLogic, using System Dynamics, for a California Hispanic population dynamics simulation.       GE Electric Vehicle Transportation and Charging Network Analysis Link:  Tags: Transportation  Overview  General Electric Company (GE) is an American multinational conglomerate operating mostly in the power, renewable energy, aviation, and healthcare industries. In 2020, GE ranks among the Fortune 500 as the 33rd largest firm in the United States by gross revenue.            To develop innovative technologies for GE’s business interests, GE Global Research (GEGR) was established, and it has become one of the world’s largest and most diverse industrial research labs. Improving business through technology, GEGR has developed expertise in simulation optimization and operations research.   Problem        When GE announced its commitment to electric vehicles (EV), it led to the need for advanced work in various related areas. Much of the technology was still developing and only just becoming widely available commercially, so many issues needed to be solved.           GE deployed a large fleet of electric vehicles for personal use, produced WattStation electric vehicle charging stations, and was conducting many other activities related to EVs, but a key question the company wanted to answer was: How would these markets evolve?           Relatively widespread use of EVs and the need to support them with services and charging network, prompted GEGR to conduct research into the related business demand and new technologies. Furthermore, any changes to electric power distribution patterns and methods were also of interest to GEGR.           In short, by conducting an EV transportation and charging network analysis, GEGR team sought to find answers to the following questions:    What does the EV adoption curve look like? What will be the impact on the electricity distribution network? How will the charging infrastructure evolve? What new business models will emerge?       GEGR decided to test if advanced simulation modeling could help answer these questions. To this end, the research team was tasked with building demonstration prototypes:    To evaluate modeling techniques as a tool to help gain insight, forecast, and make decisions in emerging business areas. To identify potential methods and approaches, as well as their integration possibilities that would help support the research.  Solution  EV Driver Statechart      For this EV transportation and charging network analysis project, GE Global Research chose AnyLogic multimethod simulation software because it supports the agent-based modeling method. Agent-based modeling allowed the engineers to describe the EV market as a system of individual agents that make their own choices. For example, consumers could individually decide whether to buy an EV or a conventional internal combustion engine (ICE) car.        With agent-based simulation, the team could also model adaptive driver behavior, such as when a driver takes an extended journey (as opposed to a regular home-work-home route) and needs to decide when and where to charge. Additionally, potential EV buyers might have very different priorities due to variables such as income, commute distance, personal preferences, and so on.         For the project, the team developed two prototype models: a granular EV adoption model and a charging network simulation. The agent-based models made use of AnyLogic’s Java platform to embed various rule-based functions and variables, and the software’s convenient graphical visualization capabilities.   The granular EV adoption model        As there was a lack of historical data on the potential-customer decision-making process, the GEGR team decided to simulate the process and used the Example-Based Evidential Reasoning (EBER) approach. This allowed the forecasting of how multiple factors could influence people’s decisions. The approach was developed by GE and has been used in several projects related to risk management and competitive pricing.            For the Granular EV Adoption model, the preference factors were selected by the team themselves. They included:    Vehicle utility (reputation, range, battery life, etc.) Consumer attributes (commute distance, income, home charger availability, etc.) Finance (payback time, annual costs, etc.) Location (climate, government incentives, infrastructure, etc.)  Simulating EV Adoption in the NY State          The model’s inputs included outputs from other models (for example, financial simulations that calculated the payback and operational costs for an EV) and data from various open-source databases.            In the EV transportation network model, all factors and preferences, such as those related to an individual's finances and location, word-of-mouth, and vehicle availability, were organized in a way that corresponded to one potential buyer. Additionally, it tracked the transition of an agent from being a potential buyer to a user of either an EV or ICE vehicle. The output was the relative preference of a potential consumer for buying an EV over a conventional ICE vehicle.            The adoption rate was defined by tracking the number of EV and ICE vehicle drivers to give a picture of the total population. For a given geographical area, New York State, the model summarized adoption rates over time. The results could be displayed on a map by zip code region, or on charts.   The charging network simulation           The simulation was created to test the impact of various charging network designs on both the satisfaction rate of EV drivers and the utilization of charging points. The team also wanted to find answers to the following questions:    What is the ROI for implementing a certain charging network design? Where are potential locations for charging points? How many charging points are needed?          Using the data on EV adoption rate from the previous simulation model, the team built a prototype simulation of EV usage in eleven New York State zip code regions with the help of GIS mapping. Using agent-based modeling, they created a custom library of objects which they could further use in other projects. For each object, the team could set properties and behavior so that, for example, households could be owned or rented and have different numbers of vehicles and drivers.           When run, the simulation showed the behavior of drivers, their movements, and decisions, based on logic rules which had been set in the model. It was possible to observe how the status of each object changed over time on a map or via an object’s statechart. By adding or removing charging points, they could alter the environment and monitor how it influenced potential EV owner satisfaction metrics.    Results           The GE Electric Vehicle transportation and charging network analysis project showed that simulation is a powerful tool for forecasting and planning in newly emerging business areas. The models developed by the research team were useful for:    EV charging station manufacturers to understand demand. Store owners to simulate how installing EV chargers could influence their business revenue. Charging network operators to decide where to place chargers to maximize their utilization. City planners to decide on charger network design that would maximize returns on investments and the EV adoption rate.          AnyLogic software allowed the GE research team to build sophisticated models with numerous entities, including agents with decision-making abilities. The models made use of AnyLogic’s multimethod modeling possibilities and built-in graphical visualization capabilities.       GE Manufacturing Plant Uses AnyLogic for Real Time Decision Support Link:  Tags: Manufacturing  Overview In 2012, GE opened a new battery manufacturing plant in conjunction with the launch of an innovative energy storage business. The new Durathon battery products, which are half the size of conventional lead acid batteries, but last ten times longer, are the result of GE’s $100 million initial investment in battery technology developed at GE’s Global Research Center (geenergystorage.com, 2014). Expanding the facility doubled production, added 100 new jobs, and brought the total factory workforce to 450 when at full capacity (geenergystorage.com, 2014). Problem GE’s exciting opportunity brought on many new challenges, such as increasing production throughput and yield under evolving processes and uncertainties, and reducing manufacturing costs in order to gain market share. With over 27,000 variables tracked daily, GE was equipped with a lot of data, but they lacked the means to answer questions, analyze the data properly, or test and evaluate options.  The GE Global Research Center sought out a powerful and flexible tool to analyze, not just the specific process, but the manufacturing system as a whole. Solution GE chose simulation modeling because it offered a dramatic return on investment, and simulation enabled the visualization of their system over time. The long term impacts could be evaluated with increased accuracy compared to using traditional computational, mathematical methods.    Full plant simulation development process AnyLogic modeling and simulation software was chosen for its agent-based and multimethod modeling capabilities. It allowed GE to solve problems in any area, combine models, input multiple data sources, and run models anywhere for complete collaboration and real time decision making. GE’s simulation models, built in AnyLogic, focused on determining baseline capacity with variability, simulating system dynamics, identifying bottlenecks, planning production ramp-up, guiding expansion, facilitating continuous improvement, evaluating P/E investment, and achieving real time production optimization.   Input Data:   Process flow  Type of machine (continuous, batch, single machine, and special machines including tube press and kiln)  Machine cycle time  Yield  Machine MTBF/MTTR  Staffing plan  Setups, cleaning or special non-std work  IPK’s  Outcome GE’s full plant simulation modeled manufacturing flow, and it was used for capacity planning (identify, evaluate, and prioritize projects), quantitatively analyzing bottlenecks, and evaluating improvement options. Real time operational decision support allowed GE to answer questions such as, “Do I need an additional

 

Section 20 of 60
operator in the next eight hours?”, quickly, simply, and accurately, by running what-if scenarios and optimizing results.  AnyLogic software gave GE the tools to make the right probabilistic and multi-scenario informed decision, which resulted in clear visibility of day-to-day operations, increased production throughput, and decreased manufacturing costs. Watch the video of Shanshan Wang, an Operations Researcher from GE Global Research, presenting this project at the AnyLogic Conference:      Handling Total Care Need for Dialysis Patients Link:  Tags: Healthcare  Problem The County of Stockholm (Sweden), like any country or region, experiences a continuous need to handle the healthcare necessities of various patient groups. Each group can be seen as a subpopulation, with its own distinctive traits, characteristics, and challenges. The discussed simulation project focused on the dialysis patients, a group who needs to visit caregiving facilities frequently. When the simulation model was originally created, the major focus was to get a better understanding of the consequences of having several decentralized caregivers, compared to more compact centralization. Most models are enhanced and further developed to handle new ideas and needs, and so the current model also looked at handling different patients in different ways (some handle their need in their homes, some visit a caregiver, some need assistance, and some do not). Solution This problem was seen as a macro problem and was best handled with a multimethod modeling approach. Agent-based modeling was used to model the environment (the county) and the patients, while discrete event modeling was used to model caregivers and simple caregiving processes. The visualization played a vital role in supporting understanding.  In this case, a number of conclusions could be drawn even before running the model through the animation and visualization. Given the scenario described in the input data (provided through a number of Excel files, and the changes made possible with various interactive controls in the model interface), the "center of gravity" was visually shown – both from the need (patient) perspective and from the capacity (caregiver). If these centers were close to each other, the situation was statically sound. The simulation then helped the modelers understand if this was also the case dynamically, over time. Outcome In most modeling situations, and especially in healthcare, the value added by models is given through a combination of quantitative consequence indicators (figures), and a more general understanding of the issue – which is of a more qualitative type. In this case, examples of indicators were achieved care production (and knowledge of whether it was sufficient), travelling distances for patients, and the utilization of resources, which resulted in a better understanding of whether the capacity was acceptable.   Model Animation on the Map  Qualitative outcomes allowed modelers to:  Drastically raise the understanding of the whole challenge among decision makers and stakeholders.  Add the geographical dimension and perspective to the whole issue (which is often forgotten).  Act as a catalyst to support and raise the level of discussions, and make conflicting stakeholders understand that the issue was complex.  Summarize relevant perspectives, dimensions, and challenges visually and dynamically.  The qualitative aspects usually contribute even more than the quantitative ones (enabling a better decision making process), even though the quantitative output has a higher quality than what could be obtained through other means. This model is currently being used as a decision-support tool by the specialists involved in addressing the challenges of the dialysis care in the County of Stockholm.    Healthcare Decision-Support by Hybrid Simulation – Mobile Stroke Units Link:  Tags: Healthcare  Problem           To understand both the medical and economic impact of new healthcare technologies, they must be evaluated before the design and development phase begins. Therefore, a decision support system approached through hybrid simulation is applied in the case of Mobile Stroke Units (MSUs).           Stroke causes severe disability, produces high costs for care and rehab, and its incidences are increasing due to an aging population. Thrombosis causes most strokes and if possible, should be treated with thrombolysis (not in the case of hemorrhagic strokes and after 4.5 hours have elapsed). Currently, the process of transportation and internal hospital administration causes the patient to lose valuable time.            MSUs have been suggested as a possible improvement. MSUs start with diagnostics and therapy steps at a stroke occurrence location. The purpose is to reduce the alarm-to-therapy-decision-time in order to prevent severe disabilities of people and high costs.   Objectives           The goal is to assess the medical and economic impact of MSUs in comparison to the current situation and optimize the geographic distribution. The main metrics are the impact on the alarm-to-therapy-decision-time, costs, and other relevant output parameters. A metropolitan area (Berlin) and a rural region in Germany are used for comparison since there are few hospitals, and they are different structures.           Figure 1: MSU scenario animation screenshot   Implementation in AnyLogic           The project has been realized with AnyLogic; which is well suited to develop models using all necessary simulation paradigms and enables the developer to add their own Java code to model unique and custom problem solutions. AnyLogic also allows the creation of illustrative animations in order to enhance communication with domain experts.           Figure 1 shows a screenshot of the MSU scenario animation. The MSUs are located in predefined positions and can be sent by the dispatcher when a stroke case occurs. If no free vehicle is available, a usual rescue service will be sent to bring the patient to the next hospital. Wrong decisions and unnecessary MSU missions (that do occur in reality) are modeled as well as cost values for the various actions and interventions. Stroke patients are monitored until 10 years after affection to enable long-term cost analyses.           Excel tables with varied parameters are used as input files for different years and are loaded during simulation runs. To evaluate multiple scenarios of healthcare decision-support a library has been developed. Different modules (e.g., demography, error injector) can be connected in order to add certain characteristics to a dedicated scenario. A positioning tool allows defining home locations for MSUs and hospitals on a map by clicking on the area before running a scenario. In the selected papers below, further modeling and implementation features can be found.   Modeling Approach           To perform a simulation and particularly within the scope of healthcare decision-support, macro-simulation, as well as micro-simulation approaches, are necessary to increase model accuracy. This is where one can benefit from multi-method and hybrid simulation paradigms. System dynamics is used for modeling at a distant perspective, while discrete-event and agent-based techniques are appropriate for detailed modeling at individual levels. Patients are represented by agents, and medical workflows are embedded in system dynamics models which represent aspects such as demography, economy, and epidemiology. The impact of medical technologies is represented by quantitative parameters.           Figure 2: Agent-Based, Discrete-Event and System Dynamics Model examples   Outcomes           One significant result from this case study is that MSUs do not automatically lead to more patients with thrombolysis as a treatment. However, those who are treated with thrombolysis receive care earlier, resulting in a reduced probability of severe disabilities. This is a clear medical benefit. Research also shows that a broad distribution (e.g., uniform distribution) of MSUs on a map leads to better results in contrast to centralization in a few locations (e.g., stations, hospitals). Furthermore, the simulation has shown that, in rural regions in Germany with only a few hospitals, such vehicles are not profitable, as there are not many affections per year, and most of the people live close to an urban center with a hospital able to treat stroke. The above case study results may differ in countries with less specialized hospitals.   Conclusions           AnyLogic allows the development of detailed models for healthcare decision-support. The configurable MSU simulation model helped the modelers to answer important questions about the impact of medical and health economics. Regulatory agencies, companies, researchers, and other decision-makers can use the results to optimize an MSU roll-out in any country and to improve stroke diagnostics and treatment in the future. Long-term monitoring of stroke patients allows the comparison of saved costs to the additional costs of MSUs, leading to a basis for an investment decision. This work has been conducted by Prospective HTA (funded by the German Government) in conjunction with doctors, engineers, and health economists, from both industry and academia.           The simulation of large-scale complex systems (e.g., in healthcare, automotive, industry, and energy) is a major field of work at the University of Erlangen-Nuremberg Computer Networks and Communication Systems Group.        Healthcare Resource Utilization Modeling Link:  Tags: Healthcare  Overview   Lean Business Services  is a leading company in the service and development of the healthcare sector in Saudi Arabia. Lean Business Services strives to improve the accuracy of public health indicators in the Kingdom of Saudi Arabia by developing innovative solutions thus boosting the health sector services.  Problem Lean Business Services wanted to help the hospitals' emergency departments (emergency rooms / ERs) with healthcare resource utilization. Patient crowding in ER caused a series of negative effects. There was poor patient treatment, lengthy stays for patients, and general patient dissatisfaction. One way for ERs to optimize their performance was more efficient management of resources and personnel in their operations.  Congestion in emergency rooms caused several issues:   An increase in patient length of stay (LOS)  An increase in inpatient waiting time  An increase in inpatient complications  A decrease in healthcare quality    Issues caused by congestion in ERs    Solution Lean Business Services intended to resolve these problems in emergency rooms. For a more informed management of resources, Lean’s engineers developed a simulation model. The model developers wanted to understand how to create a smooth patient flow and decrease the length of the patient stay. By doing so, they could simulate different situations in the model without risks and redundant costs.  The building of the ER simulation flow included four steps:    Study and analyze the city's historical medical data  Simulate exact patient flows in emergency rooms   Build simulation designs as the ER blueprint  Simulate patient and health resources in ERs  The simulation model was implemented using the   discrete-event  paradigm for modeling the operations of emergency rooms and modeling patient flows.   Simplified model flow (click to enlarge)   The model provided several what-if scenarios to see how the changes could affect the ER operations.    What-if scenarios in the model (click to enlarge)   The Lean Business Services engineers simulated six different categories in ERs. For each category, the simulation provided different outputs.   Simulation provided different outputs for each category in ER      Results  The simulation model gave opportunities for the ER managers to make decisions by analyzing and evaluating different scenarios before implementing them in real life.  The modeling enabled them to:    Identify the full capacity limit in emergency rooms. The simulation could help find the maximum capacity of patients in the hospital  Identify the root cause of ER problems  Decrease patient waiting time   Better utilize human resources, e.g. transfer from another unit  Understanding and improving resource utilization as well as identifying areas of improvement became possible thanks to the simulations using AnyLogic software. In 2D and 3D views, a friendly user interface showed the operations of the emergency department and the exact patient flow within the ER and ran adjustable scenarios with their expertise.  Watch the video about this case study presented by Lean Business Services at the  AnyLogic Conference 2021.       High Frequency Bus Route Optimization to Avoid Bus Bunching  Link:  Tags: Transportation, Road Traffic  RTC is the public transit agency for Quebec City, Canada. It ensures the mobility of people in the urban agglomeration of Quebec City by offering public transport and promoting the integration of different transportation solutions.  In search of a data-driven solution to bus bunching, RTC turned to SimWell - an industrial engineering company supporting business decisions with simulation, data science, and optimization, based in US and Canada. SimWell’s solution to the bus bunching problem involved simulation modeling with AnyLogic. Problem Bus bunching is a problem faced by transport authorities around the world and for RTC there is no exception.  In a perfect world, every bus would arrive at a stop according to planned frequency to efficiently serve customers. In reality, several problems can occur and breakdown any well-thought plans: traffic congestion, schedule gaps, mechanical failures, unexpected demand leading to overcrowding and uneven loads, etc. All of these create problems for customers and impact the rest of the transit route.  Bus bunching problem diagram (click to enlarge)  The objective of this project was to improve service quality for the customers while maintaining or improving the quality of work for bus drivers. Using a simulation model, engineers wanted to test solutions and see their potential impact and risk for customers and bus drivers before deployment into the real world. Solution Why AnyLogic? One of the main goals of using the simulation model for transportation system analysis was to figure out the best solution to solve the bus bunching problem. Another goal was to allow stakeholders and decision-makers to witness experimentation and see how solutions physically operate and not just with tables and statistics. AnyLogic’s UI and 2D and 3D animation capabilities fully covered these requirements, providing multiple views and strong features. Implementation flexibility thanks to JAVA extensibility and the ability to package the model as the standalone application was also a big advantage of using AnyLogic as transportation modelling

 

Section 21 of 60
software. The solution allowed the model to run without being installed on end user’s machines. The Solution Infrastructure  Solution infrastructure (click to enlarge)  The model focuses on short-term corrective actions. The long-term decisions are modeled by changing the input data. The historical data used to calibrate the model was taken from bus information systems:   GPS locations and runtimes from Automatic vehicle location system Destinations Automatic passengers counter Capacity Travel time  The information helps reproduce all components of the trip processes and bus capabilities. Different sorts of variability are also captured in the historical data, such as traffic congestion, traffic lights, seasonal factors, overcrowding, etc.  The model is fed from historical data extracted from RTC data warehouse. It allows calibration of different functions of the model, such as travel times, driver behaviors, ridership at the various stops, etc. Then, to test bus frequencies and schedule runtimes, schedules are taken from the RTC planning software. The simulation model provides visualization and information that can be used in decision making. The model supports various kinds of solution simulations: schedule changes, build preventive or corrective action policies for dispatchers at the control center, give the board a decision brief about strategic or technical decisions, such as changing service levels, run on headway management instead of running on schedules, or any other important changes to the process. For the reference scenario, engineers used the data from 2018 and 2019 – before the pandemic, to give a baseline. The team included various data for analysis:  Bus time at each stop (open doors duration, alighting and boarding time, additional constant time (e.g. fare), traffic lights). Passenger arrival rate at a bus stop. Passenger destination probability (how many people board and alight on the different stops on the line). Running time (based on the Markov chains principle, the time for each next pair of stops depends on the previous one).  Summary of available data (click to enlarge)  Key Performance Indicators There were no existing KPI that represent what customers and bus drivers experience in a high frequency environment. Now, with simulation modeling, it is possible to calculate several indicators and compare them to real life data:  Irregularity measure, represents the variation from the planned headway (%); Excess wait time (EWT), represents the additional time customers had to wait (min/passanger); Passengers comfort levels, can only be calculated in a simulation environment.  Result  Model window (click to enlarge)  The simulation model, created by SimWell for RTC, allows experimentation based on input data, and the assessment of multiple ideas and combinations in the search for better solutions to bus bunching. Engineers are currently planning the next phases and may include a  gaming module for training operation controllers. Another possibility is the addition of transportation optimization of the next days’ frequencies and schedules, based on resource availability and runtime predictions.  Additionally, there is a possibility to create a digital twin that could be used to trigger action suggestions for drivers to apply preventive or correcting action to prevent bus-bunching. The case study was presented by Pierre-Olivier Bédard and Nomessi Kokutse, of RTC, and Denis Matarangas, of SimWell, at the AnyLogic Conference 2021. The slides are available as a PDF >>     Highly Automated Production Line Planning and Optimization Link:  Tags: Manufacturing  Problem Centrotherm Photovoltaics AG is a global supplier of technology and equipment for the photovoltaics, semiconductor, and microelectronics industries. Specifically, they needed to:  Determine the type and amount of equipment needed to satisfy the manufacturing capacity they were planning. Evaluate different layout alternatives to improve the throughput and utilization rate. Inspect possible bottlenecks in the material flow. Evaluate operator behavior impact on the factory output. Test the consequences of maintenance within various time frames. Identify system behavior in case of breakdowns.  Identify scrap probability.  Evaluate change of performance during planned downtimes.   The ACP-IT consultants used AnyLogic simulation software to evaluate the manufacturing optimization possibilities. Solution   Manufacturing Optimization — Model Logic   Using AnyLogic Professional’s ability to create and save custom object libraries, the consultants encapsulated their vast simulation experience in the photovoltaic and semiconductor manufacturing industries and created their own libraries, which they reused in many manufacturing optimization projects, including this one. These libraries featured elements built on top of the AnyLogic model development environment and allowed the consultants to easily model different kinds of equipment, material handling systems inside factories, personnel, and production control systems, all specific to the photovoltaic and semiconductor industries. In the Centrotherm project, this solution helped modelers quickly reproduce various aspects of the client’s manufacturing system behavior with simulation.  Once the manufacturing optimization model was built, the experimentation phase of the project began. The consultants tested many parameters to find the best solutions. The input data, that included layout configurations and various parameters, was taken directly from Excel and Access files. Each simulation run reproduced one year of factory operation. First, the consultants experimented with the overall production line design, using parameter variation and optimization. They tested many parameters; including those concerning manufacturing capacity and the number of various pieces of equipment, cassettes, bins, etc., to see which configurations would work best in terms of throughput, reliability, and scrap rate. Then the consultants worked to optimize transportation policies, buffer zone allocation, and watermark control processes. Some of the proposed layout variants were rejected during this stage due to their poor performance.  Finally, they tested the resulting few manufacturing optimization solutions manually to investigate the benefits and drawbacks of each one, and to find out how they could be further improved. Outcome The proposed solutions provided Centrotherm Photovoltaics AG with the opportunity to significantly improve the production line design and choose the best solution in terms of manufacturing optimization: throughput, reliability, and scrap rate at a low cost.  Additionally, at the end of the project, the model was released to the client so that it could be used for analysis of future change in the factory. The manufacturing optimization model allows the customer to carry out their own experiments, playing with parameters and layout, changing input data, etc. This manufacturing facility simulation model will serve as a decision-support tool at the factory for a long period of time.       Identifying Inefficient Oil Wells through Oil Extraction Process Simulation Link:  Tags: Oil & Gas  Problem            One of the largest oil and gas companies faced financial inefficiency from depleting deposits: approximately 20% of the deposits yielded little if any profit. In order to maintain a strong performance in a context of high uncertainty, the company had to make operational decisions about whether they should shut down or retain their marginally profitable wells, and whether it made sense to repair breakages.            In order to meet these challenges, the company decided to create a digital twin of the deposits. The twin was supposed to help the management in decision making: to assist in simulating deposit operations based on operational data from the wells to further analyze economic indicators and highlight ineffective wells. Consultants from Focus Group Company joined the project team in the development of the core of the system – the agent-based oil extraction simulation model.   Solution            The engineers opted for AnyLogic oil and gas process simulation software as a platform for building the model. They took advantage of the AnyLogic GIS mapping feature and reflected geographical locations of well clusters and their specific performance features in the model. The engineers linked about 400 wells to their current locations and placed them on a model map. All the wells in the model were connected by the same infrastructure as in real life: pipeline network, water pipes, roads, and power lines. Once the model was launched, the well agents began to produce oil. The datasets could be uploaded into the model via Excel tables.             The following parameters could be set up:    Technical operating mode of the wells Prediction of their water cut Casing-head gas volume at a well level The cost of oil and gas for calculating financial indicators           High model detalization allowed for better well optimization and revenue and costs assessment for every well, for example, the costs of raw material transportation, layer pressure retention costs, electricity costs, staff costs, and maintenance costs.            When the model was ready, the developers simulated a year of a deposit’s operation.    Result            As a result, the refinery simulation model identified the wells that were economically inefficient and the wells where maintenance and renovation were not profitable.            Moreover, the model allowed for the real-time assessment of how a failure of one well could affect economic performance of the neighboring wells. At the same time, the refinery simulation model considered redistribution of total costs and the reduction in energy costs for raising oil in neighboring wells due to changes in pressure in the pipeline.            The engineers exported the model as a standalone application in order to communicate it to the customer. You could run the simplified version of the refinery optimization model online.   Run the simplified version of the refinery optimization model uploaded by Focus Group Company Next steps            At the next stage of the project, the model will be linked to the sources of operational data about the deposit. This will turn it into a fully functional refinery optimization digital twin and will allow for simulating scenarios based on real-time data. The company is planning to use it for the following purposes:    Economic performance assessment for each well over an annual time horizon Assessment of the wells' major overhaul economic effect Assessment of how shutting down or retaining a well will influence technical and economic indicators of the other ones           The digital twin implementation will allow for reduction of the deposit's operating cost by one million USD per year. Once the tool is implemented in one deposit, it can then be readjusted for further use at other deposits.      Improving Customer Satisfaction by Checkout Process Optimization of a Retailer’s Stores Link:  Tags: Business Processes  Overview  LTP is an analytical-driven management consultancy company combining advanced analytics with business expertise. They have proficiency in supply chain network and facility design, inventory and replenishment, resource allocation, etc. LTP carried out the checkout areas planning project in stores for Sonae MC.  Sonae MC is the leading food retailer in Portugal with more than 1,300 stores, 36 thousand employees, and a 5,4 million-euro turnover in 2021. These stores are divided into 3 groups that LTP represented in their model. It included malls, supermarkets, and convenience stores in urban areas.   Problem  Service in checkout areas is very important as crowds of customers accumulate at the exits to pay for their goods. Therefore, retailers should take care of customer satisfaction at the last stage of shopping.      Sonae MC wanted to improve service quality and customer satisfaction without increasing operating costs. In addition, they aimed to understand how to leverage the wide variety of checkout solutions available in the market.    The goal was to determine the ideal number of checkouts of each typology to be available at each store, in order to fulfil the demand and expected service level. Another aim was to identify and prioritize the stores in greatest need of reconfiguration of the checkout areas.    The trade-off between costs and customer satisfaction increased the problem’s complexity. The cost of each checkout could be divided into fixed costs such as equipment, software and maintenance, as well as operational costs (cashiers). Customer satisfaction could be evaluated by the queue sizes and the average checkout time.    Costs and satisfaction trade-off (click to enlarge)    So, LTP needed to simulate several scenarios to find the ideal position in terms of the trade-off between retailer’s costs and customer satisfaction. Solution  For this purpose, the consulting company built a multimethod simulation model in AnyLogic which included  agent-based  and discrete-event approaches.    In AnyLogic, it was possible to simulate any configuration of the checkout area and specify the characteristics for different scenarios. It was also possible to use different Java functions in order to simulate real-world process features as accurately as possible.   The checkout modeling required understanding the various stages of the process. The flowchart below shows the stages of customer service at checkout.    Benefits of AnyLogic for checkout area planning (click to enlarge)    AnyLogic simulation provided both detailed (customer’s time in a checkout area) and holistic views (performance in a year). The model allowed LTP to test, in a virtual environment, checkout configurations that currently do not exist. Also, it was possible to foresee KPIs.    Solution approach (click to enlarge)    In addition, AnyLogic software gave LTP consultants great flexibility for the solution’s development.   Results  With the help of the AnyLogic model, LTP was able to get key results as well as retrieve some valuable insights for the retail business. The insights were, for example, stores’ stress test, scalability and personalization, scenario exploration, etc.    Retrieved insights (click to enlarge)    Testing of alternative scenarios revealed opportunities to decrease costs and improve customer service in checkout areas. The company switched some customers to self-service checkouts, and due to this, queues were reduced. As a result, fixed costs were reduced by nearly 15% and operating costs were decreased by almost 12%. In addition, the quality of service was improved.   Fixed costs    Operation costs    The developed model provided the retailer with analytical support regarding the checkout dimensioning process. The simulation-based methodology

 

Section 22 of 60
ensured better support for decision-making thanks to the ability to test multiple scenarios before the real transformations.    Using AnyLogic, it was easy to identify improvement possibilities in the checkout process. Additionally, LTP was able to evaluate new and innovative checkout concepts for the retailer.     LTP plans to conduct a more profound study regarding customer preferences for each checkout typology, incorporating new market tendencies and shifts.    The case study was presented by Diogo Miranda, of LTP, at the AnyLogic Conference 2022.    The slides are available as a PDF.     Improving Mining Outbound Logistics with Agent-Based Simulation Modeling Link:  Tags: Supply Chains, Mining   Problem   One of the largest resource companies in the world, with over $80 billion in sales, decided to enter a new market. It was planning to build a new potash mine and export 90% of production. They wanted to design a reliable supply chain, with high speed replenishment, and the ability to recover, or even benefit, from disasters, both natural and man-made. Amalgama and Goldratt were contracted to design the potash mining operations and a full supply chain for outbound logistics.           Before initiating the project, it was important to understand the bottlenecks resulting from the current simulation system, built earlier by another company. This old system did have some benefits; however, the model behaved like a black box and produced results without reasoning, they could not be queried. The new project, with simulation modeling, was to visualize the supply chain processes and bring confidence to results, helping:     Design a supply chain with a high service level, at a low cost, and with low capital investment.   Choose the optimum stock management policy — Push, Hybrid, or Pull.   Find storage capacity at mines, ports, and hubs.   Determine the number of rail cars needed.            The wrong decisions could lead to hundreds of millions of dollars of profit loss over a 20-year period.   Solution           The model was required to:    Have easily adjustable nodes and links, with configurable performance parameters.   Include randomness, variability in demand and supply, and disruptions.   Capture interdependencies and performance variations with dynamic animation.   Present financial and operational performance metrics.   Perform single run experiments, scenario comparisons, and sensitivity analysis.           AnyLogic simulation software fulfilled these requirements. Allowing engineers to create a model of the supply chain, flexible and configurable as needed. AnyLogic modeling clarified the processes inside locations (ports, hubs, etc.), and showed how different elements work and interact.            The mining logistics process starts at the plant and mine storage facilities. After the products are mined and ready to be transferred, a decision is made whether to ship the product to an export channel or keep it for the domestic market. The products got to either a hub or port by train and are then shipped abroad or sent for local distribution.           In the agent-based model, sea ports and mines, as well as trucks, trains, and vessels, acted as stand-alone agents, interacting with each other. The model also includes different sources of randomness; for example, strike action, weather delays, production disruption, customer demand variability, etc. The graphs in the model show output statistics for the supply chain and its components.           Using the model, sensitivity analysis was performed to define the best policy for the supply chain – Push, Hybrid, or Pull. The analysis considered adding rail cars into the system (from 2.5 thousand up to 5.5 thousand rail cars), changing the amount of storage capacity at the mine and ports (from 150 thousand up to 500 thousand tons), and altering the service level. The world-class service level was predefined as 98%, green, and lower service levels were marked as red and yellow.            The graph shows that the Push scenario does not give any high-grade results. The hybrid scenario provides the required level of performance; however, it is better provided with the Pull policy, using 3,500 rail cars of 300-kiloton capacity or 4,500 rail cars with a 250-kiloton capacity. The system turned out to be very sensitive in terms of storage capacity.            After defining the optimal policy, complexity and volatility factors were added to the model to see the effects on service level. The Push policy was negatively impacted by adding new products, customers, hubs, or ports, whereas with the Pull strategy, high service levels were maintained regardless of any factors.   Each policy was then tested to see how the cost per ton changes when variability increases. Push almost always had the highest cost per ton index. However, the graph shows that as volatility and complexity rise, the cost per ton also increases, over time, for Pull.            Finally, the results were compared against each other using different parameters (service level, working capital, stock in hub and port, etc.), and the policies ranked.   Outcome            AnyLogic simulation modeling visually represented the supply chain processes and proved the Pull policy as optimal. This policy provided a higher level of service at a lowest cost per ton, with lower working capital and investment requirements at the same time. It also showed how the additional storage capacity would help. Other major benefits with the Pull policy are:     It can maintain a world class service level.   It is more resilient to changes in market demand and product mix.  It offers auto-prioritization stock levels are low.  It maintains a lower level of stock at port, which prevents trains from queuing.           The Push policy, applied by the company before, provided a poor level of service because it did not consider demand variability. The company used a multiproduct supply chain, and when customers started demanding a product, it could be missing due to the lack of free storage space. The Pull policy algorithm acts differently. It decides when to safely reduce stock or increase it, depending on demand, without incurring a penalty.            The model capabilities include:     Sensitivity analysis — showing how sensitive supply chain performance metrics are to the number of rail cars, and storage capacity in a port.   Scenario comparison — testing different stock management policies, as well as the financial and operational results.           The latter provided detailed results for various model parameters. For instance, the difference of Delta cost per ton/sold for Push and Pull policies was three dollars a ton. At 13 million tons per annum, this would mean $39 million of net profit loss if the wrong policy was chosen. For the Tons Sold parameter, there was a 4.1 million-ton difference between Push and Pull policy results, when using the same capacities and volatility. Multiplied by $300 per ton, this would translate to 1.2 billion dollars of revenue loss from the wrong choice of policy.            When the analysis was presented at the executive level, the pull strategy was chosen for business development.    Project presentation by Dr. Alan Barnard and Dr. Andrey Malykhanov       Improving Patient Flow at an Outpatient Clinic Using Discrete Event Simulation Modeling Link:  Tags: Healthcare, Business Processes  Problem   Indiana University Health Arnett Hospital, a full-service acute care hospital and a multispecialty outpatient clinic, faced poor statistics because the number of no-show patients (those who don’t show up for their scheduled appointments) rose dramatically to 30%. This was primarily because clinic schedules were driven by the individual preferences of the medical staff, which led to increased variations in scheduling rules.   To eliminate the problem, the client wanted to develop a scheduling methodology that would benefit this outpatient clinic, doctors, and patients. Contractors from Texas A&M University were asked to make a predictive scheduling system to optimize doctors’ schedules and decrease the number of no-shows. They also aimed to:   Increase physician efficiency.                  Increase facility utilization.                  Keep down physician overtime.                   Decrease waiting time for patients.   Solution To address the challenge in appointment scheduling, the contractors developed a discrete-event simulation model using AnyLogic software. The model simulated the patients’ appointment process and further checkup. To better represent patient flow in the model, they were attributed to one of five groups:   Patients requesting same-day appointments New patients of high priority Re-check of high priority New patients of low priority Re-check of low priority.  High priority patients had insurance, as opposed to those of low priority.  The interface showed how patients mix, depending on treatment time for patient types (it is assumed that new patients have a longer appointment time than re-check patients) and seasonal factors. The model’s input screen was used to insert the following parameters:  Number of appointment requests per hour the outpatient clinic can have at each day stage.                  No-show rate.                  Doctors’ working schedule including time of availability and number of patients they were able to help per day. It was also possible to limit the number of sick and new patients a doctor could see each day.  Sick patients’ field showing the share of patients diverted to a different doctor or nurse.    The user could change these capacity parameters to see what changes would help optimize working time for physicians and waiting time for patients. The discrete-event model showed the following sequence of operations:   The appearing patients are divided into the five groups. Same-day sick patients are treated in the same day, while others schedule their time of visit and wait at home.                  When they need to arrive at the clinic on the day of the appointment, the model calculates the no-show rate based on probability specified by the input data.                  If the patients come, they are seen by doctors or nurses, and after that they leave the hospital.   The output screen showed the patient flow simulation model results and performance measures for a simulation run. Data included:  Number of treated patients for patient visit type.                  Number of no-show patients for patient visit type.                  Appointment lead time for patient visit type.                  Proportions of discharged patients per doctor, nurse, or peer/urgent care.  Maximum daily clinic capacity.   The model also helped doctors test different theories about their working schedules. They could adjust the schedule in the patient flow simulation model and see how utilization and overtime changed.   Why Create Patient Flow Simulation Model in AnyLogic? The developers chose AnyLogic for several reasons. First, the AnyLogic software allowed them to easily capture discrete-event metrics, such as utilization rates, time patients are in the outpatient clinic, and wait time.    With AnyLogic, it would be possible to expand the primarily discrete-event model using agent-based and system-dynamic approaches. In addition, AnyLogic’s capabilities for creating user-friendly and engaging interfaces made it easy for other users to experiment with the patient flow simulation model and change the input parameters without additional training. Result                   The AnyLogic patient flow simulation model offered various ways to improve the outpatient clinic’s operational efficiency and patient satisfaction. The patient flow simulation model did not require special skills to use and provided detailed output statistics that included:    Staff time utilization and overtime amount.   Patient distribution among medical staff.   Patient waiting time, and more.   The obtained data allowed users to see how the schedule affected the clinic’s work processes and provided insight for improving patient flow at the outpatient clinic and choosing better staff management policies.           AnyLogic presented a method to test theories before implementing them in the clinic and gave different forecasts. In addition, the discrete-event model could be expanded with other simulation approaches if needed. This feature made the model more adjustable to design a predictive appointment scheduling system in other outpatient clinics with similar settings.       Improving Plane Maintenance Process with AnyLogic Simulation Software Link:  Tags: Defense  Problem            We all take commercial air flights from time to time. However, less well known is how complex plane maintenance can be. The military aircraft turnaround process (the time between an aircraft touching down and being ready to fly again) is even more complex and includes multiple interactions and parallel workflows. In addition, skilled staff are needed to maintain a sustainable level of aircraft turnaround, which leads to associated costs.            Engineers from Lockheed Martin, one of the largest companies in the aerospace, defense, security, and technologies industry, used AnyLogic simulation software to improve decision making for the whole military airplane turnaround process and to evaluate the impact of process changes on aircraft turnaround time.   Solution            For a complete model, the three main elements of the turnaround time process had to be considered:    Aircraft inspections;                          The signoff, meaning all the inspections and refueling have been completed;                          The review and storage of any maintenance codes that were downloaded from the aircraft.           Once these processes were clarified, a mobile application was designed to enable the recording, validation, and understanding of each process at every stage of maintenance. This data collection tool was used by observers who monitored the maintenance staff. The application was modified several times over the course of the project to refine its operation.                            For each step in the workflow the actors, resources, dependencies, and other process definition data were identified. The data needed for the model included the start and stop times of each task. In addition, it was important to provide an audio recording capability to capture activities that were not otherwise provided for in the application. For example, observers might record the reason that a task was taking longer than expected, or record that they had accidentally pushed the wrong start button. It made the data collection application highly flexible and adaptable.                           Studying the aircraft turnaround process revealed that the modeling and simulation environment should include experimentation and presentation

 

Section 23 of 60
capabilities. AnyLogic simulation software fulfilled these requirements and had already successfully fulfilled aerospace projects. Additionally, the process visualization possibilities allowed the model details to be presented to all levels of developers and senior management. These visualization abilities are highly valued in the aerospace and defense industry.                            Following the process capture stage, the agents, resources, and tasks were modeled and in a process flow using AnyLogic simulation software, including the creation of multiple visualizations.Baseline models of the  current processes were then built. These were iteratively run in a deterministic mode for debugging purposes, and also as single and multi-run Monte Carlo modes. The outcomes were compared to what had been experienced previously on site.                            After validating and updating, a stochastic agent-based model was able to capture the dynamic and interacting processes that comprised the turnaround process. To make the process more efficient, experiments were performed to quantify the impact of process changes, whether through the deletion of process steps, a reduction in the amount of time needed to execute a process step, or the redefinition of process sections.                            The experiments helped:   Record the characteristics of the current workflows. Explore workflow alternatives. Forecast the impact of the alternatives.  Outcome   Various experiments with the model, including with the Monte Carlo method, resulted in suggestions that showed which process modifications would make the most difference. AnyLogic helped model the people/machine/workstation interactions, and also revealed workflow peculiarities that had been unknown before the experiment, including:            People not following a linear work path. Explaining why actions in the workflow were not synchronized and acting in parallel at times. Interdependencies between tasks that were not obvious when looking at individual parts of the process. A holistic view of the turnaround process that showed where time was being lost.   This new information and the testing abilities, enabled with simulation, let engineers identify and understand the bottlenecks in the turnaround process. The resulting proposals and modifications to the workflow,  brought sizable improvements to the airplane turnaround process, proving the project, and simulation modeling, a success.             Download full case study (PDF)   Project presentation by Nadya Belov, Senior Researcher at Lockheed Martin    Improving Reliability and Profitability of Integrated Steel Supply Chain with Simulation Link:  Tags: Supply Chains, Manufacturing  Problem Today, many steel manufacturers are in need of lean manufacturing tools that will improve their return on investment and service levels. The minimum 80% reliability level most steel companies are struggling to achieve is nowhere near what today’s customers and investors want to deal with.  One of the largest and oldest European steel manufacturers came across these problems and was desperately trying to solve them. All of their initiatives ended up in endless debottlenecking, rather than building a stable system. The company called upon the assistance of Goldratt Research Labs. Together, they decided to transform and optimize the company’s supply chain because it had a high degree of fragility to external changes, low profitability and ROI. At the time, the company’s management couldn’t make any effective changes because the supply chain was very complex. It was very difficult and risky to decide on any new rules, as the outcomes were hardly predictable. The static decision support tools like ERP (enterprise resource planning software) or Excel spreadsheets were not able to help because they were never designed to support decision-making within such a complex environment.  To fully consider all the interdependencies, constraints, dynamics, and variability in the system it was decided to employ simulation modeling. With manufacturing process simulation, engineers could capture all the complex details of the manufacturing supply chain, determine causes of performance gaps, and test possible solutions in a safe digital environment. Simulation modeling was supposed to help management make faster, better decisions and predictions, so that they could turn these predictions into reliable manufacturing optimization commitments to investors and customers. Solution Several models were built, so that together, they could represent the whole integrated steel production supply chain. AnyLogic manufacturing supply chain simulation software provides an opportunity to use different modeling methods in one model. The developers took advantage of this multimethod modeling capability and integrated agent-based, discrete event, and system dynamics approaches in the models. This made it possible to reflect all the components, processes, and interdependencies. All the models had the functionality of showing the outcomes for single scenarios, sensitivity analysis for various parameters, and direct scenario comparison. The data for the models (more than 70 Excel worksheets) was collected from ERP and EMS. AnyLogic ability to create self-configurable models from external data helped cut development time significantly. The manufacturing process simulation models has detailed 2D and 3D animation. The AnyLogic platform provides easy-to-use interfaces and the ability to export models as a standalone application, and run the models on any computer without special software. Mobile work and collaboration, being crucial for fast and efficient decision making today, were facilitated by AnyLogic Cloud. This web service enables users to run demanding models online in a web browser on any device, including mobile phones and tablets, share models, discuss scenarios, and provide simulation analytics to customers. Hot Coil Finishing Simulation Model  The hot coil finishing area had a great number of congestions and there was a permanent coil movement in order to dig out the coil needed. Even the car parking was filled up with coils, which indicated the system was working ineffectively. New decisions to avoid congestion and improve flow were necessary. The designed model simulated the location of all the coils in the system. For each coil, the type, the destination, and the storage location were specified.  The model helped identify efficient options for congestion reduction based on throughput, in-process inventory, and cost impact. Operational changes, including increasing hot strip mill output and coil width doubling, were tested for better production planning and control. The model also helped find the best way to upgrade and optimize wagon fleet. In the future, the model may be used for testing new operational rules that allow for full automation of the hot coil management process. Steel and Slab Simulation Model  The steel operations and their scheduling are both very complex. It is very difficult to predict the impact of changes in product mixes or operational rules. To test them and improve efficiency, a specific model was needed. This model enabled the developers to grasp all the elements of the manufacturing system. Within the detailed 2D animation, a user can click on any crane or product to see its status and on-going operations. The heart of the model is the logic which supports the production planning and scheduling of all operations. Required product mix and other parameters can be set and altered, and manual scheduling is available if needed.  The steel and slab production simulation model made it possible to:  Capture and digitize the scheduling and operating rules that different plant operators used. These rules had never been verbalized before and could eventually be lost. Now the personnel can learn from shared experience. Develop scheduling and operating rules to avoid bottlenecks. For this, different speed restrictions and alternative prioritization rules for different products were tested and analyzed.  Forecast the operational and financial impact of producing more complex and higher margin products, which could negatively affect workflows.  Steel Supply Chain Model  As mentioned above, the company was experiencing great congestion on the factory floor and low reliability. The company had lots of inventory and couldn’t manage it properly. It was necessary to determine the main causes of such situations, identify new global optima rules to be implemented, and quantify their operational and financial benefits. Goldratt Research was tasked to provide a production optimization and decision support tool which managers could use when developing production planning and scheduling. The model represents every part of the supply chain in detail. The user can click on a link in the supply chain and see the processes inside. Model statistics presents information on stock levels, processing units, financials, etc. As a result, it was discovered that problems were primarily caused by the management always choosing the lowest cost-per-ton option for manufacturing and distribution. The other problem was permanent balancing of capacities. The model may be helpful in:  The development of new “Low WIP/Max Flow” rules. Forecasting cost-per-ton, if the company shifted to a more demand-driven approach. Making annual commitments, regarding the level of performance, using the full financials the model provided.  Outcome Using the models, Goldratt Research and its client managed to ascertain the causes of the congestions and low reliability the company was experiencing over the last years. When the time came, the models provided a safe, low cost, and very quick way to test the impact of any changes on both operational and financial performance. The models were also used to validate each other’s results. In the future, the models can be used on a weekly, monthly, or annual base to analyze workflows and make critical decisions and reliable commitments.  Project presentation by Dr. Alan Barnard and Jaco-Ben Vosloo from Goldratt Research Labs     Improving Transportation for Long-distance Emergency Patient Transfers Link:  Tags: Transportation, Healthcare    Lean Business Services is a government-owned company in Saudi Arabia and the leader in serving and developing innovations for the health sector. The company focuses on digitizing the Saudi health ecosystem and improving healthcare resource utilization.   Problem   Despite extensive emergency coverage, some rural areas still do not have access to all the necessary resources for good public health. In some cases, patients require ambulance transfer to bigger cities where these resources are available. Ambulance transfer, however, has its problems:    Lack of medical staff Long transportation times Difficulties for people with serious health conditions  The simulation of medical transfers using AnyLogic as a transportation logistics software was aimed at helping improve patient transportation by tackling ambulance transfer problems. Also, as healthcare analytics software, AnyLogic defined the impact of each major variable and evaluated different scenarios and possibilities, so that decision-makers could develop KPI for each case.   Solution   The proposed solution for patient transportation was to supplement or replace road ambulances with aircraft. Three transfer scenarios were considered when building the model:  Transfer scenarios for the model (click to enlarge)   Air transfer between airports only Air transfer from hospital to receiving airport Air transfer from hospital to hospital   Modeling these scenarios would help study the impact of each scenario compared to the actual ‘as-is’ state of road-only transport.   Lean Business Services developed a hybrid model that employed both agent-based and discrete event modeling. This multimethod approach helped include more information about the scenarios and transportation types than using a single method alone.   Each emergency call is represented as an agent in the model. These agents inherit characteristics from a database containing information about emergency cases, such as patient transport date, sending hospital, receiving hospital, hospital location, etc.    Simulation of medical transports diagram (click to enlarge)  Patients are also represented as agents and have parameters relating to their location, date, and time. The journeys between locations, however, are modeled as discrete event processes because they are best described as a sequence of separate events.   How the agents behave during a simulation run provides insights into the effectiveness of different scenarios. When the model is run and an emergency call is received, results for each transfer scenario are calculated for the KPI and to provide insights. In this way, it is possible to discover the effectiveness of each transfer scenario when faced with many different patient situations.   Simulation framework (click to enlarge)  During an experiment, the model considers various dynamic parameters for better healthcare resource utilization:    Number of doctors, nurses, ambulance drivers, or other health workers Cost of patient transportation Night-flight bans Short distance air transfer permissions  The simulation model developed by Lean Business Services also contains a GIS map that shows the patient transfer process.    GIS map showing patient transfer process (click to enlarge)  Overall, the company was able to analyze many strategic indicators:    Average transport duration Average number of unavailable medical staff Total cost of transportation Number of aircraft by type Impact of transferring patients (crowding and waiting time) Aircraft utilization percentage Number of unavailable medical staff per specialty  Result   Based on the outcome of the simulations, the transfer of patients using aircraft for the direct hospital-to-hospital scenario was recommended for three main reasons:    Low implementation cost Lowest average trip time Lowest average for the number of unavailable medical staff  Combining discrete event and agent-based simulation paradigms allowed the simulation engineers to make an easily scalable model. It could accommodate the addition of alternative scenarios from transportation logistics software as they were developed, such as changing patient transportation modes, and the addition of new resources at locations.   The case study was presented by Ahmed Alhomaid, of Lean Business Services, at the AnyLogic Conference 2021.   The slides are available as a PDF >>      Improving the Planning for Surgery in the Real World through Simulation Link:  Tags: Healthcare  Problem Using simulation to improve operating room efficiency is not a new concept. This idea has existed for many decades. Recently, discrete-event simulation has been applied to this, but it was not able to

 

Section 24 of 60
reflect real life appropriately. Many assumptions, often with a lack of data, have to be made in surgery and an attempt to reflect the real world was made in a simulation. However, those simulations were not reliable and were unable to achieve their goals.  Solution A proposition was to reverse this previous logic and so the real world would now follow the simulation. A customer could experiment with all parameters that impact the situation and environment which they are operating in. They could then set those variables as constraints in the system. They could then experiment with those constraints and set the parameters the real world should implement.  In this study the focus was on planning for surgery, and all the factors which impact what happens in the operating room. This is where simulation could be used to experiment with the different variables involved.    The elective surgery planning process conceived by TCC-CASEMIX (click to enlarge)    TCC-CASEMIX was able to measure the impact of the inputs to the process, which are the patients and the various aspects of surgery. This included analyzing the dangers involved in order to assign risk levels to the patients relative to each surgery. These two joined together had never been done before, but by using a machine learning algorithm they could create these risk groups.   TCC-CASEMIX, the consultants who built the model, and the surgical services teams who are the customers, had different needs and requirements.   TCC-CASEMIX had very technical needs and AnyLogic delivered a powerful simulation service in a highly configurable environment reflecting the real world. AnyLogic also provided a  multimethod simulation combining  agent-based with discrete-event simulation, which could integrate with TCC-CASEMIX’s database.  Part of the simulation involved using TCC-CASEMIX's custom list optimization engine, compiled in Java and imported directly into AnyLogic. Thanks to this, the developers had complete control over complex behavior in the simulation and, in addition, could change the logic and test alternative optimization engines.  The customers needed the simulation to be operational with an intuitive user interface. Also, the ability to control and examine the simulation outputs without an expert to aid them was very important. Additionally, they wanted to be able to use  AnyLogic Cloud to run the simulation directly in their web browser.   Examples of user interfaces for various inputs and outputs in the model (click to enlarge) Results Two demonstrations illustrated the power of this tool.  In the first one, the developers needed to evaluate the impact of productivity on planned absences over the summer in an   acute care trust. The developers assessed how many additional surgeries could have been performed in a 4-week period if resources were used to full capacity.  In the baseline, where only 5 surgeons were available for two 5-hour lists per day, with a patient target list capped, 82 surgeries were completed. To test the full capacity, there was an uncapped patient target list and three 4-hour lists, which led to 204 completed surgeries. In the simulation, each operating room was used more productively with all surgeons available.  In the second demonstration, developers needed to understand how to best deal with a short-term surgery cancelation. Using TCC-CASEMIX’s optimization logic developed in AnyLogic, the optimal patient to replace the canceled one was found using the database output at a lower level. The excel file was given to the surgery clients to use and after running the simulation, it was discovered that 3 patients could be fitted in for surgery instead of the one who canceled. This included two additional short procedures eliminating wasted time.  As a result of using this model, surgical service managers feel empowered to make better decisions which are reflected in their KPIs and the level of predictability that can be acquired. They also feel more confident in the services that simulation can deliver, which can be passed on to others, such as clinicians, patients, and society.   The case study was presented by Dr. Matthew Bacon and Jack Morewood, TCC-CASEMIX Limited, at the AnyLogic 2021 Conference.  The slides are available as a PDF.     Increasing Throughput of Rail Container Terminal with Simulation Link:  Tags: Rail Logistics, Ports & Terminals  Problem   RUSCON is one of the major container shipping companies in the CIS region. The company needed a container yard planning solution to modernize one of the rail container terminals and determine maximum capacity of the whole facility and its components. It sought the expertise of container yard simulation specialists from Dilibrium Consulting Company to model terminal components including the container yard, transshipment equipment, and rail infrastructure.   The following characteristics of the container terminal were identified for more granular container terminal simulation:    The railroads length amounts to 3,000 m.   The facility has its own shunting locomotive fleet.   Temporary storage area is more than 22,000 m2.     There is an open storage yard (2,500 TEU) for containers allowing to stow 20 ft. and 40 ft. containers in the customs inspection zone and temporary storage area.   The terminal provides for handling (receipt\departure, loading\unloading) of container flatcars, universal flatcars, covered rail cars, gondolas, etc.     Two frame cranes, a number of fork lifters, an in-house tractor trucks fleet, and other ancillary equipment are available in the terminal.      The consultants decided to simulate several different scenarios of terminal development to find the best alternative. Solution   It was decided to build a digital twin of the terminal in the AnyLogic container yard planning environment. This dynamic simulation model, unlike traditional analytical ones, would reflect the facility with high accuracy – the variance between real data and virtual operations results was estimated to be within 5%. Digital twin technology provides detailed modeling and allows for considering various parameters, as well as non-linear, nontrivial, and unknown dependencies and cause-effect relationships.   The starting point of the container yard simulation is the arrival of trains with loaded containers to the railway sorting yard. Then, the rail cars are switched to consumers’ railway tracks following the train schedule. The moment when trains with empty containers leave the terminal is the endpoint of the simulation. Within the framework of the project all major terminal operations were modeled including:    Internal and adjoining rail logistics   Truckage of loaded and empty containers   Storage logistics   Processing at customs checkpoints   Container yard planning model  The capabilities of the AnyLogic container terminal simulation environment enabled engineers to create 2D and 3D model animation. The animation allowed zooming in on certain model areas and watching the operation of its components. This approach helped visually control processes in each facility area.   3D animation of the container terminal simulation was useful for visual control over processes. For containers, color assignation was implemented to show their current state and changed dynamically:    Red containers were not cleared through customs yet.   Green containers were customs-cleared and ready for unloading.   Empty containers were colored yellow.    The engineers introduced in the container yard planning model “virtual sensors” which were gathering information from the digital twin and transferring operational data while the model was running. This facilitated tracking simulated facility performance and model calibration. Such a thorough approach in data gathering allowed for close matching of the digital twin with the operations of the real-life container yard.   Rail yard simulation optimization model  After the container terminal simulation model was built and calibrated, the engineers managed to determine the terminal capacity and verify the analytical model data. When comparing the performance indicators of the terminal models with the "as is" scenario, the accuracy of the analytical model was 72%, whereas the simulation model showed 96% accuracy.   The model also facilitated in testing the operations of the frame crane, the terminal’s loading machine, in a virtual risk-free environment. The model reflected three-axis movements of the crane’s trolley and spreader. Moreover, the different time duration of each crane operation was considered. As a result, crane operation bottlenecks were detected, and optimization measures were suggested and supported by experimental data.    In AnyLogic, industry-specific Material Handling Library simplifies the simulation of complex material handling operations at terminals. It can be used to design detailed models of storage facilities, including operations with conveyors, AGVs and cranes, and manage material workflows inside four walls.   The model developers also analyzed how internal logistics operations influence the general performance of the container terminal simulation. They set up the experiments and looked at how KPIs were changing. This work resulted in simulation-based recommendations on how to upgrade infrastructure and reorganize terminal business processes. Outcome   After the experiments were held and the output data was obtained, the engineers were able to define facility bottlenecks and possible optimization scenarios. They discovered that further optimization measures could enhance terminal capacity by 57%.    Infectious Disease Modeling in Saudi Arabia Link:  Tags: Healthcare  Problem  At the start of the COVID-19 pandemic, many countries made different decisions regarding policies towards this new coronavirus. Some of these decisions were risky, but all of them impacted their countries in many different sectors, including the political and economic. Quick responses were vital to control the spread of the disease.   Lean Business Services is a leading company in assisting and developing the health sector in Saudi Arabia. During COVID-19, simulation was needed to help understand different policies the government introduced and the impact of those policies on controlling the spread of COVID-19.  This simulation could help in capacity planning for health resources, which at the height of the pandemic were limited. Lean, therefore, built a simulation model using AnyLogic software to understand the spread of the disease and to identify how different policies could change the spread of the virus over time.  Solution  A SEIR(D) model is a mathematical model of the spread of an infectious disease. In a population, every individual exists in one of five states:    Susceptible (S) to the disease.   Exposed (E) to the disease.   Infected (I) by the disease.   Recovered (R) from the disease.   Died (D) from the disease.   There is also immunization to consider. This is further split into multiple models as immunity fades away and if immunization is not as effective for all people.   SEIR(D) system dynamics model – simplified base model   Lean built this core SEIR(D) model using system dynamics. Then they built many other models on top of it. The models they produced were multimethod models between agent-based and system dynamics. The agent-based part of the model simulated the behavior on the individual level in depth, while the system dynamics part handled large amounts of information on the aggregate level. Combining these two increased the model flexibility to simulate various scenarios without delay.   Multimethod simulation model showing agent-based modeling and system dynamics   A multimethod model has 82 unique parameters and 20 health directorates or divisions. For each directorate there is a separate simulation model. Therefore, in total there are   1640 parameters, which are divided into 4 main categories:    Healthcare resources.   Disease behavior specifically related to COVID-19.   Population behavior.   Demographics.   From these unique parameters, there are 95 distinctive pathways again multiplied by the 20 directorates creating 1900 dynamic events. Pathways in this case are events, and these can be based on if statements, for example, the number of ventilators available. If there are no more available ventilators, then anyone who needs one would not have access and this could lead to death and an increase in the death rate.  Events and assumptions in the model included international travel, work, Eid – which is a national holiday in Saudi Arabia, the program scenarios, vaccination, and school. Additionally, capacity planning was built into the model and included such resources as nurses, ICU beds, doctors, and so on.  The model interface is very user-friendly, giving users the possibility to change the parameters, policies, and importantly, even the population. As a result, the model can be applied to other countries.  There were also just agent-based models designed for specific simulations such as in an airplane. Here, there is clear and specific behavior for each agent, or passenger. On an airplane, for example, two people can sit next to each other and if one is infectious, then the other could be susceptible. As a result, the chance of spreading the disease is higher than if those people were not sitting next to each other.    Simulation, using the SEIR(D) model, showing agents traveling in an airplane   Notably, Lean built a separate agent-based model for the Hajj, the annual Islamic pilgrimage to Mecca, Saudi Arabia. Different policies or precautions were taken to understand the impact of each, for example, taking 2 swabs instead of 1 swab before the Hajj, or if there would be isolation for people at home or at Makkah.   Hajj integrated with the model. Assumptions for the Hajj scene    Results After running the models, Lean could provide detailed reports, including the forecasted numbers of cases for the next week or next month, the predicted outcome of schools returning to face-to-face learning, the mortality rate due to COVID-19, the impact of the Hajj, and so on.    Example report of the impact of an isolation policy on ICU and mortalities after a month    The case study was presented by Ahmed Alhomaid, of Lean Business Services, at the AnyLogic Conference 2021.  The slides are available as a PDF.     Internal Rail Logistics Simulation for the Port of Le Havre Link:  Tags: Transportation, Rail Logistics, Ports & Terminals  Problem            The Port of Le Havre, the largest container port in France, needed assistance with the construction of a new multimodal terminal. The new terminal would include the area where trains and river barges bring containers for further sea transportation. In this area, cranes move the containers from the carriers and load them onto supporting rail cars that form shuttles, which then carry the

 

Section 25 of 60
containers to sea transports. The movement of these cars was the focus of the simulation model developed by The AnyLogic Company. The simulation model had to compare these two scenarios:     Basic (using simple passive cars driven by locomotives) Advanced (using autonomous cars able to move without locomotives)           The objective was to measure the costs in each case, the Quality of Service (how long containers remain in the system), and possible improvements in the network structure of the terminal.   Solution            The AnyLogic Rail Library was used to model the transportation network. Movements of rail cars, cranes, and other elements of the network were simulated in a low level of abstraction. The consultants had to create two separate models because the two scenarios contained very different logic. The models allowed the user to:     Assign arrival times and required departure times for each container for the multimodal and sea terminals (with separation into rail and river transport). Assign train and river ship arrival and departure timetables at the multimodal terminal. Change the characteristics of different equipment (speed of completing different operations) for the multimodal and sea terminals. Dynamically register the space availability for containers at the terminals. Dynamically register the costs for different elements of the network together and separately. Monitor the status of each entity and agent in the network.  Outcome            The results included statistics collected for both scenarios. Costs were calculated for different elements of the network, such as locomotives, rail cars, cranes, and dockers. The data on Quality of Service showed that autonomous cars were more efficient and cheaper than passive ones.            By using the simulation models, the customer could compare the two methods of organization of internal rail logistics, chose the optimal one, and estimate the needed amount of rail cars.            The data obtained from the AnyLogic models allowed the customer to prove the feasibility of the terminal construction project to the potential investors.       Inventory Management and Optimization for an FMCG Manufacturing Company Link:  Tags: Supply Chains, Manufacturing, Warehouse Operations   ITC Infotech is an international provider of technology services and solutions across a variety of industries, including banking, healthcare, manufacturing, FMCG, supply chains. Using simulation and optimization techniques, as well as advanced analytics, they develop solutions for such problems as network design and planning, inventory planning, scheduling, and more.        This case study details one of their projects in the FMCG industry. The aim of the project was to determine inventory norms for storing limited shelf-life products. The solution was produced using simulation-based optimization in AnyLogic.   Problem: Inventory management and optimization in FMCG  Fast-moving consumer goods (FMCG), also called consumer-packaged goods (CPG), refer to products that sell quickly at a relatively low cost. Some FMCGs, like beverages and food, do not stay on shelves for long because they are in high demand, perishable, or both. To mitigate this quick turnover challenge, companies are constantly searching for new inventory management solutions.   A global manufacturing leader was interested in optimizing its food product inventory management process with regards to the complexity of its multi-echelon distribution network.       The problem the company faced was high deterioration of product quality and a less-than-expected fill rate. It could have manufactured more to increase the fill rate, but that would lead to higher deterioration. Alternatively, it could have manufactured less, to keep deterioration low, but that would also lower the fill rate. To solve this challenge, they approached ITC Infotech to find an optimal production quantity and frequency, as well as an optimal replenishment policy.       Solution: Building a simulation model to optimize the production quantity       Suppose a consumer-packaged good expires 100 days after it is manufactured. According to the contract, a retailer must have the product in its storage at least 30 days before the product expires. This means that a manufacturer has no more than 70 days to deliver the product to the retailer. If the deadline is violated, the product must be discarded, and the manufacturer incurs a loss.       To plan production and calculate how many days a manufacturer can keep the product in its inventory before shipping it to the retailer, ITC used a shelf-life factor. This factor is a ratio of the number of days a manufacturer can keep a product in its inventory to the period for which a product should be delivered to the retailer. For example, at a factor of 0.6, the producer will have 42 days (0.6 of 70 days) to store the product before shipping to the retailer.          ITC specialists aimed at calculating this shelf-life factor for every stock-keeping unit (SKU), as well as optimal production quantity to avoid product quality deterioration. Also, their goal was to determine replenishment and production frequency while maintaining maximum fill rate. For that, they decided to build a simulation-based optimization model in AnyLogic and test different scenarios.       Brief overview of the simulation-based inventory optimization model   A multi-echelon distribution network typically consists of a factory, a warehouse, and several distribution centers.        In this inventory simulation model, the specialists considered two agent populations: a factory and a distribution center. They also defined several agent types, including shipment (shipping goods from the factory to distribution center), demand, and batches (a certain number of identical products).       A distribution center agent responds to demand and calculates a fill rate based on how much demand has been satisfied. This in turn helps determine the service level.       The factory agent generates production orders based on the production frequency. Produced batches are queued in the factory’s inventory before they are shipped to the distribution centers. During the queuing process, the model calculates product quality deterioration for the factory.         The distribution center agent receives batches from the factory agent, processes them, and then stores them in inventory. Food products in the inventory that are not sold, because there was no demand for them, deteriorate. The model considered this information when calculating product quality deterioration for the distribution centers as well.    Output data: shelf-life factor and fill rate (service level) interdependency.  Result: Benefits of the simulation-based inventory optimization model   After executing multiple simulation model runs with different shelf-life factors, ITC specialists found the optimal shelf-life factor for each replenishment policy. They also determined a production frequency that provided the maximum fill rate and minimum product quality deterioration.    According to the output data, as the shelf-life factor increased, the fill rate improved. This is natural because increasing the factor means an increase in production quantity. It ensures a better service level but also increases the risk of product quality deterioration.     Using the simulation results, ITC plotted a curve that represents the interdependency between the fill rate and product quality deterioration.     A curve representing the interdependency between the fill rate and deterioration (click to enlarge).      ITC also provided the spread of the value obtained over the 100 model runs for each of the shelf-life factors, as can be seen on the right side of the below diagram. It helped the business to decide on the optimum shelf-life factor.    Output data: the spread of the value obtained over the 100 model runs for each of the shelf-life factors (click to enlarge)  ITC Infotech presented the project at the AnyLogic Conference 2021:      Large-Scale Logistics Network Planning and Optimization Link:  Tags: Supply Chains, Warehouse Operations    Analysts at Pitney Bowes wanted a parcel logistics network analysis tool to support decision making and help improve network performance in North America.   Pitney Bowes is a technology company based in Stamford Connecticut that specializes in services related to mailing and shipping. The company serves approximately 1 million customers and helps sort and process 15 billion pieces of mail annually.   The Pitney Bowes Innovation unit is a global development organization that support the creation of secure best-in-class products based on agile design and data science.   The simulation-based logistics modeling software developed by the innovation unit's engineers has helped deliver significant savings across key metrics, such as parcel cycle time, truck utilization, and daily throughput.   Problem   Focusing on the delivery and return service of their operations in the United States of America, engineers sought to better understand four key areas of logistics network analysis:    Site consolidation (opening and closing sites) Network client addition impact Direct lane additions (connections between sites) Network design flaw identification  This work was previously the job of a logistics network design expert alone but as the delivery network had scaled, a logistics planning tool was needed to support decision making. With the help of a simulation model of the parcel network, network design experts would be able to test their initial designs, identify bottlenecks, and generate better solutions.    Solution   A simulation model of the parcel network provided a platform for testing and analysis. The model could be configured with historical data for forecasting or with test scenarios for risk analysis and planning.   Simulation model structure. (Click to enlarge)  The engineering team chose AnyLogic because of its in-built libraries, which help speed up model development, and its customizability, which allows for the accurate modeling of specific functionalities. Furthermore, thanks to the design, the model is easy to scale. For example, facilities can be added or removed using just a database entry. After a change to the number of facilities is made, the model updates and adjusts automatically.   Another factor in choosing AnyLogic was that the development engineers could deliver a standalone application for analysts and stakeholders to use.   The simulation model was useful in six different use cases:    Network expansion and consolidation analysis Facility capacity stress testing Rerouting Service guide simulation Adding new clients Network fine tuning  Parcel network simulation model use case overview. (Click to enlarge)  Results   At present, the tool is used for network design evaluation. In the case of network consolidation, an indicative example of how the tool helps improve performance comes from the analysis of the merging of three facilities into one super center.   Facility consolidation analysis results.  Working with historical data and carrying out parameter variation experiments, the team completed a robust analysis. Modeling showed that only 70 % of the initially planned capacity was required for a new super center and that the planned facility consolidation would deliver significant savings on key metrics. Most significantly, roll over at the new center would be eliminated and carry over reduced 70%.   In the future, Pitney Bowes plans to develop the logistics analysis tool into a real-time alerting and decision support tool – to deliver predictive analytics in logistics. Simulation runs every day or hour would consider current backlogs, volume forecasts, and planned resource scheduling to determine near future needs such as extra labor, trucking, and rerouting requirements.   The presentation of this logistics analysis tool case study was given at the AnyLogic Conference 2021 by Pitney Bowes Data Scientist Cora Gao.       Last-Mile Distribution Network Optimization for a Large Online Retailer  Link:  Tags: Supply Chains, Transportation  Overview        Ozon is one of the largest online retailers in Eastern Europe, with overall sales around $5 billion in 2019. The company is growing year by year by expanding delivery zones, launching new services, etc. For example, Ozon’s turnover increased by 93% in 2019 and 115% in the first quarter of 2020. Therefore, the company needs to optimize its infrastructure continuously.    Problem        In 2018, Ozon had seven distribution centers (DC’s) in Moscow and the Moscow region, with an area range from 200 to 5,000 sq. m. In 2020, the number of DC’s increased to 11. The goods were delivered directly from DC’s to the customer’s addresses, pick-up lockers, or pick-up points where the customers collected them. To maintain a high service level and deliver goods on time, it was necessary to set up a new distribution network and minimize the distance from DC’s to final destinations at the same time. The company opted for AnyLogic as simulation software with distribution network optimization capabilities to solve this problem. This technology allowed Ozon to visualize the transportation network of Moscow and the Moscow region and test hypotheses before putting ideas into action in the real world.  The simulation was also supposed to help Ozon understand how to distribute delivery zones between new and already existing DC’s, so that the centers would work effectively with no down time.   Solution        In Ozon, a placed order is sent to one of its fulfillment centers (in-house packing warehouses) where storekeepers receive goods from suppliers, assemble them in parcels, and pack them. Then the parcels are delivered to DC’s where they are distributed among couriers. Couriers, in turn, deliver parcels within their delivery areas directly to the customers, pick-up lockers, or pick-up points. Each DC operates within a specific delivery area.   Distribution center simulation model       The company decided to develop models of each order processing stage to reflect the whole process in detail. However, in this case study we focus only on the courier-client transportation stage (last-mile delivery) optimization.       The Ozon simulation team began with data collection. In the company, all information on order processing is recorded in IT systems, so the engineers could obtain the data they needed, including:     The time a courier spends in a DC. The time a courier spends on the journey from a DC to a certain delivery area. The delivery time distribution in delivery areas. The delivery points distribution within each delivery area. The distribution of the time a courier spends travelling from customer to customer in each delivery area.       Based on this data, the engineers developed the simulation model. They considered the following

 

Section 26 of 60
limitations to capture the real system more precisely:     98% of orders must be delivered on time. During peak seasons, the utilization rate of DC measures up to 95%, but the workload should be equally and proportionally distributed in the system. Within different delivery areas orders are distributed unevenly over time and days of the week. Couriers have certain work schedules.       To set the logic of the model, the engineers applied the AnyLogic Process Modeling Library. Through flow diagrams, it helped capture the system’s dynamics and interconnections between its elements.         What’s more, it was essential to reflect the distribution network delivery routes in the model. For this purpose, the team used the GIS map to locate distribution centers in Moscow and the Moscow region and their corresponding delivery areas. After that, the routes were created automatically in AnyLogic simulation experiments. The developed model was subsequently uploaded to AnyLogic Cloud, allowing the team to share the project with colleagues and access it from any device.    Last-mile distribution network model      The engineers used the simulation model to test different “what-if” scenarios in which they could vary the system’s parameters. These parameters included the number of all placed orders, on-time delivery rate, the number of couriers sent to a delivery area, and the courier’s travel time in general. The team sought to distribute delivery areas between DC’s in such a way as to minimize the number of DC’s, yet maintain a high service level. In addition, they collected the statistics for each DC, both on its efficiency and its couriers’ delivery time.     Result        As a result, the team developed a simulation model for the last-mile delivery network reflecting distribution centers and their corresponding delivery areas, pick-up lockers, and points. They used the model to test out various scenarios. Then, considering the service level and costs, the team determined the optimal location of the DC’s and their delivery areas.  The engineers used the Process Modeling Library and the AnyLogic GIS map capabilities to set up the logic of the logistics system processes and visualize them. The simulation model and the output data helped the team to conclude that to strike a balance between KPI’s it was necessary to close three DC’s and open 11 other DC’s by the end of 2020.       Load and Haul Optimization for Largest European Potash Producer Link:  Tags: Mining   Problem Managers and engineers responsible for mining operations regularly face tasks including:  Determining realistic daily and monthly production volumes.  Testing mine plan feasibility. Evaluating the outcome of operational improvements. Quantifying return on investment. Justifying fleet requirements.  They have to make decisions and commitments while working in a complex mining system, where no component is isolated. Typical constraints of mines include:  Many interdependencies and overlapping activities.  Interaction between equipment units.  Changing cycle times of processes.  Spatial limitations, yield and give-way logic of moveable equipment units.  Complex layouts.  Limited capacity of bunkers and conveyor systems.  How do managers plan mining processes while considering all of the above? Traditionally, they make assumptions that are never close to reality, such as:  Average haulage distance. The cycle time of haulage is different, point to point and time to time, under the influence of uncontrollable parameters. Average dump time. If the system overflows or there are queues in front of the ore passes, then the dump time changes. Percentage of time a conveyor is stopped due to overflow. It is never constant, and it cannot be specified by a single number.  Simulation allows mine planners to model processes as they are and get rid of these assumptions.  As such, the largest European potash producer carried out load and haul optimization. With the help of Amalgama and one of the Big Four consulting firms, they created a mining process simulation model using AnyLogic software. A big potash mine is 8 x 8 km in size. 900 thousand tons are mined per month on three underground levels. The mine has 21 km of conveyor belts taking the ore through the system to the skip hoist. The ore is mined with borers, which continuously crush the rock and load it to their attached ore buffers. This ore is then dumped into a dump truck and the dump trucks make runs between borers and ore passes.   20 x (Borer + Dump Truck) The capacity of an ore pass is three tons, but a dump truck carries 22 tons of ore. After the first three tons, the dumping speed depends on the current load of the conveying system underneath the ore pass. Since the conveyor can already be loaded with ore from other upstream borers, the system may be constrained. To get rid of this constraint, mine planners were going to change the equipment configuration by adding Mobile Ore Loaders, or MOLs. In the TO-BE scenario, MOLs played the role of a buffer between dump trucks and ore passes. The dump truck quickly dumped ore into the MOL and returned to the borer while the MOL continued dumping ore into the conveyor system.   Adding Mobile Ore Loaders By adding buffering capacity, mine planners hoped to lower the dump truck cycle time. The main question was whether usage of MOLs would allow them to get rid of one borer while keeping the production volume. The borer carried high operational expenses and required having a maintenance team in the mine, so its removal would significantly lower the operational expenses. Additional questions included which borer to remove and where to use the five MOLs. Solution Amalgama’s simulation developers created an AnyLogic mining simulation model to answer these questions. This mining simulator included the whole mining process from drilling to hoisting, precisely as the plant was laid out.  Load and Haul Optimization Model in 3D  Simulation model of the mine  Candidates for removal with minimum influence on the mining productivity  The model was very detailed, and all the processes were simulated with minimal simplification, making the mining model very accurate.  The first experiment with the model was how the mine system would behave if the external constraint of conveyor speed was removed. This experiment helped find three borers that had a production rate limited by internal constraints such as their own performance, maintenance intervals, buffer size, etc. As a result, three borers were chosen to be candidates for removal with minimum influence on the mine production rate, since MOLs would only remove constraints caused by the speed and capacity of conveyors.   The effect of removing each of three borers  The effect of removing each of these three borers was studied with simulation. These experiments showed that removing borer #65 reduced production the least. Then several scenarios were run to determine where to send the five MOLs to maximize ore production. Five borers were chosen to send the MOLs to. This scenario showed only a 1.02% decrease in production volume, which was negligible. At the same time, this scenario showed a significant decrease of OPEX, since one borer had been removed from the mine.  Outcome The simulation model of the underground mine provided operational improvements to Europe’s largest potash producer and including with load and haul optimization. These improvements allowed operational costs to be cut while keeping the same production volume. Once the project was finished, the mining simulator has been in continuous use for monthly production planning, the identification of potential process bottlenecks, and the evaluation of proposed changes.  Project presentation by Andrey Malykhanov, Amalgama     Major US Airline Decides NOT to Charge Additional Fees Link:  Tags: Marketing  The U.S. commercial airline industry is one of the most diverse, dynamic and perplexing in the world. It is fast-evolving, labor intensive, capital intensive, hyper-competitive and very vulnerable to the ebb and flow of business cycles as well as being among the most regulated of deregulated companies.  Airline management is required to make long-term decisions regarding fleet sizes, market fluctuations, and fuel prices while discovering ways to increase profit in an extremely competitive environment. Problem A major U.S. airline was facing a situation where opportunities to extend the existing strategy were limited, coupled with an increasing cost structure due to competition, commodity prices, and acquisition integration activities. The airline began to explore several options to generate new profits through ancillary products or changes to existing policies and was under intense pressure from board members, Wall Street and various analysts to do so.  Although the revenue generation through charging additional fees was apparent in the short term, prior to implementing a policy change, the Airline opted to evaluate the long term perceived impact on brand equity, market share and customer loyalty.  Solution   Sales Funnel Simulation  PwC, the world’s second largest professional services network, was employed by the Airline to model the predicted impact of the client’s ticket market share and company brand sentiment after introducing new products or policy changes. PwC found traditional marketing mix models to be limited and unable to analyze the airline’s challenges. First, because they are aggregate, all customers are represented in a single regression equation which disregards the fact that not all consumers behave the same. Secondly, these types of models do not show interaction between consumers, when in contrary, customers share stories, attitudes, and memories, known as emergent behavior. Emergence is used to describe the behavior a group exhibits because individuals make different choices than what they would if they were not part of a group, as in the market more often behaving as a whole versus a collection of individuals. A third limitation when using typical market models is the lack of explicit representation of the process of consumer decision making. Analysts would be unable to see consumers gathering information, making informed decisions, and forming consideration sets as they do in the real world. Lastly, in traditional regression models, nonlinear relationships are not accounted for, data is limited to time series data, and there is a relatively short time horizon. In the end, this type of model is inappropriate for most consumer behavior analytics.    Consumer Choice Behaviour Representation in the Model These restrictions, and an increased likelihood of inaccurate results prompted PwC to explore other modeling options. They chose AnyLogic Multimethod Modeling and Simulation Software due to its flexibility, scalability and capability to handle sophisticated, computationally intensive techniques that model behavior of agents (e.g., consumers) in the market.  Utilizing AnyLogic software, PwC built the Experience Navigator, an agent-based consumer behavior model of multiple airline markets which included client competition, the process of consumers making choices and a relatively complete representation of the ecosystem in each market. The project used historical industry data, behavioral economics principles, and measurable experiences to create a behavioral model to help understand the impact on customers’ purchase behavior and the Airline’s social contract.  The information used during model building and calibration included:   Time series of airline market shares (i.e. volume, prices)  Cross section of individual travel behavior Market research from PwC Experience Radar (customer experience survey completed by PwC)  Market research from client discrete choice models  Process and theory from consumer choice literature  Qualitative knowledge of the airline industry  PwC and the Airline are now able to understand how the interaction of different factors (i.e. fare utility, past experience, loyalty and word of mouth) may produce behavior, influence market share and modify markets overall. Outcome PwC’s Experience Navigator is used to:   Analyze the Sales funnel at a particular segment and individual competitor level Visualize consumers changing beliefs over time  Dive into different agents to understand their positive and negative experiences  Forecast revenue impact of change in consumer experience  Set price and marketing influence levels of the Airline   Ultimately, the model results showed that losses in market share and revenue over the long term would significantly offset any gains from charging additional fees. In addition, the model proved that if the company would set the fees the same as the competition, their loss of market share would be considerably greater than the competition, because the choice behavior for this particular Airline was due to positive brand equity and positive perception.  The model provided substantial evidence to convince stakeholders and Wall Street that the Airline should not implement the charging of additional fees, but should cultivate an alternate strategy to increase revenue.  Watch Mark Paich from PwC presenting this project at the AnyLogic Conference 2013:      Manufacturing Decision Support for New Pharma Facility Link:  Tags: Manufacturing, Healthcare    The GSK Parma Facility is a biopharmaceutical manufacturing site in Italy that specializes in bringing new products to market for the science-led global healthcare company GlaxoSmithKline and other third parties. In 2020, management began a capital project to replace the sterile manufacturing facility at the site with one based on new technology. A key objective for the project was to minimize risk to the supply of critical biopharmaceutical drugs.   For analysis, planning, and decision support on the project, GSK worked with the award-winning tech application company Decision Lab.   Problem: Pharmaceutical manufacturing capacity planning   To meet regulatory requirements for manufacturing in a sterile environment, the GSK's facility in Parma, Italy, needed to upgrade its pharmaceutical production line. Key challenges to successfully carrying out these works included minimizing risk to the supply of critical pharmaceutical drugs, while also managing increased and volatile demand forecasts.   Before commencing with construction, management decided to simulate the manufacturing facility. Simulation modeling would allow safe testing and optimization of processes before work commenced on the new pharmaceutical facility, but it would need to account for a large set of input parameters.   Success would result from understanding and accurately predicting overall facility capacity regarding machinery and operations. For machinery, planners wanted to account for the quantity, size, and capacity of key equipment. And, for operational aspects, they needed to consider staff

 

Section 27 of 60
numbers, shift patterns, cleaning schedules, and more.   With a simulation model, management wanted a decision support tool that would inform equipment purchases and perform throughput optimization to improve capacity. The model would also help build the business case for potential design solutions and evaluate them against key value drivers:    Capital expenditure (capex) Operational expenditure (opex) Spare capacity Equipment size Net present value (NPV) Cost of goods (COG) Utilization levels  Solution: Pharmaceutical production simulation   Decision Lab and GSK created the production simulation model in two phases. Phase one included the production processes and used Discrete Event simulation to account for the complex manufacturing activities at the GSK Parma facility.   Simplified process view (click to enlarge)  Phase two was about information delivery, visualization, and the creation of a digital twin of the facility.   Model showcase with heatmap of worker activity (click to enlarge)  Using the DXF CAD file import facility of the AnyLogic simulation modeling software, the model designers accurately represented the facility site plan. Machine processes, operator activities, and other processes were modeled and visualized in a way that clearly defined the major and sub-processes.   Thanks to the custom extensibility of the AnyLogic simulation modeling platform, the designers could add a custom heatmap to show operator activity. This feature was important for the sterile facility because it helped focus cleaning activities.   Each part of the model was constructed to help with site planning, with the result that the design was highly flexible. For example, to compare scenarios, the model operator could easily adjust parameters, such as the number of machines and operators.   Pharmaceutical production facility decision support   During the facility design phase, planners used the model to analyze over 200 potential business scenarios. The results from simulation runs were easy to read thanks to the work in phase two on the visualization characteristics of the model.   A dashboard laid out the processes in a way that abstracts away from the underlying complexity and parallel processes. Production could be viewed on a Gantt chart to understand process timings, and in 3D to comprehend how they were being performed. With an easy way to see how long processes would take, the model could inform decisions about when to start them.   Gantt chart showing process timings  The 3D view helped engage stakeholders at all levels, demonstrating that the model was not just a ‘black box providing answers’ and strengthened design phase arguments.   3D model of pharmaceutical production facility (click to enlarge)  Result: Optimized pharmaceutical manufacturing   The dynamic simulation approach, enabled by AnyLogic, provided meaningful insight into how the future production line should be configured and operated, unlike traditional analytical tools. The simulation model gave planners quick and extensive scenario and options analysis that uncovered key constraints and sensitivities in the original design.   The robustness of the modeling phases, and the strength of support the model gave decision-makers, led to fast approval of the project. At stage-gate approval, there were no delays, and the next phase of the project could start on time.   Furthermore, cost savings came from accurately determining equipment needs and not over-purchasing. The overall capital expenditure is expected to be 20% lower thanks to confidence in equipment requirements.   In the future, engineers at the GSK Parma facility will continue using the model for planning, and are looking to test machine learning to find further process optimization.   The project and its findings were presented by Joy Kuo of Decision Lab and Giovani Giorgio from the GSK Parma Facility at the AnyLogic Conference 2021 - Manufacturing simulation for a new pharma facility concept to support key design decisions.       Manufacturing Optimization for Flexible Manufacturing Systems Link:  Tags: Manufacturing    Overview    MCM (Machine Centers Manufacturing) produces integrated manufacturing solutions according to the principles of Industry 4.0 for customers around the world. Their flexible systems are designed to evolve with production needs and operate with a high level of autonomy. To improve plant design and operation, MCM’s software division, MCE, worked with Engineering Group on developing a plant performance evaluation tool.    Engineering Group is a global software company based out of Rome, Italy. At more than 40 offices around the world, and with over 12,000 associates, they develop solutions for utilities, industry, and healthcare. The company’s simulation and data analytics practice helps customers make more informed decisions and optimize business performance using digital twin technologies.   Together, MCM and Engineering developed an FMS (flexible manufacturing systems) simulator for manufacturing optimization. Simulation and digital twin experts at Engineering tailor-made an FMS-specific simulation modeling library based on the systems knowledge of experts at MCM.   Problem: Flexible manufacturing system design   Modern machining shop floors require production systems that can be adapted to meet production problems as they occur. For technical reasons, most shop floor processes are automated, and, for economic reasons, the flow of materials and resources should also be automated to allow for long periods of unattended operation. The resulting systems are highly complex and would benefit from better forecasting and analytics.   MCM has also found that when designing such complex systems for successful tender, precise dimensioning is necessary to win the bidding process. Further design challenges arise when trying to predict system behavior or plan reconfigurations. And, without knowing system behavior well, it is difficult to define machine control policies.   A performance evaluation tool would help MCM address the challenges associated with FMS plant design. They wanted a tool to help support several activities:    Performance evaluation Initial plant configuration support Automation dimensioning performance insights Insights on component marginal utility Configuration comparisons New FMS plant management business opportunities  Solution: Manufacturing performance evaluation tool   At a high level, the simulation specialists at Engineering fulfilled MCM’s requirements for an FMS Performance Evaluation tool by leveraging the capabilities of AnyLogic simulation modeling. The flexibility of the simulation tool, its connectivity, and its extensibility, made it ideal for meeting MCM’s needs.   AnyLogic-centric solution.  To aid plant design and configuration, Engineering developed an FMS-specific library in conjunction with the plant specialists at MCM. The library was made from a collection of reusable agent and Java classes related to FMS applications such as production units, machines, pallet and tool racks, shuttles, etc.   FMS Library palette and an example component.  Using the FMS-specific library alongside the standard AnyLogic libraries allowed MCM engineers to rapidly prototype FMS facility designs. In testing, they could quickly layout a shop floor using the new library and combine it with regular AnyLogic functionality to develop facility-specific algorithms, such as a plant-dependent control policy.   Machine tooling layout and model logic.  During prototyping and for analysis, model simulations run in a custom UI, as required. These are ideal for verification with non-technical people and analysis by a wide range of stakeholders.   Custom user interface showing data outputs and FMS layout rendering.  Results:   With the FMS Simulator, MCM and Engineering achieved what they set out to do and more.   The system allows modelers to quickly provide high-level simulations during the preliminary stages of selling a plant and give end customers a preview of how the real-life solution will be.   A powerful capability of the FMS Simulator is that it allows the introduction of complex control policies. The blocks in the FMS library were designed so that it is possible to easily detail the main system management algorithms. There exists, within the tool, the capability to reproduce the algorithms governing each production cell. This capability makes it possible to create digital twins for monitoring the real-time status of a plant and carry out virtual commissionings for testing purposes.   Sensitivity analysis for different echelons and parameters indicates a future direction for increasing overall supply chain performance. Different behavioral parameters show diverse influences on the backlog level towards Tier-2 suppliers, hence, the chip shortage for the whole supply chain. More up-to-date and lower time lag in information flow reduces the backlog level.   The tool developers also foresee using FMS simulations for the training or assessment of policies obtained using reinforcement learning.   You can learn more about the project from Engineering's simulation and digital twin expert Roberto Grugni and MCE director Giuseppe Fogliazza in their joint presentation at the AnyLogic Conference 2021:      Maximizing Push Boat Fleet’s Net Voyage Revenue Link:  Tags: Transportation  Problem InterBarge, a first-class waterway operator, affiliated with SCF Marine, a part of Seacor Holding Group, operates freight along the HPP Waterway (Hidrovia Parana Paraguay, located in Argentina, Paraguay, Brazil, and Uruguay) on a dedicated contract carriage. Both push boats and barges are preassigned to these contracts. During certain seasons of the year, these resources are free of contract commitment and/or have free convoy capacity on certain trips. The company’s challenge was to use this free capacity as a fleet, maximizing net voyage revenue, and deliver the dedicated contract freights by choosing the best convoy sizes and vessel allocation. The managers questioned how to schedule all of their operations through this new fleeting mode, where there is complete independence between push boats, barges, and nondedicated contracts, while including the dedicated contracts in the system. They wanted to analyze the behavior of the system where each push boat would decide, in an intelligent way, when arriving at a port or fleeting point, which is the best route to follow, and which barges they can use to build up temporary convoys. Ite Consult found simulation modeling to be the best tool to provide InterBarge with the answers to these questions. The goals of the simulation project were:  To maximize net voyage revenue of the company vehicles and resources through a planning tool (deciding which trips to do, which contracts to commit, where to allocate push boats and barges, and more).  To evaluate the risk of new push boats’ and barges’ acquisition.  To create a tool to identify revenues and negotiate contract freight price.  To create a tool to plan all operations on a short and long term basis.  Solution The model was designed with AnyLogic, using both discrete event and agent based methodologies. Push boats and barges navigate along the rivers, stop in each node or port, and then decide if they should use it as a point to start a nondedicated contract carriage. Push boats or barges make decisions according to their geographical location at the moment the decision has to be made. The model calculates Time Charter Equivalent for each active/available contract and recommends the best convoy configuration, taking into account all potential parameters and constraints. The input data included:  Available waterway demand and seasonality (contracts, products, prices, fees, and more).  Fleet of push boats and barges (resource type, intake, speed, draft, power, bunker consumption, maintenance requirements, and more).  Rivers and ports specifications (water levels, loading and unloading capabilities, rates for each product, and more).  Distance between ports and destination points.  Operation costs and timings of fleet usage and ports’ operations.  Barge Operation Animation      Model constraints and parameters included:     Restricted routes/nodes for push boats due to rivers’ water level or flag restrictions.  Push boats’ pulling capacity when navigating upstream or downstream.  Push boats’ speed when navigating upstream or downstream, loaded or unloaded.  Bunker consumption.  Contracts’ weekly offers and seasonality.  Preassigned fleet.  Barges’ intake by barge type, products’ cubic factor, and water level by month.  Loading and unloading times.  Outcome The model simulates a 5-year period in less than 300 seconds, while considering nearly 250 external variables and multiple scenario options. This decision-support system allows users to easily identify the expected net voyage revenue for given resources and waterway freight transportation demand, and creates recommendations on which strategy to choose.  Users are provided with a group of key indicators such as tons delivered per contract, navigating times and costs, barge and push boat usage and locations, mooring and waiting times in each port, bunker consumption, etc. This information enables managers to make the best decision among those provided by the system. It also allows them to plan routes for push boats and convoys on a short or long term basis. The output data is exported in Excel files in order to make additional reporting or analysis convenient. It is also possible to integrate the model with databases in the company’s existing IT infrastructure. The resulting simulation model utilizes user-driven inputs and scenarios, and can be easily adapted to changes and/or new requirements.     Metro Station Vulnerability Analysis with Crowd Simulation Link:  Tags: Passenger Terminals, Defense  Overview STAM is a multidisciplinary engineering company based in Italy that provides high-tech custom solutions to meet its client’s business challenges. The company leverages its expert knowledge from across engineering domains, including the security and transport sectors, sustainability and the circular economy, defense robotics, industry 4.0, and more. The company is an active participant in European Union research projects. Problem Due to terrorist attacks in Europe in recent years, the European Commission developed its funding opportunities for projects that improve public security and counter terrorist activities. STAM worked on one such project. Its engineers created a simulation model of metro station crowd behavior for the purpose of analyzing different attack scenarios.  The objective of the crowd modeling was to simulate passenger behavior in metro station infrastructure during its normal state and facilitate comparison with various attack scenarios. Engineers also wanted to reveal and analyze any critical vulnerabilities in public

 

Section 28 of 60
infrastructure, using the capabilities of the model for evacuation simulation.  Modeling people's behavior in a standard conditions (click to enlarge)  Solution Engineers simulated an open-air metro station with two independent entrances and two floors. The train platform is located on the upper floor. The model also included station facilities that can be used by pedestrians during their travel time, such as ATMs, toilets, a coffee point, and shops.  Within the framework of the model, STAM engineers created three metro station attack scenarios:  Bomb explosion Melee weapon attack Drone attack  Each type of attack was represented in the model as an agent. For each agent, engineers created a statechart that represented the different phases of an attack. And, by combining statecharts with flowcharts, the agents could move around inside the simulation environment. The only agent without a flow chart was the explosive agent, due to it not needing to move and being represented as a static agent.  Statecharts and flowcharts for the agents (click to enlarge)  Using AnyLogic’s 2D and 3D animation capabilities makes it possible to observe simulation model runs in many different ways, allowing analysts to examine agent behavior for each scenario in a simulation environment.  Simulation of the scenarios   For the first scenario, with the explosive, the model showed the effects of the bomb’s explosion and the response triggered by it, including the reaction times of pedestrians in the vicinity and their evacuation. For the scenario with the melee weapon, the model revealed that crowds took longer to react and evacuate due to the localized character of the attack. Also, the model showed the movement of the terrorist trying to leave the station and being stopped by security.  Finally, in the third scenario, with the drone, the attack happens while a train is arriving at the station – when the amount of people on the platform is at a maximum.  Result The density maps associated with the crowd modeling helped analysts identify critical areas for pedestrian flow and the points where potentially dangerous situations may arise. All scenarios provided results for comparison, showing the number of casualties, injured, and exposed, as well as the overall evacuation time and critical areas.   Simulation results (click to enlarge)  The results of the crowd modeling show that the average time of evacuation is approximately the same regardless of the scenario. The explanation is simply that the metro station infrastructure is the same for all scenarios. The capacity and evacuation routes are the same in each case. Although evacuation times are similar, the number of casualties is not and depends on the type of attack. The drone attack caused the greatest number of casualties and injuries due to its targeting of a train arrival, when pedestrian numbers are at maximum density. The company continues to collaborate with EU Commission on the terrorism safety project. Furthermore, STAM plans make additions to their project. For example, a smart signaling system to improve evacuation times. They also intend to implement and simulate security systems that are based on artificial intelligence algorithms that can recognize suspicious objects, such as a bomb. Evaluating these systems can determine how they can best help prevent an attack.   The case study was presented by Pietro De Vito, of STAM, at the AnyLogic Conference 2021.  The slides are available as a PDF >>  344+ 34+ 34+  Similar case studies  Passenger Flow Simulation at Frankfurt Airport  As the operating company of several major international airports, Fraport AG is one of the main “Global Players” in the airport industry. With more than 140,000 passengers per day and over 80 aircraft movements per hour, Frankfurt airport – an aviation hub of worldwide significance – is Fraport AG's...  Read more  Montreal International Airport: Simulation for Early Bag Storage System Implementation  GSS Inc. is a Canadian engineering company that provides strategic and technical consulting services with a strong focus on simulation and optimization in the context of major infrastructure and transformation projects. GSS helps clients in different industries including healthcare, airports, transportation, logistics, and...  Read more  Optimizing Airport Processes and Designing Transportation Facilities with Simulation  TranSystems is an architect and engineering company with over 25 years of modeling experience in the transportation industry. The company works on projects related to railroads, airports, seaports, roadways, transit supply chains, industrial facilities automation, and even quick-service type operations in restaurants and...  Read more  Crossing the English Channel with AnyLogic  AREP, the subsidiary of «SNCF Gares & Connexions», developed a simulation model which optimized the utilizing of the Transmanche Terminal of Paris’ Gare du nord. The main goal was to reduce the waiting time upstream and at the points of control of passengers. The model enabled to locate, value and represent...  Read more  Creating a Smart Baggage Handling System for Montreal International Airport   From 2020, due to the global pandemic, airports saw decreased passenger numbers around the world. As this pandemic starts to subside, airports are looking to increase their traffic numbers. Therefore, Montreal International Airport enlisted GSS, a Canadian based consultancy firm, dealing primarily with Airports, Aerospace &...  Read more  Improving Plane Maintenance Process with AnyLogic Simulation Software  The military aircraft maintenance turnaround process (the in-between time when the aircraft touches down, is refueled, rearmed, and inspected, in order to be released) is complex and, being fairly time-consuming, includes multiple interactions and parallel workflows. Engineers from Lockheed Martin, one of the largest companies...  Read more  AnyLogic Tackles Eiffel Tower Crowds  The Eiffel Tower operating company (SETE) used AnyLogic software to optimize the flow of tourists at the monument. With its help the engineers increased the number of visitors without redesign, but with people movement control.   Read more  Military Aircraft Maintenance Scheduling and Staffing Optimization   The Corps of Royal Electrical and Mechanical Engineers (REME) maintains all electrical equipment within the British Army. Among other duties, engineers maintain the Apache Attack Helicopter, one of the world’s most advanced multi-role combat helicopters. This aircraft is very maintenance demanding: it takes about 35 hours of...  Read more  Disaster Response Planning Using Agent-Based Simulation  In an effort to find practical operational solutions for a response to an unexpected crisis or natural disaster, Battelle, the world’s largest, non-profit, independent R&D organization, needed to test the effectiveness of a 48-hour shelter-in-place order for an Improvised Nuclear Device scenario. The goal was to reduce...  Read more  Automated Driving Systems Testing Using Agent-Based Modeling   One of the SwRI's research areas is automated driving systems. Engineers decided to make autonomous vehicles free, not only from the driver, but also from a control center. According to this idea, vehicles would communicate in a distributed manner with each other, share information about their current location and environment...  Read more  More case studies  Get a brochure with industry case studies Download  © The AnyLogic Company  | Privacy Policy   | Cookie Policy contact us  download free simulation software AnyLogic Cloud anyLogistix supply chain software blog use of simulation  agent-based simulation discrete event simulation system dynamics material handling library manufacturing optimization  manufacturing capacity planning epidemiology simulation predictive modeling in healthcare pharmaceutical simulation optimizing airport processes  subscribe to our newsletter      (function() {         var didInit = false;         function getCookie(name) {             var value = "; " + document.cookie;             var parts = value.split("; " + name + "=");             if (parts.length == 2) return parts.pop().split(";").shift();         }         function initMunchkin() {             if(didInit === false) {                 didInit = true;                 Munchkin.init('038-NFP-336',{'domainLevel':2});                  let url_string = window.location.href;                 let url = new URL(url_string);                 let hash = url.searchParams.get("hash");                 let mkto = getCookie('_mkto_trk');                 if(hash && mkto){                      let user = {                         hash: hash,                         cookie: mkto,                     };                      let response =  fetch('/local/ajax/associate.php', {                         method: 'POST',                         headers: {                             'Content-Type': 'application/json;charset=utf-8'                         },                         body: JSON.stringify(user)                     });                 }             }         }         var s = document.createElement('script');         s.type = 'text/javascript';         s.async = true;         s.src = '//munchkin.marketo.net/munchkin.js';         s.onreadystatechange = function() {             if (this.readyState == 'complete' || this.readyState == 'loaded') {                 initMunchkin();             }         };         s.onload = initMunchkin;         document.getElementsByTagName('head')[0].appendChild(s);     })();                      var ct_checkjs_val = 'd411ec76d588be778be009b314ae4b14', ct_date = new Date(),                      ctTimeMs = new Date().getTime(),                     ctMouseEventTimerFlag = true, //Reading interval flag                     ctMouseData = [],                     ctMouseDataCounter = 0;                      function ctSetCookie(c_name, value) {                         document.cookie = c_name + '=' + encodeURIComponent(value) + '; path=/';                     }                      ctSetCookie('ct_ps_timestamp', Math.floor(new Date().getTime()/1000));                     ctSetCookie('ct_fkp_timestamp', '0');                     ctSetCookie('ct_pointer_data', '0');                     ctSetCookie('ct_timezone', '0');                      ct_attach_event_handler(window, 'DOMContentLoaded', ct_ready);                      setTimeout(function(){                         ctSetCookie('ct_timezone', ct_date.getTimezoneOffset()/60*(-1));                         ctSetCookie('ct_checkjs', ct_checkjs_val);                       },1000);                      /* Writing first key press timestamp */                     var ctFunctionFirstKey = function output(event){                         var KeyTimestamp = Math.floor(new Date().getTime()/1000);                         ctSetCookie('ct_fkp_timestamp', KeyTimestamp);                         ctKeyStopStopListening();                     }                      /* Reading interval */                     var ctMouseReadInterval = setInterval(function(){                         ctMouseEventTimerFlag = true;                     }, 150);                      /* Writting interval */                     var ctMouseWriteDataInterval = setInterval(function(){                         ctSetCookie('ct_pointer_data', JSON.stringify(ctMouseData));                     }, 1200);                      /* Logging mouse position each 150 ms */                     var ctFunctionMouseMove = function output(event){                         if(ctMouseEventTimerFlag == true){                              ctMouseData.push([                                 Math.round(event.pageY),                                 Math.round(event.pageX),                                 Math.round(new Date().getTime() - ctTimeMs)                             ]);                              ctMouseDataCounter++;                             ctMouseEventTimerFlag = false;                             if(ctMouseDataCounter >= 100){                                 ctMouseStopData();                             }                         }                     }                      /* Stop mouse observing function */                     function ctMouseStopData(){                         if(typeof window.addEventListener == 'function'){                             window.removeEventListener('mousemove', ctFunctionMouseMove);                         }else{                             window.detachEvent('onmousemove', ctFunctionMouseMove);                         }                         clearInterval(ctMouseReadInterval);                         clearInterval(ctMouseWriteDataInterval);                                     }                      /* Stop key listening function */                     function ctKeyStopStopListening(){                         if(typeof window.addEventListener == 'function'){                             window.removeEventListener('mousedown', ctFunctionFirstKey);                             window.removeEventListener('keydown', ctFunctionFirstKey);                         }else{                             window.detachEvent('mousedown', ctFunctionFirstKey);                             window.detachEvent('keydown', ctFunctionFirstKey);                         }                     }                      if(typeof window.addEventListener == 'function'){                         window.addEventListener('mousemove', ctFunctionMouseMove);                         window.addEventListener('mousedown', ctFunctionFirstKey);                         window.addEventListener('keydown', ctFunctionFirstKey);                     }else{                         window.attachEvent('onmousemove', ctFunctionMouseMove);                         window.attachEvent('mousedown', ctFunctionFirstKey);                         window.attachEvent('keydown', ctFunctionFirstKey);                     }                     /* Ready function */                     function ct_ready(){                       ctSetCookie('ct_visible_fields', 0);                       ctSetCookie('ct_visible_fields_count', 0);                       setTimeout(function(){                         for(var i = 0; i < document.forms.length; i++){                             var form = document.forms[i];                             if (form.action.toString().indexOf('/auth/?forgot_password') !== -1)  {                                 continue;                             }                             form.onsubmit_prev = form.onsubmit;                             form.onsubmit = function(event){                                  /* Get only fields */                                 var elements = [];                                 for(var key in this.elements){                                   if(!isNaN(+key))                                     elements[key] = this.elements[key];                                 }

 

Section 29 of 60
/* Filter fields */                                 elements = elements.filter(function(elem){                                      var pass = true;                                      /* Filter fields */                                     if( getComputedStyle(elem).display    === 'none' ||   // hidden                                         getComputedStyle(elem).visibility === 'hidden' || // hidden                                         getComputedStyle(elem).opacity    === '0' ||      // hidden                                         elem.getAttribute('type')         === 'hidden' || // type == hidden                                         elem.getAttribute('type')         === 'submit' || // type == submit                                         elem.value                        === ''       || // empty value                                         elem.getAttribute('name')         === null                                     ){                                     return false;                                     }                                      /* Filter elements with same names for type == radio */                                     if(elem.getAttribute('type') === 'radio'){                                         elements.forEach(function(el, j, els){                                         if(elem.getAttribute('name') === el.getAttribute('name')){                                             pass = false;                                             return;                                         }                                     });                                 }                                  return true;                             });                              /* Visible fields count */                             var visible_fields_count = elements.length;                              /* Visible fields */                             var visible_fields = '';                             elements.forEach(function(elem, i, elements){                               visible_fields += ' ' + elem.getAttribute('name');                             });                             visible_fields = visible_fields.trim();                              ctSetCookie('ct_visible_fields', visible_fields);                             ctSetCookie('ct_visible_fields_count', visible_fields_count);                              /* Call previous submit action */                             if(event.target.onsubmit_prev instanceof Function){                               setTimeout(function(){                                 event.target.onsubmit_prev.call(event.target, event);                               }, 500);                             }                           };                         }                       }, 1000);                     }                      function ct_attach_event_handler(elem, event, callback){                       if(typeof window.addEventListener === 'function') elem.addEventListener(event, callback);                       else                                              elem.attachEvent(event, callback);                     }                      function ct_remove_event_handler(elem, event, callback){                       if(typeof window.removeEventListener === 'function') elem.removeEventListener(event, callback);                       else                                                 elem.detachEvent(event, callback);                     }                      if(typeof jQuery !== 'undefined') {              /* Capturing responses and output block message for unknown AJAX forms */             jQuery(document).ajaxComplete(function (event, xhr, settings) {               if (xhr.responseText && xhr.responseText.indexOf('"apbct') !== -1) {                 try {                   var response = JSON.parse(xhr.responseText);                   if (typeof response.apbct !== 'undefined') {                     response = response.apbct;                     if (response.blocked) {                       alert(response.comment);                       if(+response.stop_script == 1)                         window.stop();                     }                   }                                   } catch (e) {                   return;                 }                }             });            }  if(!window.BX)window.BX={};if(!window.BX.message)window.BX.message=function(mess){if(typeof mess==='object'){for(let i in mess) {BX.message[i]=mess[i];} return true;}}; (window.BX||top.BX).message({'JS_CORE_LOADING':'Loading...','JS_CORE_WINDOW_CLOSE':'Close','JS_CORE_WINDOW_EXPAND':'Expand','JS_CORE_WINDOW_NARROW':'Restore','JS_CORE_WINDOW_SAVE':'Save','JS_CORE_WINDOW_CANCEL':'Cancel','JS_CORE_H':'h','JS_CORE_M':'m','JS_CORE_S':'s','JS_CORE_NO_DATA':'- No data -','JSADM_AI_HIDE_EXTRA':'Hide extra items','JSADM_AI_ALL_NOTIF':'All notifications','JSADM_AUTH_REQ':'Authentication is required!','JS_CORE_WINDOW_AUTH':'Log In','JS_CORE_IMAGE_FULL':'Full size','JS_CORE_WINDOW_CONTINUE':'Continue'});BX.setJSList(['/bitrix/js/main/core/core_ajax.js','/bitrix/js/main/core/core_promise.js','/bitrix/js/main/polyfill/promise/js/promise.js','/bitrix/js/main/loadext/loadext.js','/bitrix/js/main/loadext/extension.js','/bitrix/js/main/polyfill/promise/js/promise.js','/bitrix/js/main/polyfill/find/js/find.js','/bitrix/js/main/polyfill/includes/js/includes.js','/bitrix/js/main/polyfill/matches/js/matches.js','/bitrix/js/ui/polyfill/closest/js/closest.js','/bitrix/js/main/polyfill/fill/main.polyfill.fill.js','/bitrix/js/main/polyfill/find/js/find.js','/bitrix/js/main/polyfill/matches/js/matches.js','/bitrix/js/main/polyfill/core/dist/polyfill.bundle.js','/bitrix/js/main/core/core.js','/bitrix/js/main/polyfill/intersectionobserver/js/intersectionobserver.js','/bitrix/js/main/lazyload/dist/lazyload.bundle.js','/bitrix/js/main/polyfill/core/dist/polyfill.bundle.js','/bitrix/js/main/parambag/dist/parambag.bundle.js']);  (window.BX||top.BX).message({'LANGUAGE_ID':'en','FORMAT_DATE':'DD.MM.YYYY','FORMAT_DATETIME':'DD.MM.YYYY HH:MI:SS','COOKIE_PREFIX':'BITRIX_SM','SERVER_TZ_OFFSET':'10800','UTF_MODE':'Y','SITE_ID':'s1','SITE_DIR':'/'});  (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':                     new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],                 j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=                 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);             })(window,document,'script','dataLayer','GTM-NC48GT');       require(["jquery", "analytics", "domReady!"], function($, analytics) {         $(document).on('click', '.link-analytics-count', function(){             analytics.trackEvent('all', 'case-studies-brochure', 'Case studies brochure download button');         });     });    Microsoft End-to-End Supply Chain Management Solution: Design, Testing and Implementation Link:  Tags: Supply Chains  Problem            Microsoft is one of the largest tech companies in the world, with a sales revenue of over 90 billion US dollars. More and more of its sales revenue now come from selling physical devices, and this exposes the company to an accompanying level of complexity and uncertainty. The company has over 30,000 products with a large variation in life cycles, more than 600 suppliers, 13 contract manufacturers, and 52 distribution centers distributing products into 191 countries.            In 2015, Goldratt Research Labs was contracted by Microsoft to design, validate, pilot and implement an end-to-end Supply Chain Management solution based on Theory of Constraints (TOC) best practices. This formed part of Microsoft’s “One Devices Supply Chain” (1DSC) solution designed to achieve:    One set of Planning, Execution and Improvement rules for managing all products and order fulfillment strategies within Microsoft’s supply chain. One system (SAP) in which to implement these rules.           A critical part of the project was for Goldratt Research Labs to develop a self-configurable Supply Chain simulation model using AnyLogic to validate the likely operational and financial improvements that would be achieved from the TOC based solution design before going live.   Project developer            Dr. Alan Barnard and his Goldratt Research Labs (GRL) simulation team lead by Dr. Andrey Malykhanov are very passionate about helping organizations answer two simple but important questions: "How much better can you do?" and "How best to do it?". They work with leading organizations from around the globe, in both private and public sectors, among them: BHP Billiton, Cargill, TATA Steel, ABB, Daiwa House, Utah Gov., and Larsen & Toubro.            GRL uses robust research methods and advanced technologies like Simulation modeling, Artificial Intelligence and Expert Systems to identify how much better organizations can do if they switched from “local optima” to the “global optima” rules of Theory of Constraints (TOC). GRL is also a spearhead for new TOC based research and development, to develop new knowledge and applications of the ever-growing TOC body of knowledge.   What is Theory of Constraints (TOC)?                            According to Dr. Barnard, Theory of Constraints is called “a theory” for a good reason. Kurt Lewin (1890 –1947) one of the pioneers in organizational psychology famously said that “There is nothing as practical as a good theory”. In Science, we call something a “theory” if it provides a good explanation of why something is important and useful. Theory of Constraints, as a Theory of Management, explains why knowledge of a System’s constraint, is important and useful in helping managers make five challenging decisions in any complex system:   How to define ambitious but achievable goals? Knowing the constraint of a system determines the theoretical maximum this system can produce. How to decide where to focus? The real constraint in any social system is limited management attention simply because the demand on our attention will always exceed our available attention available. Knowing where the constraint is in a system helps managers decide where to focus their limited attention to achieve more with less in less time.  How to judge the system impact of changes? Knowing the impact of a change on the system constraint provides a “shortcut” for judging the impact of any changes on the whole system. How to decide what rules to use? In turns out that the rules needed to optimize a system constraint — making sure that the constraint is never blocked, starved, overproducing, or wasting capacity in other ways is also the rules to optimize the performance of the whole system. How to decide when to change the rules? If the constraint moves, the rules must change. If you have a production constraint, then running long batches on the bottleneck machine to save setups make sense. But when the constraint moves to the market, this rule has to be changed to ensure capacity is not wasted or sales lost producing things the market don’t want.                           Theory of Constraints creator, Dr. Eli Goldratt, and TOC practitioners like Dr. Barnard and his team from GRL, has over the past 30 years, identified a number of often counter-intuitive but global optima rules that can result in step-change improvements in managing operations, supply chains and project environments.                            A number of these TOC best practice rules for managing supply chains was used in the core design of the Microsoft end-to-end SCM solution and additional innovations in Supply Chain planning and execution was developed to cater for unique complexities within the Microsoft landscape.   Project description            The project started in September 2015. The Theory of Constraints principles and best practices were used by GRL to design a world-class supply chain for Microsoft. During the pilot stage, the Theory of Constraints based SCM rules were tested in an AnyLogic simulation model to validate that it will result in better operational and financial performance (e.g. less shortages and surpluses resulting in more revenue, higher profitability with lower inventory). By mid-December 2015, the first set of TOC derived rules for Build-to-Order (BTO) went live within Microsoft’s SAP system. In 2016, additional rules for Build-to-Availability (BTA), “Assemble or Customize-to-Order” (ATO) and “Build-to-X” (BTX) were tested in implemented. The whole project was finished in less than 9 months.   Solution            A Strategy-and-Tactic tree (S&T Tree) was developed to show exactly what rules were required to achieve the desired performance improvement and objectives of the 1DSC initiative. As per figure below, six rules needed to be changed:    Order fulfillment strategy: should it be make-to-order (BTO), assemble-to-order (ATO), make-to-availability (BTA), or build-to-X(where X is launch target quantity) (BTX) ATO/BTO buffering: how to quote reliable lead times to BTO/ATO customers. BTA/BTX buffering: how to ensure we have sufficient stock of parts and finished goods. Release control: how to control the release of BTO/ATO, BTX and BTA orders. Shared priority: how to achieve a Single Priority System throughout supply chain. Improve flow: how to systematically improve flow.           With the S&T Tree, the Why, What and How of each of these rules were defined to help all stakeholders understand the assumptions behind the rules and to find the simplest way to implement these within standard SAP.   Designing TOC-based SCM solution           The AnyLogic simulation model that was developed to test and verify the operational and financial performance of the new rules.            Supply Chain and Product Data is extracted from the SAP system into an Excel file to fully model and configure the Microsoft global supply chain. Additionally, data of actual customer demand and actual daily on-hand-stock over the modeling period were extracted to allow the model to compare simulated (using the new TOC-based rules) vs. actual past performance.            Randomness was introduced in the model by the Supply chain planning team around such things as variability in production cycle times and distribution supply lead time. Also, information on random events such as planned and unplanned maintenance as well as the impact of positive and negative demand forecast error can be integrated to stress-test the Microsoft supply chain.            The AnyLogic custom model was designed to have a simple user interface to allow stakeholders to manage the scenarios and select the TOC rules to use, the simulation time period, product-channels to include or exclude when running the model without any knowledge of AnyLogic. The output of the simulation is visible directly from the AnyLogic model and can also be exported to Excel. Should results not meet expectations, extensive logs are created to help with diagnostics.            The

 

Section 30 of 60
model can be run in three different ways:    Single run experiment Sensitivity analysis to test how sensitive certain parameters are to the outcomes. Scenario comparison  Project basic architecture           The AnyLogic model setup user interface includes network flow and world map to visualize the supply chain configuration for which data was imported, the product categories as well as Replenishment types. Users can view cumulative plots of actual vs. forecast demand vs. production capacity to get a sense of possible issues and users have the ability to use both actual demand forecast and customer orders or to generate forecasts from actual orders or orders from forecasts with a predefined positive or negative forecast error to test how the supply chain will cope with demand that is significantly more or less than expected.            During the running of the simulation model, users can view the operational and financial performance and zoom into specific parts, such as a contract manufacturer to see if backlogs are developing and compare simulated vs. actual stock-on-hand for any specific DC and any specific product to show the benefit of using the TOC rules.            Users can “zoom” into a specific DC and a specific product in that DC to see how the TOC-based rules for managing inventory that dynamically resizes the Target Stock Levels based on “too much red” and “too much green” buffer zone penetration performed against past actual daily stock-on-hand.   Outcome           Microsoft’s CTO of Global Supply Chain, Robert Meshew, confirmed the incredible results they have achieved since implementing the new TOC-based end-to-end Supply Chain Solution into their SAP system. “The outcome has been nothing short of remarkable. In that time, we've seen our service levels rise to our customers by over 5%. At the same time, we've seen our inventory levels drop by quarter billion dollars across the board which has led to reduced markdowns and reduced excess and obsolescence of over a hundred million dollars”.           Using the capabilities of AnyLogic, offered a fast, low cost and low risk way to validate and stress-test the Theory of Constraints based end-to-end Supply Chain solution design.            The next stage of this project will be to work closely with the Microsoft team lead by Manohar Madhira integrate the model into the company's Sales and Operations planning process as well as to enhance its functionality to be used for “What’s ifs” related to Global Product Launch to support better faster analytics and management decisions.    Project presentation by Dr. Alan Barnard, Goldratt Research Labs     Military Aircraft Maintenance Scheduling and Staffing Optimization  Link:  Tags: Defense  Problem           The Corps of Royal Electrical and Mechanical Engineers (REME) maintains all electrical equipment within the British Army. REME engineers are tasked with the maintenance, recovery, repair, and manufacturing of the battle equipment to keep it in fighting order at the battlefields and at the military bases, both at home and overseas.            Among other duties, engineers maintain the Apache Attack Helicopter, one of the world’s most advanced multi-role combat helicopters. This aircraft is very maintenance demanding: it takes about 35 hours of maintenance to cover one hour of flying. That was one of the reasons REME management considered their unit understaffed and claimed that it had to expand the mechanics department. However, their supervisors from Air Army Corps felt like the REME was being over-resourced, since on average they had enough staff to manage this workload.            Lack of agreement between these two military organizations made it clear that to solve REME’s workforce planning problem, the management needed to look deeper into data to make more evidence-based decisions. They tasked dseConsulting  (SimulAi) company with the challenge to create a robust tool for improving staffing strategies that would help them optimize scheduling and manning, and then increase the availability of helicopters. For these reasons, the consultants applied AnyLogic maintenance simulation and optimization software. The consultants aimed at:    Representing deployment cycle processes in the digital environment. Analyzing cause-effect relationships between the processes. Determining robust manning strategies, including the targeted use of planned contractors based on sound evidence. Forecasting peak and drop cycles in the workload. Learning how human actions, one of the key components of the deployment cycle process, might impact the system.  Solution            The consultants built a simulation model for analysis and further optimization of the maintenance processes during the deployment cycle. The model included three areas of complex behavior lying within the real-life system:    Aircrafts — in the model, they were deployed to different sites, where they were maintained by accompanied engineers, and then returned to the base where maintenance was required. Aircraft components could, in turn, randomly deteriorate during their life cycle. Deployment — aircraft deployment in the model was simulated for a five-year period. As it was impossible to plan the deployment for such a long term, this uncertainty was reflected in the model. In addition, consultants simulated the wearing out of aircraft components because of environmental conditions. People — based on the real data, consultants displayed how aircraft maintenance staff experience, individual efficiency, and stress influence deployment cycle processes.           With the AnyLogic multimethod simulation approach, consultants were able to model deployment cycle processes and handle their complexity without any simplification. Among other approaches, agent-based simulation allowed the modelers to reflect the behavior of aircraft engineers in detail, including their experience and the level of burn-out, to demonstrate more robust statistics.            In the simulation model, the following statistics were collected:    The number of contractors at a given time of simulation and the number of aircrafts Utilization rate of staff over time Average level of stress Average level of experience Average staff efficiency           By analyzing the maintenance optimization model results, the consultants concluded that the human factor aspect played an important role in deployment cycle activities. For example, if an engineer was tired and inefficient, the job would take longer and affect all related processes. However, if new staff was recruited, the efficiency level would also drop because new recruits tend to be less experienced. As a result, the REME management had to find the balance between pushing more work to the experienced personnel, causing burn-outs, and hiring less experienced contractors.            The simulation model provided some significant insights for the management:    When helicopters are at the deployment, the on-base staff utilization is low even though some helicopters are still at base. However, when helicopters are back, they are being maintained with a burst of employee engagement. This fact should be considered in further scheduling. When engineers leave for the deployment, few people stay at the base. That is why a risk emerges: if too many helicopters come back at the same time, there will not be enough personnel to service the helicopters. In addition, on-base aircraft engineers might experience heavy overload, which can negatively affect overall job results. As planned and emergency contractors are costly to recruit, the model allowed the management to analyze how the number of contractors could be optimized while facilitating workflows.  Result            As a result of the maintenance optimization modeling project, the consultants offered the customer a decision support tool that could be used for planning staffing policies and improving staff coordination and management.            At this stage of the project, simulation indicates that with smarter planning there is the potential for a 20% aircraft fleet availability improvement at a lower cost, in addition to the organization's prime targets.            It is estimated that the second phase of the project might save REME about $2.7 million in staff costs.      Modeling Intelligent Control Systems Based on a Digital Platform Link:  Tags: Asset Management  Overview INTELAB is an energy lab within the  RTSoft  group. The company's goal is to develop and implement intelligent software services and solutions for distributed energy resources (DERs). INTELAB is focused on:    Improving the efficiency of energy management through optimization and forecasting algorithms.   Upgrading projects with the help of modern technologies for storage and generation of electricity.   Modeling energy resources digital twins.   Development of the digital platform to make their solutions more flexible.   Problem The company needed a digital platform to develop applied control systems for distributed energy resources because the existing solutions were too expensive and complicated to configure. The platform allowed the energy lab to integrate different software products and reduce the time-to-market and the cost of the control system.    Platform description    INTELAB chose simulation to show the performance of the platform to customers and assess the technical and economic effect of using the platform.  Solution Conventional programming didn't fit INTELAB’s objectives. It required a lot of specialists’ time to prepare an MVP and develop a graphical user interface. It was also difficult to model specific applications where the control object could be presented as a population of agents (e.g., electric vehicles or aggregated power facilities).  Therefore, INTELAB decided to use AnyLogic software. AnyLogic provided  agent based simulation, visualizations for nontechnical stakeholders, easy connection to the digital platform to show its functions, and quick preparation of the prototypes demonstration.    The process of simulation    The purpose of the simulation was to demonstrate the effectiveness of the platform for managing microgrids. The simulation goals were the testing of the short term optimization module and demonstrating the benefits of using the optimization algorithm. Its function was to minimize the cost of generating electricity while satisfying system boundary conditions.    Model description (click to enlarge)   The model included the following agents:    Renewable generation (Wind Plant)   Accumulator (Electrical energy storage system)   Power Plant (Diesel generation sets)   Uncontrolled load   Platform (DER control system)   Consume   Outputs provided the following:    Operation time of diesel generators, hours   Energy generated from the start, MWh (megawatt-hours)   Cost of diesel generation, $/kWh (kilowatt-hours)   Total cost, $   Results INTELAB compared two methods:    Local control algorithm. This was the simpler one, which they used mostly for microgrids. INTELAB’s specialists should always maintain the load of the switched-on generators between 37% and 75% of the rated power.   Optimization module provided by the platform.   The results with AnyLogic showed that usage of the second method shortened the number of diesel generators operating hours by 41% and reduced cost of diesel power plant generation up to 20%.  It was much easier for INTELAB to use AnyLogic than to create their own software. AnyLogic reduced the time spent for preparing MVP of platform applications. Simulation modeling demonstrated the effectiveness of the optimization module.  They also realized that it could be used to expand the platform. For example, universities could use the AnyLogic model as a teaching platform for making control systems.  Because of this success, INTELAB plans to start an  electric vehicles  (EV) charge management project. AnyLogic is the perfect solution to model the behavior of EV. It makes it possible to model the development of electric transport in the city and to assess its influence on the electrical grid. To simulate scenarios such as EV charge management, the use of AnyLogic is necessary and can significantly reduce the cost of MVP development.   Read more about EV operations simulation model in the case study of SimPlan  ”Modeling of municipal electric vehicle fleets”.  Watch the video about the case study presented by INTELAB at the  AnyLogic Conference 2021:       Modeling Social Behavior on Energy Consumption Link:  Tags: Social Processes  Problem A major problem that building owners or managers face is how to motivate people to conserve energy, while at the same time influencing them to encourage others to do the same. In other words, how can they implement behavioral influencing strategies to improve efficiency in commercial and educational buildings.  Solution An agent-based model was created in AnyLogic to solve this problem.   Noorjax Consulting  applied a data model during a simulation runtime in order to change the behavior of a system based on that data model. The researcher chose a university as the building to be simulated, and to simplify it, only one floor of that building.   Two hypotheses were developed:   A young, healthy person might choose to use the stairs instead of the elevator, turn off the lights, or not use the air-conditioning.  Interventions could be generated, such as money, public recognition, etc., which might help individuals choose to conserve energy.   A socio-technical framework was built in order to understand the problem in a more structured way.   Socio-technical framework of the model (click to enlarge)   The agent-based predictive model was built using subjective data from a sample survey. There were 140 individuals in the survey and about 40 questions. Using the replies from the survey, answers to two more questions could be predicted, which could then be used to make further predictions about individual behavior. To predict the values for these two questions, a Python model was built using a variant of multivariate regression.   The two questions were:    Do I consciously turn off lights and appliances when I do not need them no matter where I am?   In a normal situation, do I prefer to take the stairs or use elevators to get to my destination?   The answers can be predicted based on the independent values from the survey. Demographic information will not change in the short-term, but views about climate change and energy conservation could change through influence, education, or rewards. After these changes, the predictive model could then predict new behavior on energy savings in the building by an individual.  Pypeline is a connector library for AnyLogic, and it allowed the easy and smooth creation of an interface between AnyLogic and Python. The most popular machine learning library for Python, scikit-learn, was used. In this

 

Section 31 of 60
scikit-learn library, there are regression methods available, and they were used to make the predictive model that was needed. This static predictive model was created to understand how an individual would behave in terms of energy savings based on the demographics of that person. The simulation then made the model dynamic.   For example, during a meeting between staff and students on climate change there could be a set of three abstract independent discrete variables – the questions in this case, valued 1 to 5. Then there could also be a dependent variable – the incentive for someone to conserve energy, in this case, known as behavior S, also valued 1 to 5.   During the meeting someone could be convinced about the topic, A or B, or C, etc. If the person was influenced and changed their answer to question A from 2 to 3, for example, the Python model would predict the new value of S. This is illustrated in the model below.    Influence and predictive model showing a change in thinking and a new behavior    The building manager in the university could then understand how to push forward on education and information in order to fulfill the energy-saving agenda by using the predictive model. This predictive model is enough to make decisions because it illustrates which variables have more weight in the prediction of a dependent variable.  Results The prediction model helped during the simulation runtime to change the characteristics of individuals who belonged to the system.  After running multiple simulations there was a 6-8 percent reduction in energy utilization based only on education and influence. Interventions such as money or discounts should give a similar level of improvement.    Results of the simulation showing the energy saving percentage    There was also a secondary effect of this study which was not illustrated. This was an improvement in the general health of the population who visited the building because they used more calories when taking the stairs instead of the elevator.  The results, however, are not as important as the general concept portrayed in this case study. This project is not completed yet and further elements and scenarios will be added in the future.  The use case was designed to inspire AnyLogic users or team leaders to apply this idea to their own models. It was also supposed to inspire users to think about the possibility of using  machine learning models or statistical models in their simulation.  The case study was presented by Felipe Haro, of Noorjax Consulting, at the AnyLogic Conference 2021.  The slides are available as a PDF.     Modeling and Optimization of a Maritime Transport System for an Offshore Oil Platform  Link:  Tags: Oil & Gas  Problem  The Prirazlomnaya oil-producing platform owned by Gazprom Neft company is currently the first Russian hydrocarbon production project on the Arctic shelf. Since it was designed for the Arctic region, the platform can be used in extreme weather conditions and withstand maximum ice loads, which makes it a unique and technically complex project. The first loading of oil from the platform to an arctic tanker was carried out in April 2014, while by 2021 the platform operator is planning to increase the rate of oil production up to five million tons. However, gained experience revealed that a marine transport system for oil export and delivery of supply cargoes to and from the platform should be improved. Taking into account the variability of the Arctic sea meteorological and ice conditions, it was necessary to increase the effectiveness, survivability, and safety of the maritime transport system. To meet this challenge, in 2016, the experts from a state research center studied a transport system for the Prirazlomnaya platform, for the period up to the year 2038, using a detailed simulation model developed in the AnyLogic environment. Solution The marine transport system includes the platform itself, two shuttle tankers, and several offshore supply vessels. Despite the apparent simplicity, the transport system is quite specific due to several important features:    Each of the four platform’s cargo terminals has its operational limitations due to meteorological and ice conditions characterized by high variability. Since oil is loaded to shuttle tankers directly from the platform, together with the processing of supply cargoes, vessels of different types have to compete with each other for weather windows of terminals. Сargo flows of oil and supplies vary significantly both in their quantity and structure. The capacity of oil tanks and supply storages on the platform is limited, which is why just-in-time organization of supply and oil export processes is needed, without creating dead stocks of cargoes.  By using agent-based and discrete event modeling in the AnyLogic environment, the state research center experts created a simulation model of the transport system. The model allowed them to consider all of the important technical characteristics, along with physical and logistic processes of the real system. The vessel operations between the Prirazlomnaya platform and the Murmansk port were modeled in the GIS environment, taking into account natural conditions on the route.  Marine Transportation Network Model  Several interacting computational processes were represented as separate simulation algorithms in the general simulation model:  Cargo transportation, i.e. vessel operation according to the voyage plan. Generation of weather conditions and determination of cargo terminals’ accessibility. Modeling of dynamics of filling and discharging of platform storages. Modeling of ship operations near the platform. Auxiliary stochastic processes and events, such as the addition and removal of vessels from operation, the arrival of a helicopter, etc.  Simulation Model of the Transport System  To describe simultaneous accessibility of the four cargo terminals while considering their limitations, the experts created the stochastic generator of natural conditions at the area of platform location. The generator enabled them to model a time series of 15 environmental parameters, such as speed and direction of wind and current, height of waves, cloudiness, air temperature, visibility, and others, taking into account the interrelation of different parameters. For example, wind and current have an impact on speed and direction of an ice drift.  The model also described the complicated logic of multiple ship approaches to the platform, that are affected by varied weather conditions. The experts added to the model the ability to conduct consecutive operations of various types of supply cargoes, as well as unscheduled interruption of operations due to weather window termination, transition of the vessel to another terminal, or departure of the vessel beyond the three-mile border zone of the platform. Also, the model included an algorithm for a local decrease of oil production in case of a risk of a full filling of oil storage.  The following data served as input parameters: planned cargo flows of crude oil and supplies for the period until 2038, tactical voyage plan of vessel operations generated using the optimization algorithm, and average duration of ship operations. Results The state research center experts analyzed eleven improving measures to increase the efficiency of the marine transport system of the platform. The list of measures included: putting into operation an additional shuttle tanker, increasing the speed of oil offloading, using an additional icebreaker, and other methods. The practical aim of the analysis was to increase the efficiency of the transport system in terms of the ratio of the cost and the achieved reduction in volumes of underproduced oil. The volume of underproduced oil for the period from 2016 to 2038 served as the main efficiency criterion. This volume was calculated based on the number and duration of cases of reduction in volumes of oil production, which occur when storages are fully loaded and tankers cannot offload oil fast enough. The obtained data helped the experts discover that the construction of a terminal or an additional oil storage shows an absolute effect, i.e. the absence of underproduced oil. However, the practical implementation of these measures proved to be costly and technologically complicated, which is why they were excluded from further consideration. The simulation modeling allowed the experts to reveal that the expansion of accessibility of oil terminals has the highest positive effect on system efficiency of all the other measures. This can be achieved by implementing a range of technical measures, and by using the short weather windows for tanker cargo operations that allows for increasing the total duration of weather windows by 10-15%. However, the crucial point for system efficiency was not the total duration of accessibility periods, but the availability of a tanker to approach the platform at a desired moment of time. The model enabled the experts to discover that putting an additional shuttle tanker into operation has no significant impact on system efficiency, because the bottleneck of the system is oil offloading under weather change, not the shortage of tankers. The results of the study collected with the AnyLogic model helped the experts determine technical and operational characteristics for each improving measure and evaluate statistical distribution laws of all key parameters. The obtained data formed a basis for making managerial decisions at the top level of the Gazprom Neft Shelf Company. Another project carried out by the state research center experts for Gazprom Neft, with the help of simulation modeling, was the design of a maritime oil transportation system for the harsh ice environment.     Modeling of Banca d'Italia Back Office System Link:  Tags: Business Processes  Banca d'Italia processes a certain amount of manual credit transfers every year. These transfers cannot be processed automatically and require two divisions of employees in the back office of the bank. The bank wanted to determine if merging these two divisions would be beneficial. Problem The back office operations included all of the checking and settlement activities (see the figure) related to the trading operations performed by the corresponding front office. Two divisions, or units, were involved in the execution of domestic credit transfers. Employees in the units worked in shifts from 7:30 a.m. to 7:00 p.m. There were three categories of employees in each division (assistants, coadjutants, and officers) and each category performed different tasks. Officers in unit A authorized, by signature, the transmission of the payments registered within the IT system to unit B so that they could begin working on them. An assistant in unit A could not fulfill the tasks of an assistant in unit B. Unit B started its operations only when unit A had completed its tasks for each single process. Therefore, the tasks of the two operating units were different and sequential. The domestic credit transfer process was not the only task performed by these units, but it was the most important one, as every payment had to be carried out within a time limit (5:30 p.m.). If this time was exceeded, clients would apply to the bank for the payment of penalties, which is why this process was a top priority for both units.  The goals of the simulation modeling project, carried out by Fair Dynamics Consulting, were:  To verify the effect of employees’ absences (due to holidays, training, sickness, etc.) on the process completion time. To verify the effect of stress situations (high number of payments to be processed, high percentage of completely manual payments, high presence of priority payments to be processed) on the process completion time and on the employee utilization ratio. To investigate possible advantages of organizational variation (a merger of the two units) or a change in the authorization and control process.  Solution The consultants employed AnyLogic’s unique capability to use different modeling methods and created two models of the system, one in Discrete Event and the other in Agent Based, to ensure the output. The numerical results were the same. Traditionally, such systems are often simulated with the Discrete Event method. In this case, the Agent Based model was easier to use and quicker to build. Several experiments with the system were carried out:  Normal activity: units’ time performance in a day with a standard volume of payments. Absence of employees: units’ time performances in a day with a standard volume of payments and with the absence of one critical resource per shift. Abnormal activity: time performances in a day with a 300% increase in the volume of payments.  The efficiency of the “as is” (current situation) and “to be” (possible merger of the two organizational units) scenarios is compared on the graphs (see the graphs). The graphs illustrate what would happen in a normal working day for both scenarios.  The advantage of merging the two divisions was clearly shown in the simulation for an absence of employees. In the current scenario, the units were not able to meet the deadlines for processing all the payments if an employee was absent. The merger would solve this problem.    Outcome The simulation showed that the merger of the two current operating divisions into one would be very beneficial. The merger would produce the following advantages:   An evident increase in the productivity of the whole process, by increasing the effectiveness of the process and freeing part of the employees’ working time, possibly for use in other processes. A sensible reduction of operational risks, by improving the volume/completion time trade-off and carrying out more payments, within the completion time limit, with the same number of employees. A reasonable reduction of the employee stress threshold, especially on abnormal activity days.  Watch Luigi Geppert from Fair Dynamics of Italy presenting this project at the AnyLogic Conference 2012 or download his presentation or a paper based on this case.       Modeling of Municipal Electric Vehicle Fleets Link:  Tags: Transportation, Road Traffic    Climate change and environmental impact are hot topics in Germany. In 2019, EU parliament issued a directive that stated by 2025, 45% of public buses should be low- or zero-emission. This ratio was expected to grow further over the next five to ten years.       In light of this, SimPlan, a simulation and optimization service provider based in Germany, ran a project with the city of Hanau and Frankfurt University of Applied Science. The goal was to model and develop a digital twin of municipal electric vehicle (EV) fleet operations.   Electric vehicle modeling problem   For the project, SimPlan considered two vehicle types: public buses (HSB - Hanauer Straßenbahn) and garbage collection trucks (HIS - Hanau Infrastruktur

 

Section 32 of 60
Service). For the vehicle types, they needed to assess three areas:    The fleet mix of electric vehicles powered by lead-acid batteries (BEV) and EVs powered by hydrogen (FCEV). The optimal charging and refueling infrastructure configuration and how much power it would require. The operational consequences of transport schedules and routes.       Additionally, SimPlan encountered two challenges that they also needed to consider for electric vehicle modeling.     Firstly, the electric vehicle fleet has travel range limitations compared to conventional diesel-fueled vehicles. A conventional bus has a range of 500 km, while a BEV and FCEV will only be able to cover 300-350 km.    The problem is that this EV travel range is less than the public transport’s current tour distance. Consequently, vehicle schedules and routes would need to be adjusted for the new EV-based fleet mix.   Secondly, the charging process is non-linear. How quickly an electric vehicle charges depends on its state of charge, the amount of power available at the charging stations, and the number of vehicles charging at the same time.    Solution   To build an EV operations simulation model, SimPlan first needed to gather data on the vehicles (their type, travel range, consumption), schedules, locations of bus stops and garbage containers, costs, and so on. The data was imported from Excel into AnyLogic.    In AnyLogic, they used an agent-based modeling approach to build two electric vehicle operations models. In the models, agents represented vehicles, facilities, transport management, and charging management. SimPlan described the charging process with statecharts and used Java code to develop heuristics for location mapping, vehicle scheduling, the arc-routing problem, and tour assignment.    Modeling a garbage collection route  During modeling, SimPlan discovered four additional routing restrictions that had to be considered to accurately capture the systems’ behavior:    The direction of a street (one- or two-way). The side from which a truck should collect garbage (left or right side of a street). Avoiding U-turns. Large vehicles should avoid them in order not to block traffic. Hardly accessible garbage containers. SimPlan developed algorithms to determine the closest accessible road to such a container so that a truck can collect garbage.  For the electric vehicle operations models, SimPlan also created a custom user interface so that users can  easily configure them and run scenarios.        Modeling electric vehicle operations (bus)   One of the models that SimPlan built for this project was an electric bus operations model. On the settings screen a user can choose vehicle types (diesel, BEV, FCEV), select the number of each type in a fleet, and set the number of charging stations in the depot, and their maximum power. The user can also select an option to consider the temperature effect on vehicle energy consumption.    Electric bus operations model (interface of the model and map view)  On the map view of the model, the user can see different types of buses moving on the streets of Hanau, their states of charge, energy types, stops, and the charging station (the depot).        Modeling electric vehicle operations (truck)   The second model SimPlan built was a garbage collection simulation. Because the settings are similar to the previous model, we will only highlight the differences in the map view.   Garbage collection simulation (interface of the model and map view)       On the model screen, a user can see not only garbage trucks moving on the streets of Hanau, their states of charge, energy types, and the charging station (the depot), but also containers for specific garbage types. The user can also choose the preferred weekdays and the number of a week and observe changes in trucks’ schedules. If a truck reaches its maximum load, it drives to the disposal facility, unloads the picked-up garbage, and resumes the tour.    Statistics   While a model is running, AnyLogic’s visualization capabilities provide dynamic insight and track KPIs – showing the state of charge, tour duration, tour distance, and assignments per vehicle.        After running the models, they exported the data to an external database, evaluated the results, and compared scenarios.       Electric vehicle modeling results       In this project, SimPlan specialists built models of municipal electric vehicle fleet operations. In the models, they considered two vehicle types: buses and garbage trucks (both conventional, battery- and fuel-cell-powered) and modeled them as agents. To accurately describe the systems’ behavior, they used AnyLogic statecharts and Java for heuristics development.    The company used simulation modeling because it can accurately forecast large-scale EV fleet tasks and assess their daily operation.   Modeling results: number of electric vehicles.   After running simulation scenarios and processing the results, SimPlan analyzed a report on a number of required vehicles of each type. It showed that the cold season required more zero- and low-emission vehicles compared to warmer months, and more zero- and low-emission vehicles than conventional vehicles in general.   Modeling results: electric vehicle energy consumption.       Additionally, they gained insight into potential power needs for the charging infrastructure. With these statistics, they could analyze what would happen to the networks’ performance if they had a limited number of charging stations or limits on power in general. Furthermore, they could evaluate how that would impact vehicle schedules.        In the future, SimPlan plans to investigate opportunity charging instead of overnight charging and whether on-demand bus tours (without a certain schedule) can fulfill real-time customer requirements. For day-to-day operations, the company plans to develop the simulation model into a digital twin.    The project and its findings were presented by Dr. Nadia Galaske of SimPlan at the AnyLogic Conference 2021 – Agent-based model to design and support e-mobility transformation for municipal vehicle fleets.       Modeling of a Pharmaceutical Product Launch  Link:  Tags: Healthcare, Marketing            One of the huge pharmaceutical companies employed Bayser Consulting for development of product launch strategy. Simulation modeling was applied for reconstruction of interactions between the company, doctors and patients.   Objective            Identify the optimal promotional strategy for an upcoming product launch and the corresponding sales. In particular, what is the ideal breakout between Direct-to-consumer advertising (DTC) and Personal Selling? Address specific questions such as:    Does the client need a Contract Sales Organization (CSO) and if so, how large and for how long?  How should the client reshuffle the current field sales plan to best free up primary position details?   Solution           The model included:     Probabilistic generation of medical prescriptions as the result of the meeting of the physician (exposed to reps and Key opinion leaders (KOL)) and the patient (exposed to DTC and co-pay assistance), and the transformation of the prescription into drug consumption and subsequent refills.  Animated visual display (using AnyLogic) of the interactions between patients, physicians, and representatives, the outcome of those interactions, and the resulting sales. This is an excellent way to grasp the essence of the problem and generate penetrating questions.  An "Analysis of Misses" feature could speed up the search for optimal allocation by suggesting how the current promotional strategy needed to be altered to maximize sales.   Figure 1. Screenshot of the Agent Based Model    Why Agent-based Modeling?   The modelers needed to simulate the patient, the physician, and the sales representative as distinct entities since they were interested in their interactions and how promotional exposure altered their behaviors over time.  The modelers wanted to study group dynamics and, as such, needed to differentiate between members of the group. To that end, personality traits needed to be assigned to individuals following, say, a Gaussian distribution.  The modelers needed to simulate various promotional strategies that concurrently affected sales at different levels. In particular, they needed to model the lifecycle of the drug starting from the patient falling sick to the consumption of the prescribed drug.  The modelers needed to peer into the workings of the group practice: how it embraced the drug, what bottlenecks impeded adoption, and what corrective measures could be taken to maximize drug uptake.          It is the need to take into account these factors that determined the consultants’ choice of AnyLogic, which is powerful at Agent Based Modeling.    Figure 2. Types of interactions between Agents  Outcome           The results of the project were:     Identification of the optimal promotional strategy and assessment of various candidate strategies.   Recommendations regarding the CSO and how to reshuffle the product portfolio accordingly.   Uptake curve of the drug and corresponding sales forecast.   Understanding of the various decision points, and how they interact with each other towards overall sales.   Insightful questions that the analysts would not have thought of in the absence of this model.  Note on Emergent Behavior            The emergent behavior of a group is the behavior a group exhibits because individuals make different choices than what they would, if they were not a part of the group. What's more, the choices of an individual impact choices other members of the group make.    Figure 3. Emergent behavior of ants           The foraging path the ant colony ends up taking with depends on the initial decisions individual ants make. Likewise, the behavior of the group practice depends on and evolves with the decisions of individual physicians.   Conclusion            The Agent-Based Modeling module of AnyLogic is the platform of choice as:    it allowed the modelers to answer all the questions they posed,  it provided a close-up view of the dynamics of the group practice, and  it was ideal for studying emergent behavior.           The model can be scaled up so the allocation question can be broadened to include payer rebates for improving access, in addition to DTC and Personal Selling.      Modeling the Cladding Leak Detection Shop of a Nuclear Reactor's Module  Link:  Tags: Manufacturing  Customer Rosatom State Atomic Energy Corporation (Rosatom) is a state holding which incorporates more than 360 enterprises in the nuclear field. It includes all nondefense nuclear ventures, enterprises from the nuclear weapon sector, research organizations, and nuclear icebreaker fleets. Rosatom is a leading organization in the nuclear industry. It holds second position in world uranium stocks, fifth in mining, and fourth position in world nuclear energy production. It controls 17% of the world nuclear fuel market and 40% of the world market of enrichment services.  Model developers: Yuriy Podvalny, Denis Gerasimov. Problem When designing the fuel cladding leak detection shop, developers needed to collect the data and system parameters in cases of ruptured fuel elements production.  The cladding leak detection shop is part of an automated line of fuel assembly production. Leak control is based on heating the fuel element groups. While warming up, defective units eject the control gas, which is detected by a leak locator. The defected group is divided into two parts. Each part is screened in the same way until the leaking fuel element is found.  System engineers needed to define the dependence between annual output (production), input storage volume, and fuel element group size, to charge into furnace or different spoilage rate. In addition, they needed to define the amount of dead fuel elements assemblies due to system downtime when input storage is full.  Solution Since leakage rate has a stochastic nature, developers built a simulation model of a cladding leak detection shop and tested different scenarios and system operation algorithms, running multiple experiments on the model.     Simulation Model Screenshot  The model, built in AnyLogic, simulates two algorithms of leak detection shop operation. The algorithms are based on different approaches for the detection of a leaking fuel element when the fuel element group is screened and leakage is detected.  First algorithm: when leakage is detected, half of the fuel elements group is uncharged from furnace #1 to furnace #2. Both groups are heated and examined for leaking fuel elements. The qualified group is charged to the output storage. The defective group is again divided into two groups and examined for the leaking fuel element, and so on. If both groups appear to be defective, half of each group is charged to the output storage and the examination of the rest of the halves begins. The fuel elements groups in the output storage are inspected after examination of the first halves is completed. The input storage picks up incoming assemblies with scheduled frequency. Thus, the ruptured group is "uncoiled" by two furnaces. A new group of fuel elements is not charged until the ruptured element is found. In case leakage is not detected, the furnaces operate in course, with the second furnace waiting for the first one to complete the examination of the fuel elements group.  Second algorithm: the furnaces operate independently. The ruptured group is "uncoiled" by a furnace in isolation from another until the leaking element is found. A new group of fuel elements is not charged into the furnace until the previous group is completely examined. This approach allows two furnaces to operate simultaneously.  Model user can vary the following parameters of algorithms:   Leakage frequency Size of fuel elements group to charge into the furnace for examination  Input storage size  In the course of the research, every combination of value parameters was run on the model about 100 times for "one year" in terms of simulation model time.  Results  Production rate of operation algorithms was tested for different leakage frequency cases. The size of the fuel elements group to charge into a furnace was defined to provide the maximum production rate for the given leakage frequency. Simulation brought out the possible loss reduction due to down time if the volume of input storage is increased. Leak detector production stats were received for various leak frequencies and different parameters. Each time, the model was run for 100 "simulation" years.  Simulation spotted the dependence between maximum volume of input storage and the size of the fuel elements group to charge into the furnace. Numerical values were received to express this dependence for various leak frequency.   Conclusion At the stage of designing the fuel cladding leak detection

 

Section 33 of 60
shop, experiments with the real system required lots of financial and time expenses. Analysis of data collected during simulation allowed the engineers to define the optimal design parameters to provide maximum production.  The customer is planning to use the simulation model for testing possible changes in the production line, like adding a new furnace. Users can vary parameters by editing data in the model interface. The simulation model will serve as a decision support tool for the equipment buyer for many years.     Montreal International Airport: Simulation for Early Bag Storage System Implementation Link:  Tags: Passenger Terminals  Overview GSS Inc. is a Canadian engineering company that provides strategic and technical consulting services with a strong focus on simulation and optimization in the context of major infrastructure and transformation projects. GSS helps clients in different industries including healthcare, airports, transportation, logistics, and manufacturing. Their list of customers worldwide includes Johnson & Johnson, Bombardier, ABB, and Nikon, as well as Canadian government agencies. Montreal International Airport is the third busiest airport in Canada, and one of the most important airports in North America in terms of connections. At the start of the project, 34 airlines had direct flights to 148, mostly international, destinations. Due to the significant recent growth of passenger traffic, and being a GSS customer for more than twelve years, Montreal International Airport invited GSS to assist in the design of an Early Bag Storage (EBS) system. Later on, GSS and Montreal International Airport combined to work together on a Smart Baggage Handling System (BHS). To learn more about this, read the case study:  Creating a Smart Baggage Handling System Problem In the last five years, Montreal Airport’s passenger traffic has increased by 32% and is anticipated to continue growing at an average annual rate of over 4%. This has prompted the airport to launch many infrastructure projects to meet the anticipated growth in demand for support services. These support services include the baggage handling system (BHS), which is the focus of this study. The BHS consists of several stages from passenger to plane. First, passengers leave their baggage at check-in or drop-off points. Conveyors then take the bags to an explosive detective system (EDS), and from there they continue to an outfeed and await loading. When a plane is ready for loading, its baggage is transferred from the outfeed to the plane on small cars called unit load devices (ULD). Aircraft turnaround time and the overall capacity of the BHS at Montreal Airport was limited by the number of piers and carousels in the outfeeds. As such, fluctuations in demand meant that the BHS could not meet the demand of the airport’s departure schedule during busy periods. The schedule could not be altered due to external factors. Meeting increased demand and improving aircraft turnaround time could be solved by increasing the capacity of the BHS with more conveyors, piers, and carousels. This work, however, was not possible in the short term and the problem required an immediate remedy. Furthermore, airport management was planning to invest significantly in an early bag storage (EBS) system. The EBS would provide a centralized and automated storage system to hold passenger luggage until required. Airport management and GSS wanted to know if the EBS could be used as a buffer to smooth baggage flows and shorten the time piers were allocated to a flight. Doing so would improve aircraft turnaround time and shave peak demand. More efficient delivery of baggage to aircraft could reduce aircraft turnaround time and help accommodate more flights during peak times. Simulation modeling was selected to identify the optimal EBS flight list. It would consider how to:  Generate infrastructure savings. Minimize the impact of any changes on other operations. Maintain service levels. Optimize the EBS size.  Solution The simulation model helped develop a schedule for all the airport’s baggage handling, including:  Airport simulation and pedestrian simulation model   Which pier and how much pier time should be allocated for each flight. What baggage should be stored in the EBS depending on the departure schedule. The amount of baggage. The baggage-handling system operational constraints.  Since this was a multi-criteria optimization problem with thousands of agents and factors, the engineers decided to create a digital twin. This would enable the analysis of operations and the testing of ideas without disrupting the airport’s real baggage handling. To begin, GSS specialists brought together all the relevant airport departments, airlines, and other concerned parties to understand their needs and begin the process of developing a LEAN design solution. They decided to begin by implementing a manual EBS (where employees handle the bags) for the first two years while the automated infrastructure was built. There were three phases of model development:  Based on the analysis of the historical bag arrival profile per flight, a simulation model was developed to estimate the expected bag flow of the BHS. This model was also used to find the best combination of flight allocations and choose the candidate flights for peak-shaving. Different scenarios of flight lists were generated and then tested on the EBS to find the best configuration given the capacity and technical constraints. Since the automated solution would take around two years to build, a temporary manual EBS solution was implemented where airport staff would manage the bags and put them into the carts. The second modeling phase simulated this manual operation, including the logistics and operational constraints of storing bags in temporary carts instead of an automated infrastructure. This model allowed airport managers to define the best staffing allocation and to restrict the flight list if necessary. At the time of this writing, they are developing the simulation of the fully automated EBS and its integration with the existing automated BHS. This will produce a digital twin of the full baggage handling system.  GSS chose the AnyLogic platform for the simulation and multi-criteria optimization because of its performance and ease of use when developing complex models. AnyLogic provides engineers with many built-in libraries. For this project, both the Process modeling library and the Material handling library made it significantly easier to model all the conveyors, piers, and carousels. The flexibility of AnyLogic enabled high- and low-level decision support with one model as well as the integration of databases, Excel files, and external libraries – specifically, some scripts written in R that allowed the incorporation of the baggage arrival profile into the model. AnyLogic also made process automation possible and provided the speed for engineers to run many optimization “what-if” scenarios with multiple influencing factors and KPIs. Outcome Montreal International Airport was equipped with a decision-support simulation tool that allowed it, and the airport’s main stakeholders, to make better timely decisions and solutions.  Using the multi-phase approach, the airport managed to maintain its service levels throughout the transformation process and to meet the increased demand.  Importantly, the EBS operations, developed with the help of the model, helped the airport avoid numerous costs related to poor service quality. This was achieved by:  Eliminating costly operational contingency plans. Reducing safety risks. Better resource usage. Reducing supplementary working hours.  The project also produced cost savings by avoiding the need for an additional sorter with ten piers. Overall, these project-related savings have helped Montreal International Airport launch further infrastructure projects and continue the airport’s development and expansion.  Watch the video of Alvaro Gil, M.Sc. Strategic Consultant at GSS, presenting this case study at The AnyLogic Conference, or download his presentation.       Oil Pipeline Network Design: Finding Bottlenecks and Choosing the Right Policies Link:  Tags: Oil & Gas  Problem One of the largest oil and gas pipeline operators in North America was delivering oil to a client that was not always able to accept the incoming batches. The operator was challenged to quantify the system impacts of deferred downstream deliveries. They also needed to determine whether the existing tankage at upstream oil terminals would be adequate to store the deferred batches. To better understand the system behavior and impacts of the existing problems in the pipeline network design, the company authorized the development of a network simulation model. The project was carried out by the AnyLogic Company and Stream Systems Ltd. The business objectives of the project were:   To understand and evaluate the impact of customer decisions (for example, rejecting or accepting the product) on pipeline network behavior. To identify and remove bottlenecks in a multi-terminal oil and gas pipeline network to increase system throughput.  The consultants’ task was to create a model that would be flexible enough for the oil and gas pipeline operator to reframe the problem and experiment with the model to solve other challenges.    Solution    Pipeline Network Design — Simulation Model Interface  The simulation model included terminals, tanks, pipelines, and oil batches. Each batch was designated a route that included the start and end terminals. The route could go along different pipeline segments and terminals, and a batch could be temporarily stored in a transit terminal when going through a route. Oil type compatibility rules were taken into account when there were different types of products in the same pipeline or terminal. With probability set from the input data, a batch could change its route in one of the transit terminals. Batch movement speed could change because of the lack of free tank capacity in a downstream terminal or equipment breakdowns (reliability information was taken from the input data). The model output included:   Batch movement history.  Terminals’ and tanks’ used capacity.  A pipelines’ batch movement speed and throughput.  The model also included animation of the pipeline network on a map that visually highlighted the location of each batch and each pipeline’s segment status (normal/congested). Outcome The key performance metrics that were taken into account during model result analysis were: throughput and infrastructure capacity. The company was able to:  Discover system bottlenecks.  Evaluate mitigation options (experimenting with the pipeline network by changing rules and design, introducing additional infrastructure elements).  Assess and measure the effects of changed customer behavior on the network. Extend this model by adding additional terminals, pipelines, or tanks. Adjust the model to the changing conditions.  The analysis of the experiment results led to an organizational policy change that allowed them to achieve $50-$80M in capital savings, and created additional network throughput of $2M monthly revenue from that point forward. These results were achieved with approximately a $3M investment, which was a phenomenal outcome. First, the operators’ specialists suggested creating a simulation model at a low level of abstraction. They were familiar with simulating oil movement in pipelines at a molecular level. These models were useful for leak detection, fluid dynamics, and hydraulic problems, but were far too complex to simulate pipeline network behavior. To do this quickly and easily, and at a desired accuracy and reliability level, they needed to look at the system at a higher level (for example, simulate oil batches, not molecules). Stream Systems’ consultants convinced the oil and gas pipeline operator to use AnyLogic to complement this task. Its multimethod modeling features allowed the users to choose the desired abstraction level to solve the challenges of the business. The created model utilized AnyLogic’s multimethod capabilities to the full extent. The model was a combination of all three simulation paradigms: agent-based, system dynamics, and discrete event methods. This allowed the users to save time and effort reproducing the pipeline network design in the model.   Project presentation by Allan Chegus and Dumitru Cernelev from Stream Systems Ltd.:     Oil Well Modeling and Optimization Using AnyLogic Fluid Library Link:  Tags: Oil & Gas  Problem            Canada has the world’s third largest oil reserves. However, most of the oil is in oil sand — a mixture of sand, oil and water — which has to be heated up with steam to retrieve the oil. Though operational costs for extraction are high, capital investments in this sphere reached $26 billion in 2014, indicating a promising future.           To extract oil from sand, a complex system of oil wells, pipes, steam generators, and other equipment is needed. It is costly to maintain such a distribution system, and outages may lead to disruptions in steam injection and oil production. For oil well optimization, cutting expenditures and capturing production lags, Stream Systems used AnyLogic simulation modeling. Using spreadsheets, engineers could model the working process of 10-20 oil wells. Adopting a simulation approach allowed them to model production for hundreds of oil wells.   Solution           The oil production process consists of three major elements:     Central processing facility (CPF)   Reservoirs, which act as the source of the oil sand   Stand-alone wells and well pads (set of wells), extracting the oil   The simulation model consisted of smaller models of the system’s elements. This modular approach allowed engineers to see how the work of a particular operation might affect other elements.            Because of the model’s complexity, the AnyLogic multimethod approach (a mix of agent-based, system dynamic, and discrete event modeling) was applied. The system acted as a liquid carrier, and the lag in one component might lead to lags in other ones. The AnyLogic fluid library was used to capture these lags, including for emergency situations.            AnyLogic was seamlessly integrated with external data sources, allowing modelers to use any type of files to input the data into the model. To manage additional calculations, and make the model more realistic, external Java libraries were incorporated. The model’s input data included:     Operational data - infrastructure, layouts, configurations of system’s components, seasonality, etc.  Production profiles  Excel spreadsheets and text files          The model consisted of oil wells that acted as individual agents. Each well had a particular behavior and was connected with pipes and other flowchart components. It was easy to add and adjust components in the

 

Section 34 of 60
model if needed.            Apart from looking at specific parts of the model, it was also possible to observe the production process, at a high level, to make operational and strategic plans. At both levels, it was possible to set parameters and run various experiments to perform optimization. Dashboards showed statistical data to visualize changes in the system.            Output data accounted for:    Throughput of wells and pads Steam-oil ratio  Steam, water, and oil production rate  Number of pads and wells per pad  Product quality   Outcome            AnyLogic simulation modeling helped connect parts of the system, located above and below ground. With the multimethod simulation modeling approach, it became possible to run multiple scenarios, including Monte Carlo and what-if experiments, and analyze variabilities in terms of scheduling and maintenance, providing the system’s optimization.           The Fluid Library helped represent each part of the oil well optimization model with high granularity and display the ripple effect and breakages in the system. Moreover, it helped consider dynamic changes and their impact on the system. With this approach, it became possible to make real time decisions and capture the quality of the oil.            The model also contributed to decision making on future capital investments as well as their reduction: establishing when to replace or provide maintenance.     AnyLogic software has also been successfully incorporated in the company’s other projects.       Optimal Decision Making in Logistics Link:  Tags: Transportation, Rail Logistics  anyLogistix optimization software success story. Problem One of the largest beer manufacturers in Eastern Europe faced a logistical challenge most manufacturing company’s face, high transportation costs. The company’s plan was to decrease distribution and transportation costs from the plant to suppliers and ultimately lower costs for their customers. A large fleet of rail cars and trucks, its own warehouses in several regions of the country and a complex set of variables such as loading time, customs and vehicle break downs lead to a various number of decisions throughout the transportation process and produces many outcomes. Traditional forecasting could not provide the beer manufacturer the required insight to make the most strategic decisions. Integrating the corporate ERP system with AnyLogic Transport Operations Manager (now part of anyLogistix) allows the company to consider all possible outcomes and make the most profitable decision. The project goal was:    Optimize the Company’s own rail car and truck fleet usage and the usage of third-party carriers.  Create a mid-term planning tool (next 60 days).  Scale the solution for operational, short-term planning (next 10 days).   Solution  The model included in the decision support system simulated transportation of goods from the Company’s manufacturing plants, through multiple distribution channels to suppliers. The input data included:  Sales forecasts provided by the client.  Rail car fleet data: type of container (normal or hot food) and maintenance schedule. Current rail car location. Costs for own or third-party fleet usage.  The system considers the following:  Station capacity for loading/unloading cars.  Own and third-party fleet usage limitations due to tariffs and seasonality. Hot food container cars need to be heated from time to time on certain stations. Some clients can receive only deliveries by auto trucks.  Delivery times. Loading/unloading times. State border crossing times.  The goal of the optimization experiments included delivery of goods to clients and the minimization of transportation costs. Outcome The decision support system allows the Company’s logistics specialists to forecast transportation scenarios and analyze optimal variants. By letting the software assess risks, delivery times and costs, the Company is able to compare all possible outcomes. The Company’s logistics department is enabled to develop ten and sixty day forecasting plans which lower transportation costs and ultimately lower the costs of goods.     Optimal Longshore Labor Dispatch Link:  Tags: Ports & Terminals    The BCMEA (British Columbia Maritime Employers Association) is an organization created by maritime employers operating in the five port regions of British Columbia, Canada. The employers are terminal operators, stevedores, and shipping agents who depend on longshore workers for the loading and unloading of cargo and other dock activities. The association represents over seven thousand active longshore workers and is involved in worker training and jobs dispatch.   Partnering with SimWell and the Simon Fraser University Beedie School of Business, the BCMEA created a digital twin of the Vancouver longshore labor dispatch.    SimWell are prize winning specialists in industrial engineering simulation and optimization based in Canada and the United States of America. Their team of simulation modeling experts advised BCMEA on digital twin capabilities and helped implement an agent-based simulation using AnyLogic.   Two student groups from the Simon Fraser University Beedie School of Business worked on the project. One group used machine learning to train a model for labor and skill availability, and the second group created visualizations to help scenario comparison.   Problem: longshore labor shortages   The BCMEA is responsible for ensuring a reliable supply of trained, qualified longshore labor for all its ports with the aim of avoiding shortages. During 2016, the BCMEA unexpectedly began suffering record shortages and part of its response included the development of short, medium, and long-term plans to improve longshore worker training and availability.   Project origins, the short to long-term plans to address labor shortages. (click to enlarge)  As part of the plans, they wanted to simulate the labor dispatch process. This would make possible the analysis of skills-matching at dispatch and the determination of training needs.   The complexities of longshore labor dispatch   For the BCMEA, dispatch happens three time a day and involves matching required jobs to available employees. Matches depend on various parameters:    Ratings — indicate an employee is trained with a skill and can perform a specific job Capabilities — employee skills typically relating to specific types of cargo or machinery Restrictions — indicate an employee cannot perform specific jobs or at specific sites Boards — a method for fairly distributing jobs among available workers Union and Casual — employees are either unionized or casual and the dispatch processes differ  The workforce can choose where to go and who to work for, meaning labor availability is unpredictable. The exceptions are for callbacks, where employees are called back for a job position they have been working, and for those who are directly employed.   Not knowing which workers will be available from one day to the next means that shortages can occur at any time and that jobs requiring certain skills can go unfulfilled. The answer is to have a deep pool of trained and qualified labor to draw from. This in turn leads to an ongoing challenge: how much training is needed?   Until this project, the amount of training was determined by gut feeling – from the estimations of people in conversation with employers. That way of working had been satisfactory but was now less effective at meeting the shortage KPIs, since jobs were requiring more skills and more training.   Solution   The BCMEA’s medium- and long-term plans, which were created in response to 2016’s unexpected shortages, lead to the creation of a data analytics program.    To begin, the BCMEA created datasets and dashboards to understand current and historical states for longshore labor dispatch in British Columbia. The result of this initial work gave them the capability to recognize trends and better inform planning.   Following on from the datasets and dashboards, the aim was to develop predictive analytics capabilities. These would enable the BCMEA to look into the future and analyze scenarios, such as the opening of a new container terminal or the effect of training more truck drivers. For the decision makers, it might be easy to predict that more training will improve labor availability, but the aim for the predictive analytics was to quantify the labor and training needs.   Model schematic and user interface example.(click to enlarge)  SimWell advised and helped build a digital twin of Vancouver longshore labor dispatch as the basis for developing predictive analytics. They implemented an agent-based simulation model using AnyLogic that worked with the necessary metrics and represented the behavior of the real-world system.   Feeding into the digital twin, thanks to its easy extensibility, was a separate machine learning (ML) model of labor availability. The availability model was developed by Simon Fraser University Beedie School of Business, who also provided visualizations for better understanding and communication.   The developers separated the availability model from the rest of the model so that it could be iterated independently of the simulation model. The ML model is external and connects to the AnyLogic simulation model via web API.   The reason the availability model uses ML to capture labor variabilities is because the method proved more accurate over time than alternatives such as heuristics. The patterns in labor variabilities that are present in historical data are difficult to capture explicitly but ML provides a way to work with them.   A key design feature was the separation of the data parameters from the process configurations. This distinction made possible data driven scenario creation where historical data, such as employment records, can be modified in various ways to test hypotheses. For example, analysts can investigate the impact that different training programs might have for different scenarios.   At present, the digital twin can simulate a year of Vancouver’s longshore labor dispatch in six minutes. Simulations can run with a user interface or in a headless mode called from a Python script. Every action in a simulation is logged and available for further analysis outside the platform using Python scripts and Tableau data visualization.   Results  The longshore dispatch tool’s visualuser interface. (click to enlarge)  The digital twin is useful in many ways. Initial results include analysis of the bulk operator pool, a complex rating that requires a large training effort. Using the digital twin tool to analyze training needs determined an optimal three-year return on investment for training.   In another case, the model was used to examine rubber-tired gantry (RTG/RMG) labor needs in a port terminal expansion scenario. The results indicated the port would have a 300% increase in RTG/RMG labor needs.   Overall, the Vancouver longshore labor dispatch digital twin will support long-term strategies and predict trends in demographics and technologies, and enable business based on scenario modeling. The digital twin will also be extended to other regions.   This case study was presented [PDF] at the AnyLogic Conference 2021 by Bart Fransen, Senior Data Scientist at BCMEA.      Optimization and Simulation of Coal Mining Operations Link:  Tags: Mining     Problem: Mining productivity improvement    Vale is a Brazilian multinational corporation engaged in metals and mining. It is also one of the largest logistics operators in Brazil.        The company wanted to maximize the productivity of its open-pit coal mines in Moatize, Mozambique. To test different scenarios without disrupting the operations, Vale opted for simulation modeling and contracted with Genoa, a decision support company specializing in optimization and simulation.        Solution: Optimizing and simulating mining operations   Engineers from Genoa built a model of two open-pit coal mines. Overall, the model considered:      8 concurring mining fronts operating simultaneously  One loader per front  80 trucks with various capacity, speed, and failure profiles  10 different coal types (plys)  One ROM (run-of-mine) pad, one crusher, and one tailing silo per mine.   Coal mining process diagram (click to enlarge)  Each mine has several mining fronts from which various coal types are retrieved. Retrieved coal is moved by trucks either to the crusher or the ROM pad next to it. Along with coal, mining extraction machines also retrieve waste that has to be transferred by truck to waste deposits.    Extracted coal passes through a crusher and then goes to plants where it is processed. The plants produce three types of products: thermal (steam) coal, metallurgical (met, coking) coal, and a subproduct called tailings. The tailings are moved to a silo by a conveyer and then to the tailings dam by a truck.        In the mining model, the engineers also considered the shift times of staff working on the site, as well as equipment failure and maintenance.    Why AnyLogic for mining optimization?      During the first stage of the project, Genoa built an optimization model with an external solver to maximize the transportation capacity in the mines. Then they used the optimization model’s outputs as inputs for the mining operations simulation model.       In AnyLogic, the engineers visualized the results provided by the optimizer using charts, diagrams, and timelines. In addition, the software also allowed the analysis of other key open-pit management metrics, including:     Uncertainties like equipment failure (trucks, loading equipment, and crusher) Cycle times of trucks: travel time, queue before loading, loading time, queue before unloading and unloading time  Utilization levels for the trucks, loading equipment, and crusher  Crusher input rate (mine throughput)  Operator shift and maintenance planning.   Coal mining simulation and optimization model (click to enlarge)  Thanks to AnyLogic’s capability to import external Java libraries, the team could import and use the data from their optimizer directly inside the simulation model.     Using the optimization results data, the engineers built a simulation of coal mining operations that showed how the mine’s processes could be improved. The team used discrete-event modeling as the main approach for modeling equipment and machinery movements. To simulate truck behavior in detail, they also used agent-based modeling and statecharts.    Result        Genoa built a simulation model of coal mining operations in Moatize, Mozambique. Using an external optimizer, they determined the best truck fleet size and maximized the amount of coal that could be transported, while also considering the transportation of waste and tailing constraints.        Vale can now use the simulation model to assess different mining operations scenarios and visualize and compare results.

 

Section 35 of 60
They can also analyze the mine’s performance with different truck and equipment fleets and simulate shift and maintenance policies.       For further model improvement, the team considered modeling mines as a population in AnyLogic, which would help scale the model for other scenarios. Finally, to simulate the movement of the trucks in detail, they could use AnyLogic Road Library instead of schematically drawing the routes.    Genoa presented this case study at the AnyLogic Conference 2021.       Optimization of Utility Companies’ Mutual Assistance Using Agent-Based Modeling Link:  Tags: Business Processes  Problem When people are impacted by a natural or man-made disaster, utility companies look for ways to provide resources as soon as possible, and reduce outage time. To assist and better coordinate with each other, Canadian companies from closely located territories created alignments. So, if a disaster happens, and a local utility company does not have enough resources, another company from a closely located region can come to assist. However, taking expenses and distances to cover into account, the responding company might question the assistance possibility, whereas the requesting company needs to find assistance shortly. Engineers from York University applied simulation modeling to provide the utility companies with a better decision-making tool for managing the process of mutual assistance. Solution  To identify the criteria that contribute to decision making, industry specialists were interviewed. Overall, 13 criteria were chosen and then grouped into 3 categories:   Mutual aid requesting criteria - including distance to emergency site, extent of damage, etc.   Mutual aid responding criteria - including emergency conditions in own region, availability of resources, etc.   Disaster criteria - including size of disaster, disaster type, etc.           Criteria were assigned with numerical values and weights, showing the importance of a parameter in a particular situation.           An agent-based simulation model was designed to test various mutual aid scenarios. The model’s interface allowed users to choose agents, acting as requesting and responding companies, which would then be marked in GIS space. It was also possible to set weight and value for each criterion.   This state chart of a utility company shows the decision-making process when a responding company gets a call from a requesting company. To decide whether the help could be provided, the model's algorithm calculates the score, based on preset values and weights of criteria.  When the decision is made, the crews of the responding companies start moving to the place of emergency. While in route, the crews may be distributed among several places of emergency. At the same time, it is possible to see the following outputs:   Number of utility companies available, as well as ones that agreed to assist   Number of crews deployed and arrived   Crews’ distances between departure and arrival points   Time it will take for each utility crew to cover the distance  Outcome                    AnyLogic simulation modeling helped develop a tool for better planning, according to which mutual assistance might be recommended. Simulation modeling made it possible to optimize the mutual assistance process by running various experiments and avoiding mistakes in the decision-making process. GIS capabilities allowed users to visualize the routing of utility companies’ crews and redirect them if needed.       Optimizing Airport Processes and Designing Transportation Facilities with Simulation Link:  Tags: Passenger Terminals  Overview TranSystems is an architect and engineering company with over 25 years of modeling experience in the transportation industry. The company works on projects related to railroads, airports, seaports, roadways, transit supply chains, industrial facilities automation, and even quick service type operations in restaurants and hospital facilities. Through their work, TranSystems has developed a variety of specific digital tools that help in many of the projects the company undertakes. But sometimes, especially in cases when multimodal operations or transportation systems modeling is required, the standard transportation digital tools are not applicable. For airport simulation, transportation planning, construction optimization, pedestrian simulation, and other complicated projects, TranSystems applies AnyLogic software. Designing large transportation facilities, such as airports, ports, or railroad corridors, demands a lot of planning in multiple phases. Such projects are extensive, long-term, and usually involve a lot of stakeholders who are interested in the best value-for-money option, and it is important to get the support and cooperation of all involved parties. In order to address all these requirements, it is necessary to apply a multimodal system that can integrate all necessary elements, such as trains, vehicles, buses, people, etc. In architecture, any issue should be presented and agreed upon, which demands a lot of diverse situation modeling with tight deadlines. In such cases, AnyLogic modeling is especially advantageous, as it allows for creating dynamic construction simulation models that can provide useful insights about a system in a relatively short period of time. At the same time, it also provides data visualization tools that allow for easy communication and explanation of findings. Case study 1 Consolidated rental car facilities: Sizing the deck and planning in the early stages Consolidated rental car facilities are giant structures that consolidate all the rental car operations of multiple agencies scattered around airports. These facilities are becoming more and more popular at large airports. TranSystems cooperated in the development of such facilities at the airports of Chicago O’Hare, Los Angeles International Airport, San Antonio, and Phoenix. Consolidated rental car facilities are not just parking structures – they are complex and dynamic with different brand operations and peak demands. The bigger brands utilize their cars across their own sub-brands to keep their fleets fluid and get the maximum return from the cars they own. Consolidated rental car facilities also have integrated facilities for fueling, washing, and repair. Each airport has its own unique demands depending on when it is most busy. The rental facilities have to consider all these factors in order to plan how much space and how many cars they need. Problem: Sizing the deck Rental facilities need to plan for all big events, and also for seasonal cases like summer activities. For rental facilities, big events like college football games can be disruptive because they need a massive number of cars for a very short period of time. It is the work of consultants to make sure that a facility can meet these peak demands without too much excess. As part of their operations, consolidated rental car facilities also have to provide customer access to terminal buses and/or vertical access between levels. All these requirements need to be addressed to make sure that an airport maintains its standards on how long it takes for a customer to get a car. A lot of consolidated rental car facilities are four stories high and this can be a problem for rental car agencies. To maintain a productive operation, they have to avoid situations where their personnel drive unnecessarily. Such situations result in a waste of resources. The rental car agencies were interested in determining how much space would be enough for them to optimally operate within a building. There was a need to strike a balance between a good mix of cars for customers to choose from and space to deal with the return surges. In that regard, it was important to determine a deck that could support peak demands. Planning facility in the early stages Los Angeles International Airport was planning a very large consolidated rental car facility with four decks. Having considered all the options for a new facility in Los Angeles International Airport, the consultants concluded that the new facility should be served by connectivity that could deliver customers from terminals to the facility’s upper deck. Despite this, the facility was to open before the connection, so it was necessary to support a bus operation on the lower level in the interim. There were fears that during peak hours there might not be enough space for buses. It was necessary to understand the operation of the facility in various specific scenarios. It was also necessary to determine which vertical access option was best for the project. Solution The developers used AnyLogic modeling and customer arrival patterns to define optimal decking. The model had to reflect such elements like staffing and equipment, and provide time-based outputs. It needed to be capable of measuring the utilization of staff and facility resources, as well as certain other metrics. The resulting data would allow the stakeholders to determine whether such operations were feasible for them and whether to agree on the project. On the input screen, developers could set parameters such as the number of parking places each agency allocated for returns and on-site storage. The design planning model allowed for testing out how much space should be allocated for each rental car agency and indicated how that deck design was going to operate. This model provided animated visualization of facility occupancy during the day. The visualization helped the agencies understand what their typical day would be like and how much work would be required to maintain their operations on these decks.  Los Angeles airport shuttle bus pedestrian model to facilitate early stage facility planning   The model also provided outputs in graphical form: vehicle quantity in on-site area by hours, fueling station utilization, and wash bay utilization. This data helped the agencies determine if they were going to have enough space, cars, and units of equipment to maintain the required turnaround of the cars. The consultants used the AnyLogic Pedestrian Library to create an animated crowd simulation model which included incoming buses to determine if there would be enough space. This model also showed how pedestrians use elevators to move between floors. The early iterations of this pedestrian simulation model simulated escalators instead of elevators and showed good results, but the cost of implementing escalators was very high. It also turned out that this option was unsafe should there be a fire. Therefore, the interested agreed on elevators. The pedestrian simulation model helped to collect the necessary statistics on the load of elevators and, as a result, it was possible to substantiate the idea of using elevators. Case study 2 Operations study to add a new plane arrival at La Guardia southwest terminal LaGuardia airport planned to add a new flight to the schedule of the southwest terminal. The airport administration wanted to understand how the introduction of a new flight would influence terminal capacity. Problem In order to understand the scale of the problem, the developers conducted a preliminary static pedestrian flow analysis based on data of how long before the flight passengers arrived at the airport. In the picture, the solid line represented the number of seats in the waiting area, the red stacks represented the number of passengers in the terminal before introducing the new flight, and additional passengers from the new flight were represented by purple areas. The graph showed that if the new plane took off in the afternoon at 5:00 pm, the already crowded waiting area would have to bear an additional burden that could lead to a significant problem. The developers used the AnyLogic Pedestrian Library to create a crowd simulation model of the terminal in order to examine the use of seats under different scenarios. The basic model displayed the operation of all terminal areas before the introduction of the new flight, and then various assumptions could be checked against this model. The best situation was when people were waiting for departure at their gates, but the consultants wanted to check how far they would have to move away from their gates to wait for their departure.  Southwest La Guardia airport gate area layout in the crowd simulation model   To set up the crowd simulation model, the developers used tables of passenger preference for waiting areas. The model showed how far from their gate people would have to wait. The results of modeling the base scenario, without the new flight, showed that some of the peaks were reduced compared with the static analysis. This was due to passengers lining up 30 minutes before their flights. The model also showed where the people would actually wait. From this, it could be verified that there was no overflow and that the situation was stable. In the afternoon, the waiting area was a lot more heavily utilized. There were a lot of passengers mixing in different areas and waiting for different gates. With the new flight at this peak time, some of these areas would get extremely overloaded. This pedestrian simulation was very useful in showing the operations of this terminal and how adding the new flight would affect the passengers in this area, including how far they would have to move to wait for their flights. Solution Designing large transport facilities requires careful consideration and agreement on every detail. That means that such projects must go through a great deal of decision making. The initial task of engineers usually produces alternatives and functional designs. These consider physical requirements and standards, but whether business or operating objectives will be met can be hard to determine accurately.  It is here that AnyLogic based modeling helps by enabling faster decision-making and significantly improving insight into the various tasks that engineers face when planning large transport facilities. Watch the video of Beth C. Kulick presenting this case study at The AnyLogic Conference, or download her presentation.       Optimizing Cargo Transportation in France Link:  Tags: Transportation, Rail Logistics  Problem This project was carried out by The AnyLogic Company for PREDIT, French programme of research, experimentation and innovation in land transport. The operator of the railway network of the country wanted to know if rail cargo transportation could compete with auto truck transportation. They wanted to make truck-rail-truck transportation more effective. The specific challenges of the project included:  Determine how many rail cargo terminals were to be built for the optimal functioning of the system. Their characteristics also had to be defined, including location, utilization, and parameters of shipments going through these terminals.  Determine how the train management was to be organized. This included the train

 

Section 36 of 60
departure timetable (trains could only depart in certain time slots), and the size of the trains (the trains had to be short enough for stations and terminals to be able to handle them, but long enough to be cost- effective).   Solution  The decision was to create an Agent-Based model. Terminals, stations, and even some particular segments of rail network were modeled as agents, while shipments, trains and railcars were the elements passively managed by the agents. This approach was used because main decisions concerning the shipments were made at the stations.   In order to compare the two transportation methods, it was decided to double each shipment and send it by combined truck-rail-truck route and by truck only at the same time. The best price and time of shipment was selected by a virtual expeditor and reflected in a database.  Outcome  The research indicated that with the current structure of the transportation network and the prices, using only trucks was cheaper and faster than the combined truck-rail-truck system. The reason was mainly because of the size and the structure of the network.    The AnyLogic Company provided the customer with a flexible tool for optimizing the existing railway network structure. The user of this tool would be able to locate terminals, change costs, size of trains, etc. to see how the system would react. With this tool, the customer would get the opportunity to see which conditions had to change so that they could achieve their goals of making the rail cargo transportation more efficient.       Optimizing E-Commerce Warehouse Operations Link:  Tags: Warehouse Operations    The global e-commerce sector has seen 320% growth in five years, and demand is increasing following the COVID-19 pandemic. To reduce costs and remain competitive, DHL Supply Chain ran a warehouse operations optimization project. The project involved developing a smart and robust warehouse simulation tool for testing wave picking strategies. For a warehouse with 500,000 SKU (stock-keeping units) and 249 staff, the warehouse optimization project produced strategies that reduced the time required for order completion by 8.2% and the number of required staff by 66. DHL Supply Chain is a division of Deutsche Post DHL Group with a global network and extensive logistics portfolio, including warehousing, transport, and value-added services. Problem   DHL Supply Chain determined that two goals were essential to staying competitive in the growing e-commerce sector: meeting customer SLA (Service Level Agreements) and reducing operational costs.    To meet the goals, the organization chose to optimize its e-commerce operations, including the inbound activities of receive, stage, sort, and put away; and the outbound activities of pick, sort, pack, stage, and dispatch.   Jigar Panot, a specialist at DHL Supply Chain’s Global Solutions Design Center, worked on a solution for a large-scale warehouse:  Area: 111,000 sq. meters Products: ~500k Pick zones: 12 Daily volume: ~171k Staff: >3000  Warehouse layout (click to enlarge)  The project objectives were to develop a robust and smart system for testing different wave strategies and determine optimal warehouse throughput and resource utilization. Warehouse operations and pick path optimization Large warehouses use batch picking when collecting orders because individual order picking and cluster picking may involve an item picker spending time traveling several kilometers to fulfill an order. The aim is to find an optimal pick path.  Methods for picking order items in a warehouse (click to enlarge)  Large DHL warehouses use batch picking with wave release. Batches group orders together and waves group batches together for periodic release. Typically, a batch contains 14 orders. Waves help coordinate shop floor activities by time — allowing other operations, such as stocking and cleaning, to take place efficiently.  On release, batch items are divided for picking by zone so that the distance between items for a picker is minimized. After all the items from a zone are picked, they are combined into whole batches in a process called staging. Complete batches move to sortation where orders are brought together in put walls before being sent for packing and dispatch.  Warehouse picking operations diagram (click to enlarge)  Batch pick operations and put wall activities are grouped into several key processes:  Wave release — orders are grouped into batches and sent for picking. Picking — the collection of items from storage in warehouse zones. Picking in zones prevents long distances between items. Zoned picking can be for items in different orders and batches. Staging — all items in a batch are brought together. Batches are made up of several whole orders. When a batch is complete, it is sent to a put wall. Put Wall — orders are assembled from batches and sent for packing and dispatching.  Solution: Warehouse modeling and testing   Phase one of the solution involved modeling the warehouse processes as they were and back testing the model with real-world data for calibration. This ensured model accuracy and provided a baseline to compare against order picking strategy proposals. The modeling in this phase was done using AnyLogic’s built-in Process Modeling Library. The library is specially designed to simplify and speed up the accurate capture of business systems and workflows.  AnyLogic process model for a large-scale DHL Supply Chain e-commerce warehouse  After creating an accurate representation of the warehouse, the project began phase two to test different wave release strategies. The strategies were dynamic, based on metrics such as staging and put wall occupancy, the number of batches in the pipeline, and so on.  Phase two of the project had three stages:  Dynamic waving — where different wave strategies were created, and bottlenecks were identified. Scenario analysis — to understand how a wave release strategy affects completion time and the average queue size at staging. Comparative analysis — using KPI to compare the strategies, including the baseline.  During phase two, the engineers investigated the trade-offs between scenarios. These investigations helped them understand resource constraints and find an optimal balance of resources and speed. The result was a dynamic waving model that increased resource utilization and decreased cycle time when compared to the ‘as-is’ model of the then current warehouse operations setup. Results: Optimized warehouse throughput and resource utilization From phase one, with the as-is model, it was possible to see an opportunity to increase resource utilization and reduce task completion time because staging was not being used at, or close to, its maximum capacity. For staging, there were instances of idleness at some times and long queues at others. This can be seen in the Staging Statistics chart where the occupied time drops very low at some times and rises to a plateau at others.  Charts showing warehouse process throughput and cycle times for the ‘as-is’ model of the warehouse (click to enlarge)  Phase two of the warehouse optimization project produced a dynamic wave release model that optimized resource utilization and minimized cycle times. While put wall processing times remain roughly the same, the put wall utilization was increased by evening out the staging demand. The effect was to reduce overall order cycle time.  Results from the dynamic wave release model that show resource utilization and process cycle times (click to enlarge)  In comparison to the model of the original warehouse operations, the dynamic wave release model reduced order and batch cycle times, increased resource utilization rates by almost 10%, and delivered an overall completion time saving of 8.2%. The savings meant either 66 fewer staff were needed, or process completion could be two hours quicker.  Comparison of the baseline ‘as-is’ warehouse process model with the dynamic wave release model developed by the DHL Supply Chain engineers in AnyLogic simulation software (click to enlarge)  The e-commerce warehouse process optimization project showed DHL Supply Chain how to deliver significant savings for their large-scale warehouse operations. The supply chain engineers used simulation modeling to accurately capture warehouse operations that could be checked and verified using real-world data from operations. Confident in the model’s behavior, the engineers then designed and tested a dynamic wave release strategy to deliver the operational gains needed to compete in modern global e-commerce.   This case study is from a presentation given by Jigar Panot, Consultant at DHL Global Supply Chain’s Global Solutions Design Center, at the AnyLogic Conference 2021:   Learn more about using AnyLogic for warehouse operations modeling and optimization.      Optimizing Energy Systems with AnyLogic Simulation Modeling Link:  Tags: Asset Management, Social Processes  Preface   The European Institute for Energy Research (EIFER) is a joint research center created by Karlsruhe Institute of Technology and EDF. It works on the decentralization of energy systems in various territories and promotes renewable energy sources. Localized energy systems, unlike classical ones, do not need to transmit electricity over long distances, as they are located close to the load they serve.   Local energy grids are often more at liberty to accommodate power storage, renewable energy sources, and smart technology related to demand forecasting. EIFER engineers chose AnyLogic simulation modeling to determine how localized systems with various novel features could be implemented. The software allowed the design of multiscale models, representing all parts of the energy system, and allowed the investigation of the causes and effects of phenomena throughout a system. With AnyLogic, the institute also modeled the handling of emergency situations that might affect a decentralized energy system.   Case #1: Smart Grid Modeling on Island Systems – Demand Side Flexibility  Problem       Energy production on island territories is expensive and often depends on the cost of oil. As part of its work to reduce oil dependency and help cut costs, EIFER wanted to test how the inclusion of photovoltaic solar power would affect classical energy systems in such territories.   Solution   The current system comprised three grid levels based on voltage. The area with the lowest voltage (the consumer level) was chosen for the implementation of photovoltaic solar. After the inclusion of solar panels in the model, experiments were run to understand the effect they could have on the whole system and how to avoid blackouts or brownouts.   A scenario with the island partly covered by clouds was designed to test system stability. This lead the analysts to discover that under certain circumstances thermal power generation during times of cloud cover could cause system imbalances.   Outcome   AnyLogic modeling helped simulate the influence of photovoltaic solar on the island’s current energy system. Using cloud coverage scenarios, EIFER showed the impact meteorological conditions could have on the electricity system.   Case #2: Optimization for Local Energy System Management  Problem   EIFER researchers study not only new ways of energy generation but also existing ones, namely co-generation. This process involves the simultaneous generation of electricity and useful heating, cutting production costs overall. It is one of the most cost-efficient methods for reducing carbon emissions.   EIFER engineers used AnyLogic to model and optimize a co-generation energy system’s behavior and find out how its various energy sources interact.   Solution   An agent-based model represented the co-generation plant's electricity and heat production, households, and other tertiary sector consumers. The heat went to households, whereas electricity could go to households or to the grid. The model's output statistics showed how energy was distributed among the consumers and made it possible to analyze capital and operational expenditures.   Outcome   AnyLogic helped represent the energy system holistically, connecting different systems and entities in one model. Thanks to AnyLogic's extensibility, a modeling library was created to make it simpler to reuse custom blocks and agents in other models. And, for end-users, a user-friendly interface simplified the interaction with the model.   Statistics from simulation model runs allowed EIFER analysts to discover seasonality factors and how they affect the energy system, as well as observe related economic indicators.   Conclusion Energy systems are made up of multiple levels and are subject to numerous external factors. With AnyLogic simulation software, it is possible to model complex systems and uncover issues connected with variabilities and interconnections, demonstrating how alterations in one area can affect the whole system.    To learn more about how EIFER performs energy systems’ decentralization and applies renewable energy sources, read the following papers:     A Complex Systems Modelling Approach for Decentralized Simulation of Electrical Microgrids  An Agent-Based Multi-Scale Wind Generation Model     Optimizing Freight Transport Systems in New Zealand Link:  Tags: Transportation, Rail Logistics  Transportation, and in particular the freight sector, has a very high energy demand. Oil’s higher energy return on investment, containerization, expansion of road networks, and deregulation of the trucking industry have resulted in a sector which is heavily dependent on fossil fuels. This is a problem that requires a rapid transformation in order to combat greenhouse gas emissions.  Problem Existing freight modeling methods have many limitations:   Travel time and cost are prioritized over the environment.  Models are based on uncertain economic projections and technological developments.  Data for freight movement is scarce or unavailable.  Lack of a logistics component in freight transportation.   As a result, a researcher at The University of Canterbury had to generate a low-carbon design of a future freight system using a new set of tools. The problem was – what government investment now would provide the solution to New Zealand’s freight supply chain for the future?  Solution New Zealand’s North Island was chosen as a starting point for this new development. The researcher developed a multimethod framework consisting of three main components that would address the current model limitations and create potential outcomes. The goal was to expand freight train supply chain analysis to inform low-carbon transition pathways.  The three main components of the framework were:   A freight distribution model.  A freight network design model.  A discrete-event simulation model.   Official documents, databases, and open-source geographic data was used in the framework.    Multimethod framework of the

 

Section 37 of 60
model (click to enlarge)   In general, this framework gave a new simulation component to traditional transport modeling approaches. Integration was achieved through a central GIS based optimization component.  Component 1 – Freight Distribution Model. This was a preliminary step that enabled data sources to be arranged for further stages. The final output consisted of a data set with several different line features representing destination pairs between different facilities.  Component 2 – Freight network GIS based analysis mode. This was used to build a virtual network using elements from the field of multimodal transports and GIS. It enabled different layouts or scenarios based on energy and travel time optimization. The process involved loading a network data set along with the locations of the facilities, running a shortest path optimization, extracting relevant information, and then building a database.  Optimization was performed using either time of delivery or energy used as cost attributes.  Different outputs were derived, such as traffic assessment, location of intermodal terminals, and optimal delivery plans.  Component 3 – Discrete-event simulation model. The researcher used AnyLogic as it enabled the integration of  GIS maps, provided a  multimethod environment, and  specific libraries for simulation of railway systems, manufacturing, and warehouse flows.  Agents were facilities, terminals, ports, trucks, and trains. These agents interacted with each other as orders were received.  The performance of the system varied as the network infrastructure changed. Such changes included single tracking or double tracking of the railway network, train cruise speeds, and terminal parameters.  Agent parameters were taken from the database created in the second component. AnyLogic facilitated the modeling of this process and enabled the interaction of the model with a central database.  Component 3 provided the flexibility needed to connect networks planners, transport, and terminal operators. It was able to evaluate network and terminal capacity, while delivering a conceptual design that could meet the current freight requirements with half the current energy demand.    Railway and road network map (click to enlarge)   Results 6 simulation experiments were run, and in the table below the different parameters which were set can be seen. The experiments showed how the model could be used to streamline the design.    Table showing the setup parameters and resource utilization for each simulation experiment (click to enlarge)   Results from these experiments are shown in the chart below.    Experiment results   Experiments 1 and 2 had relatively low cost but were unsatisfactory in terms of performance.  In experiment 3, double tracking reduced the round-trip times by approximately 500 minutes.  Experiment 4 explored faster trains and led to a further 500-minute reduction in round-trip time. This reduction in travel time could give the possibility of adding more trains as required. In addition, the time savings could enhance additional clearance for loading and unloading operations at terminals. However, this was the mostly costly arrangement because of the need to upgrade the network for faster trains.  In experiment 6, the impact of the number of wagons was observed and more wagons could result in a slightly increased round-trip time if compared with experiment 1. It should be noted that, all trains in this experiment still managed to do a round-trip before the end of the day despite the low speed and single tracks. This was the most cost-effective solution and proved that substantial reductions in energy demand and greenhouse gas emissions do not necessarily require costly measures.  In conclusion, the results suggested that the implementation of intermodal terminals should be a priority, and accessibility to this intermodal network is key. A fully intermodal setup could reduce urban congestion as well as energy consumption and greenhouse gases by 48% and 47%, respectively.  In the future, investments should prioritize the development of intermodal hubs over other costly alternatives.  The case study was presented by Patricio Gallardo, of The University of Canterbury, at the AnyLogic Conference 2021.  The slides are available as a PDF.      Optimizing Large-Scale AMR Fleet Operations Link:  Tags: Manufacturing    Automotive industry leaders use autonomous mobile robots (AMR) in their production facilities to improve productivity. In this case study, Tesla Material Flow Engineer and former BMW Group PhD Student and AMR researcher, Maximilian Selmair, describes standard industry practice when deploying large-scale transporter fleets and demonstrates how AnyLogic cloud-based simulation helps develop optimal task allocation algorithms.   Why AMR and not AGV?   Autonomous mobile robots are more capable than automated guided vehicles (AGV) because they are more complex. AMR have greater software capabilities and navigate using maps, without the need for guiding wires or strips, so they are not restricted to fixed routes like AGV. As a result, AMR are more flexible in the tasks they can do, and they can be redeployed quickly with only a software update.   Compared to AGV, modern AMR technology is seen as being more cost-effective thanks to reduced infrastructure requirements and quicker deployment that does not cause production interruptions.    Problem: How to efficiently assign tasks to an automated fleet   The optimization of production line transporter operations has two objectives. Primarily, optimization should result in no late tasks. And secondly, the density of traffic in a production facility should be minimized.   Late tasks lead to delays that reduce efficiency and increase costs. For BMW, avoiding production stoppages was the main aim of optimization work related to transporter operations.   In production facilities, the space available for pathways is a limited resource and, as a result, autonomous transporters must share routes with people and other vehicles. The reduction of traffic has several desirable effects, including increased safety, less congestion, and fewer late tasks.   The research aimed to meet the goal of no late tasks with a minimum of transporter driving. A challenge that can broadly be characterized as the assignment problem.    Solution: Simulation to test AMR task assignment methods   To solve the assignment problem, a hybrid simulation model of a vehicle production line facility made it possible to test various methods. AnyLogic simulation software, with its multi-method modeling capabilities and its built-in Material Handling Library, allowed for quick modeling of the workspace, including the addition of automated transporters. The simulation uses both agent-based and discrete event modeling approaches.   Simulation model for testing task allocation algorithms for autonomous mobile robots (click to enlarge). Example cloud model  While AnyLogic includes a variety of methods for assigning tasks to transporters, it also offers the flexibility of including custom code. Using custom code allowed the testing of any assignment algorithm that might best solve the AMR task allocation problem. In testing, both heuristics and exact methods were analyzed. The exact methods are algorithms that always produce one optimal solution, such as with linear optimization. By contrast, the heuristic methods are based on approximation and may not be as accurate as exact methods, but are usually faster.   The methods tested when designing a task assignment algorithm for autonomous mobile robots (click to enlarge)  The research results from testing the various methods in an AnyLogic simulation model show that the Jonker-Volgenant-Castanon (JVC) assignment algorithm is superior when assigning tasks to transporters in a vehicle production facility.   Faster AMR scenario analysis with cloud-based simulation   After creating the simulation model to test different assignment methods, it was necessary to conduct many simulation runs. Each assignment method needed testing with parameter variations and in different scenarios.   In this case, 40 different fleet sizes were tested against different assignment methods. And, with each run taking two hours to compute nine hours of simulation, a solution that could speed up the process was welcome.   The AnyLogic Cloud platform is a scalable computing environment that allows parallel run execution. This computing capability allowed for rapid parameter variation and multiple scenario testing. For example, simulation runs for all 40 fleet sizes can be run simultaneously. Also, with the computing taking place on cloud servers, the simulation modeler is free to continue using their computer without hindrance.   Results: An optimal AMR task allocation algorithm   Compared to the baseline scenario that allocates tasks based on the nearest agent, the method developed from the JVC assignment algorithm reduces the number of AMR required by 30%.   In an example scenario of 7,500 tasks, the traditional method required 58 transporters to achieve a late task total of three. For the algorithm that resulted from the testing and research, only 42 transporters were needed to achieve the same number of late tasks.   Such a reduction in the number of AMR transporters required for a desired level of service is significant because of the high upfront cost of AMR, in comparison to AGV and manual methods.   Reducing the number of AMR required to achieve just three late tasks in 7,500 also helps meet the goal of minimizing traffic density.   The combination of simulation, for creating a testing environment, and cloud computing, for running the experiments, allowed the rapid development of a custom assignment algorithm for AMR task allocation in a vehicle production facility. The algorithm improved on standard nearest agent assignment by 30%, achieving the aims of a low late-task rate and reduced traffic density.   Maximilian Selmair presented the research at the AnyLogic Conference 2021:   Learn more about cloud-based simulation on our dedicated AnyLogic Cloud page.      Optimizing New Hyperscale Data Centers Designs for Rack Delivery Workflow Link:  Tags: Business Processes  A Hyperscale Data Center (HDC) is essentially a massive building filled with thousands of servers, racks, and network equipment, often several football fields large. Meta has many of these on their campus and uses a number of designs for them, including the H-shaped and I-shaped design.  Problem  When designing a new HDC, Meta needed to be sure that it would work well without delays or bottlenecks in all connected functions. One important element to consider was how to improve workflows connected with the racks in the building.  There are three types of workflows associated with the racks, but the focus in this case study is on “receiving”. This workflow occurs when a data center is coming online and racks are being brought into that particular data center.   The teams that support the workflows are highly skilled resources. Thus, these workflows and how to stage and plan the work should be optimized.   However, Meta did not have a good process to visualize and simulate operational constraints in HDC designs, and therefore, could not understand bottlenecks and throughput capabilities.   Solution  To better understand the requirements of a new HDC, Meta decided to implement a modeling approach. The first step was creating a 3D visualization for an agent-based model to facilitate rack flow data validation and accelerate teams’ workflows and resources’ learnings. Thus, they could see the entire workflow within the space where it is going to happen. This was all done in advance, prior to construction of the HDC.  The 3D visualization gave insights into the parameters necessary to set up the discrete-event simulation model, as well as optimizing them.    The 3D agent-based model   For the discrete-event simulation, a number of assumptions were necessary:   The HDC is an object with multiple parameters in the AnyLogic platform and the throughput of a variety of HDC types can be predicted.  The process to model is rack receiving with multiple steps spread amongst different teams (receiving and positioning – team 1, energizing – team 2, cabling – team 3, provisioning – automated).  Simulation is one week, and throughput is measured as a percentage of the total.  The number of resources in each team is configurable and the utilization is set between 60-80%.  Shifts start at 8am and overtime is permitted.  Team 1 processes include unloading of the truck, unpacking and paperwork, dock queue, data hall runs, elevator capacity, 1st or 2nd floor (50/50 chance).  Normal distribution for team 2 and team 3.  Provision has two steps – switch and server, with both being set to 80% (this means that 20% of the time they need to rework).   Discrete-event model overview flow chart in AnyLogic (click to enlarge)   One of the many great features in AnyLogic is the ability to develop a UI for the model, then change parameters, and see the results. Meta created a UI for their receiving model where each user could change the parameters for each team, e.g., the number of people, add overtime, change the unloading time, etc.   Receiving model parameters (click to enlarge)   Results  Discrete-event simulation Using a discrete-event simulation, Meta ran the model with regular parameters. The target throughput was 100%, but they achieved only 40% with an average duration of 3.7 days. Bottlenecks were identified in cabling and positioning within the racking process.    Summary of model results   Optimization experiment In order to solve these issues, it was necessary to identify the optimized value for each parameter. This was done using an optimization experiment, with the objective of maximizing the throughput.  The optimization results can be seen in the tables below. Based on these results the team could run the model again and achieve 92% throughput with an average duration of 2.2 days. This was a reduction of 1.5 days from the initial model. As a result, more racks could be received per week.    Optimization results   In addition, there were no bottlenecks and the only reason it was not 100% was because the time was set to one week and provisioning (automated process) could not be completed within this time constraint.  However, having an optimized model does not tell the whole story because, in the real-world, there exists an element of uncertainty.  Monte Carlo Experiment Meta understood this and decided to use a Monte Carlo experiment, which is a stochastic method using a random sample of inputs to create an output for the model.   Running the Monte Carlo experiment a number of times gives a distribution of output, and instead of just one scenario, you can have multiple scenarios at the end.   Meta ran the model 10,000 times and the results can be

 

Section 38 of 60
seen in the illustration below. The X-axis is the throughput, and the Y-axis is the probability of that throughput. The chart shows that 40% of the time the throughput is going to be 90%. There are also other options shown as well, such as a 20% chance of the throughput being 30%.  What these results show is that 90% throughput is not guaranteed, but it is the most probable outcome of the model.    The results for the Monte Carlo experiment based on the optimized model   Next steps   Include rack redistributing and refreshing processes to the model.  Add more details to energizing, cabling, and provisioning steps.  Create sensitivity analysis to the model to understand the best values for parameters.   After adding these steps to the model, the team can do the last step, which is increasing the time of simulation to one year and analyzing the results.  The case study was presented by Peter Lopez, Mohammad Shariatmadari, Marcin Starzyk, and Lakhwinder Singh, of Meta, at the AnyLogic Conference 2022.  The slides are available as a PDF.     Optimizing Quicklime Production Operations Link:  Tags: Manufacturing  Overview Tata Steel is one of the world’s leading steel producers. The company has operations in 26 countries, a production capacity of 34 million tonnes (metric tons) of steel per year, and, in 2021, had a turnover of $21.2 billion.  As the first steel manufacturer in India to receive the ISO14024 based CII Green Pro certification, Tata Steel is certified for good environmental practice in its industrial operations. The company’s engineers worked to improve efficiency at a limestone processing facility and identified possibilities using simulation modeling. The results reduced machine operation and cut costs.  Problem Lime is the most important flux used in the LD (Linz-Donawitz) steel making process. The LD process calls for stringent quality control and a steady supply of lime, which led to Tata Steel running its lime plants continuously. The company suspected efficiencies could be made, and started a project to optimize processes at one of its lime plants. The plant had nine kilns for the calcination of limestone to produce quicklime. Processes along the whole limestone conveying circuit would be analyzed, from the arrival of limestone until its firing in the lime kiln.     The limestone conveying circuit  At the plant, the limestone was unloaded in a tippler (rotary car dumper), sent to the primary screening house by conveyor belts, and passed through the double deck screening process. After screening, the material was transferred to storage bins and then to surge bins. From the surge bins, it was passed again through a single deck screening process and then transferred in weigh hoppers. The material was dropped into the kilns from the top using skip buckets.  Engineers thought the feeding circuit was underutilized and that running it continuously was unnecessary. The objective, therefore, was to optimize and better schedule limestone reclamation to maximize the utilization of the feeding circuit, as well as to reduce running hours.  Solution Tata Steel engineers developed a model for testing different what-if scenarios and understanding the utilization of the limestone feeding circuit.    The model was developed with a help of the AnyLogic Fluid Library (click to enlarge)   Scenario 1 The average current production at the site was calculated from plant data and fed into the simulation model, which yielded several insights. Production at the current rate could easily be accommodated with only one reclamation shift. All bins would maintain 20% to 100% capacity, and it would be sufficient to run the stockyard to storage bins circuit for just one shift each day.    Simulation schema for the scenario 1 (click to enlarge)  Scenario 2 The objective of the next simulation experiment was to find bottlenecks in the circuit. The modeling in AnyLogic helped to identify that Bin 2 capacity and Weigh Feeders 3 and 4 were the bottlenecks for the feeding circuit.    Simulation results for the scenario 2 (click to enlarge)  Scenario 3 The model developers also wanted to determine the maximum possible production with regards to the circuit constraints and utilization of surge bins.  As surge bins were available, Bin 2 stock could be transferred to them, meaning Bin 2 was no longer a constraint. This relied on there being free space in the surge bins at the start of a shift. The result was increased production, however, the surge bins had sufficient capacity for only six hours of production. After that, they had to be refilled again.    Simulation results for the scenario 3 (click to enlarge)  Scenario 4 Buffer stock analysis using the model allowed the plant to obtain the minimum stock levels that needed to be maintained in Bin 2, Bin 3, and Bin 4.  The Bin 2 constraint was no longer applicable for kilns 1-6 if the company utilized a surge bin with a minimum 400 tonnes capacity for initial stock. In such a case, it would be feasible to take full capacity production from kilns 1-6.  Enough buffer stock could be maintained for production from kilns 7-9 for two shifts. But the minimum available stock in the bins was not sufficient for two shifts of production from kilns 1-6, in case of any breakdown.  Scenario 5 Tata Steel engineers tested the schedule to understand buffer stocks in the circuit and determine sufficient levels. They developed the optimal strategy for daily production operations.    Spreadsheet showing processes and their capacities over three shifts The transfer from the storage bin to the surge bin could be taken in two periods of eight and six hours. This circuit could then be turned off for ten hours a day, saving power.  Results The simulation with AnyLogic helped Tata Steel to maximize the utilization of the feeding circuit. Thanks to modeling, they carried out five experiments and developed favorable plant operation schedules that could reduce machine running and cut costs related to electricity. Then, the specialists calculated the power consumption savings as well as cost savings per day.    Spreadsheet showing equipment power usage and savings Watch the video about this case study presented by Tata Steel at the  AnyLogic Conference 2021.       Optimizing Rail Operations of a Container Port Link:  Tags: Rail Logistics, Ports & Terminals  Overview  Port Botany is the second largest container port in Australia handling 2.8m TEU (a twenty-foot equivalent unit) p.a.   Over 40% of all NSW (New South Wales) household goods are imported through Port Botany. It contributes $13.6bn to NSW Gross State Product and supports about 52,000 jobs in the Sydney region.   GHD is a global professional services company that provides clients with integrated solutions through engineering, environmental, design, and construction expertise. GHD helped NSW Ports to assess infrastructure and operational changes.  Problem  Growing rail mode share was critical to optimize Port Botany's capacity and reduce road congestion in Sydney. The purpose of developing the model was to extend the rail mode share from 15% to 40%  by 2045 and significantly increase anticipated port rail throughput.   Additionally, Port Botany needed to resolve some challenges with an inflexible rail schedule and inefficient rail operations. There were also some constraints in the infrastructure.   Port Botany already had a model to test different scenarios, change some parameters, and see the impacts.   Port Botany’s supply chains were complicated. The main reasons for that were the following:    Port Botany was a tightly integrated supply chain that consisted of different components.    There were many stakeholders: 3 stevedores, 20 shipping lines, and State Government representatives.    There were many ways to influence rail throughput including improving scheduling and operations.     Solution  GHD used the existing model of NSW Ports and expanded it to assess infrastructure and operational changes. GHD included a number of different components for a realistic representation of rail operations.    The port supply chains model (click to enlarge)    Firstly, GHD  developers simulated the alternative train schedule and rebooking rules.   In this visual example, the rail schedule, with the inputs at the top,  shows different rail slots and trains assigned to arrive at different times. At the bottom, there are outputs from the simulation of one week. There could be free slots, rebooking of a train slot, and cancellations.    The additional elements GHD needed to simulate were the train shunting and cargo loading process, for example, any limitations on trains that can enter and leave the port.    The train shunting and cargo loading process (click to enlarge)    The rail capacity limits   Scenarios tested with simulation modeling (click to enlarge)    In the illustration on the right, the rail capacity limits were also tested with the simulation model. GHD set up a scenario and ran it for a year with test scheduling, operational changes, and infrastructure.   If everything was working fine, then rail cargo grew linearly as cargo was added until it started hitting certain capacity limits. Then the rail cargo throughput stopped increasing as the model developers continued to scale off the cargo entering the system.   Other indicators of system capacity limits were long cargo dwell times, as well as imports and exports building up.   The simulation model has helped address port planning challenges. Potential growth of 20% in rail throughput capacity was identified from operational changes. AnyLogic simulation software enabled GHD  to find the critical infrastructure bottleneck for the next decade of rail throughput growth. In future, this model could be expanded by adding total cargo costs and emissions impacts.   On top of that, using this approach, a staged infrastructure upgrade pathway was identified. In the illustration below on the left, GHD set up several scenarios with different types of rules regarding infrastructure and operations to find out the rail capacity.   They started with a current-day rail usage throughput. The red line shows future rail capacity if business as usual continues. The blue line illustrates rail scheduling changes that could increase potential gains by 20%. And the yellow one shows that critical identified bottleneck around rail shunting could unlock an additional 120% of rail capacity.  Results  The AnyLogic model was fundamental to developing a rail strategy of NSW Ports to optimize rail operations and maximize rail capacity. NSW Ports have used this model to test a number of different scenarios and understand what potential benefits for business could be through the operational changes or investment in rail infrastructure.   Additionally, they are also using the outputs of the model to advocate for changes in government policy to support increased use of rail.   Thanks to the model, it is possible to assess the environmental benefits of rail. It is important for sustainability, particularly in supply chains which are very heavy industries producing a  lot of emissions. NSW Ports can test environmental outputs and choose the scenarios which are more beneficial for them.    Port planning (click to enlarge)    And finally, NSW Ports uses the AnyLogic model to support business cases for investment in rail infrastructure. The model outputs are helpful in developing new infrastructure and removing the identified bottleneck. NSW Ports are also using the model to support business cases for investment in other rail infrastructure, for example, an automatic rail unloading crane.   The case study was presented by Jarrad Cayzer, of NSW Ports, and Rhet Magaraggia, of GHD, at the AnyLogic Conference 2022.  The slides are available as a PDF.      Optimizing Trading Business Process with AnyLogic Strategic Planning Software Link:  Tags: Business Processes, Asset Management  Problem Fannie Mae (The Federal National Mortgage Association) is a US government-sponsored enterprise that operates in the secondary mortgage market. It purchases and guarantees mortgages from lenders, such as banks and other financial institutions, securitizes them, and then sells them back into the secondary mortgage market as mortgage-backed securities. This provides market liquidity and helps stimulate activity. In 2018, Fannie Mae was ranked 21st on the Fortune 500. Operations form the core of Fannie Mae’s business and ensure the company’s successful performance. They are required to soundly, safely, and speedily execute the processes necessary to support Fannie Mae’s business functions while meeting customer user experience demands. In 2008, Fannie Mae was deeply affected by the US housing crisis and the company was placed under government conservatorship. This experience, combined with rapid and unpredictable consequences, led the company to understand its need for a strategic planning software or a digital tool that could provide deep insight into the organization’s processes, improve process handling, and assist in preparing for various future challenges. The resulting simulation model would enable Fannie Mae’s specialists to:  Test and validate new processes. Quantify potential process improvements. Identify possible bottlenecks. Connect processes for larger-scale insights. Evaluate business resiliency strategies.  The company decided to begin with a quick proof-of-concept project. To achieve this, they focused on the processes of trade confirmation and trade assignment. Trade confirmation entails the comparison of the trade agreement from both counterparties, verification of the accuracy of its execution, and strict SLA fulfillment. Trade assignment is a three-party agreement: a selling party assigns their obligation to Fannie Mae who then assigns the trade to a third party or to itself as a buyer. Trade assignment must also go through a trade confirmation process. It is challenging to get a holistic view of these two processes. They can vary by both the type of trade and the level of automation – from fully automated to completely manual and dependent on skilled analysts. Fannie Mae wanted their project management optimization model to fulfill the following tasks:  Resource optimization in project management (number of analysts required to accomplish all processes and to identify possibilities for multitasking) Preparation for trade volume increases (what-if scenarios concerning the capacity needed in different cases) Sensitivity analysis and resilience testing (identifying potential bottlenecks and the analysis of possible process changes)  Solution Agent-based modeling allowed the trade assignment and confirmation processes to be captured accurately and both the trade (task) and the analyst (resource) aspects were modeled. The analysts were modeled because they are necessary for manual processing. The Poisson arrival rate was taken for trades, and the

 

Section 39 of 60
triangular distribution for the manual processes. The choice of AnyLogic as a simulation platform was based on a number of AnyLogic advantages. AnyLogic is flexible and powerful strategic planning software that made it possible to build a model in 90 days, from idea to final product. Furthermore, the visualization capabilities of AnyLogic enabled Fannie Mae to make the model easy to understand for the finance professionals who were the target users for the tool.  Optimizing trading business process with simulation   The main inputs for the risk analysis model are trade booked and trade assigned. In addition, variables can be set for trade exceptions: delay time by type and the proportion rate. System users can also define the number of analysts available to process different types of tasks at both headquarters. Running the strategic planning model, the user can see the process overview visualization. The results are shown in charts, tables, and graphs. They show how many trades are processed over time and the capacity of different types of analysts. Running what-if scenarios makes it possible to see what will happen in various situations, such as peaks in trade volume of varying duration. The model can also suggest how many and which kind of analysts the company needs to deal with different trade volumes. Outcome This proof-of-concept project showed that simulation is effective and helpful for financial operations management optimization. The completed model can help identify potential bottlenecks, simulate the effects of extreme cases (like fluctuations in trade volume), and propose workforce and project management optimization.  Watch the video of John A. Coaster presenting this case study at The AnyLogic Conference, or download his presentation.       Optimizing Warehouse Operations for Pharmaceutical Distribution Company Link:  Tags: Warehouse Operations, Healthcare             Cardinal Health, a billion dollar pharmaceutical distribution and logistics firm, manages multiple products from brand name pharmaceuticals and generic drugs to over the counter drugs, health & beauty items and their own private label. They face a multitude of typical distribution warehouse challenges that are further complicated by the nature of pharmaceutical products, which are smaller in size, consumable, expensive, and could be life critical. Brian Heath, Director of Advanced Analytics at Cardinal Health, and an experienced user of AnyLogic software, employed agent based modeling to solve various business problems, saving Cardinal Health over $3 Million annually.   Problem   Tested Warehouse Layout Configurations           Cardinal Health is an essential link in the healthcare supply chain, offering next day delivery to over 30,000 locations including hospitals, retail pharmacies, physicians' offices, and direct to consumer. Other value added services including efficiency and demand management, working capital management and contract credit management add to the difficulties of poor manufacturing reliability and supply disruptions in the market due to FDA and DDA regulations. In summary, Cardinal Health must keep up with the variability in pharmaceutical distribution management.            Cardinal Health considers facility layout, flow of product, order picking, labor planning & scheduling, customer order requirements and congestion for analysis and day to day operations management. Traditional analysis tools such as empirical trial and error, are risky, expensive and difficult to make changes. Industrial engineering operations researchers would suggest mathematical models, inexpensive, but the models do not capture unexpected dynamics. If anything is open or has emergent behaviors such as congestion, a standard mathematical model would not be able to solve. Thirdly, process driven or discrete event modeling is not advantageous due to its inability to represent a facility naturally. This led Brian Heath and Cardinal Health to explore alternative analysis options.   3D Animation  Solution            Agent Based Modeling (ABM) with AnyLogic Simulation and Modeling software gave Cardinal Health the device required to tackle many distribution warehouse issues without the restrictions of traditional tools. ABM represents abstractions of distributed autonomous entities that can interact with each other and their environment through space and time, allowing Cardinal Health to capture work time allocation, congestion wait time, cycle times, distance traveled, worker variability and other important metrics.            The model built was ultimately concerned with the activities of employees and the interaction with each other during the day, making it necessary to import data such as picking time and performance standards into the model. Now, Cardinal Health can gather congestion wait time data and see how much of a problem it is causing in the warehouse since "agents" are modeled as individuals with special relationships to each other. Additional parameters included in the model are several worker speeds, worker behavior, learning curves, cycle times, product turn-around and distance covered walking or driving.            The ability to import Excel files was also imperative as Cardinal Health has numerous warehouses, and it is mandatory to test multiple layouts. Using AnyLogic, if a change is needed, it's as simple as updating the Excel file, importing it into the model and running the model again.   Outcome            The Agent Based Model built with AnyLogic software allows Cardinal Health to compare layouts, picking technology and product slotting strategies. In addition, they can evaluate different methods of picking to update staffing models and for on-the-floor support if a workload changes as orders vary on a day to day basis. Statistics is also gathered such as tact time, how many batches are completed in an hour, truck unloading time, and sequencing of events.            Besides the clarity given through the above metrics, the model revealed a problem due to the random distribution of work. Each employee's work load was uneven making one faster and one slower. By balancing the workload, employees began working at a similar pace and congestion decreased dramatically.   Some of the Project's Results            By minimizing congestion using AnyLogic software, Cardinal Health was able to decrease the average shift length from 10.5 hours to 7.25 hours and increase the amount employee capacity. Cardinal Health saves over $3 Million annually using Agent Based Modeling with AnyLogic Simulation technology.            “AnyLogic’s agent libraries, flexible architecture, and integrated animation enables the continuing success of this project,” declares Brian Heath, Director of Advanced Analytics at Cardinal Health, you can view his presentation and learn more about using Agent Based Modeling for real world application:       Order to Delivery Forecasting with a Smart Digital Twin Link:  Tags: Supply Chains    Accenture’s Applied Intelligence HSA team delivered an Order to Delivery (OTD) forecasting system for a US-based exercise equipment brand that was affected by COVID-19 related supply chain disruptions. The forecasting system, based on a supply chain digital twin, increased OTD forecasting accuracy by 57% and reduced costs by 20%.  The OTD forecasting system, its results, other improvements, and development plans are detailed in this supply chain digital twin case study. The project was carried out by Pablo Rodriguez Varela, Team Lead, Patricio Ivan Pipp, Manager, and their Applied Intelligence HSA team at Accenture Argentina. The team specializes in supply chain and operations analytics. The Applied Intelligence approach is to combine artificial intelligence with data, analytics, and automation under a bold strategic vision to transform business across every function and every process, at scale. Problem: Avoiding long order to delivery times   The client wanted to predict supply chain behavior and enable intelligent execution for its exercise bike supply chain in the United States of America.    During the COVID-19 pandemic, demand for exercise bikes grew significantly and caused order to delivery (OTD) times to increase from five days to sixty days. The challenge was to reduce the OTD time and improve planning. Five main questions needed answering and they fell into two categories:  Planning  What is the expected OTD time for any given new order within the planning horizon? What throughput should we expect through any given location within a 12-month planning horizon? What is the expected inventory at any location?  Execution  What inventory levels should be fulfilled and from where? Should inventory arriving soon at a port be rerouted to different DCs given the latest delay status?  Solution: A digital twin based on dynamic simulation   The Applied Intelligence team took the unconventional approach of creating a supply chain digital twin. Taking this approach created extra challenges and meant working closely with the client to ensure buy-in.   The supply chain solution focused on predicting OTD times, to help reduce them, and providing a basis for a smart inventory-allocation solution, to improve planning. By creating a digital twin as the solution, it is possible to understand why something is happening and ask what-if questions. A digital twin is a dynamic model of a system based on real-world data that can be inspected and observed in operation — the reason for results and behavior can be seen and explained.   The team chose to use AnyLogic simulation modeling software for constructing the digital twin because of its flexibility and machine learning integrations, which they planned to use later. In AnyLogic, the team replicated the entire supply chain from vendors, through distribution centers (DC), to final-mile sites using discrete event modeling.  The entire supply chain structure: from vendors, through distribution centers (DC), to final-mile sites (click to enlarge)   The vendors were bringing 40,000 products into the US each week from Asia, while the factories based in the US were producing 4,000 units per week. The network had 150 final-mile sites from which customer orders were fulfilled. The main elements of the model were orders, nodes, and trucks, each had associated data:  Orders: order creation date, product, node, order delivery date. Nodes: location, type (vendor, DC, final-mile), inventory, processing time. Trucks: capacity, leadtime.  Map of nodes in the supply chain digital twin (click to enlarge)   The digital twin of the supply chain had a forecast range of one day to one year. Running Monte Carlo simulations on the model in the digital twin provided forecast ranges for OTD times. For each set of simulation runs the model required key inputs and provided key outputs: Key inputs: location master, inbound/outbound location capacity, inventory position and backlog, target days-of-supply, lead times between nodes, demand forecast, production forecast Key outputs: forecast OTD, final-mile order backlog, inventory position by network node, resource utilization (trucks) For the solution to be a digital twin and not just a simulation model, there needed to be a live data connection. In this case, the data came from various Amazon services and spreadsheets all connected through Amazon S3. In addition, Tableau was connected for business analytics on the model outputs. A schematic of the system can be seen in the information flow diagram.  Information flow diagram for the supply chain digital twin (click to enlarge)   The digital twin was configured with the supply chain’s current state and verified. It showed the expected behavior and identified where improvements could be made.   After testing and analysis of the original state of the supply chain, the team used the supply chain model to heuristically optimize supply chain operations. Variables such as required inventory and safety stock were no longer fixed by network-wide rules. Also, parent replenishment DCs could be reassigned depending on the needs of final-mile sites.  The optimized supply chain delivered significant improvements (click to enlarge)  Results: The benefits of a supply chain digital twin   For the focus areas of OTD prediction and smart inventory allocation, the expected benefits of the supply chain digital twin initiative were significant. Respectively, an increase in accuracy of 57% for order to delivery forecasting and a 20% cost reduction for inventory allocation logistics costs.   Additionally, the expected accuracy of the estimated time of arrival for leads increased from 40% to 76%.   These results came from a thirty-minute simulation that only needs to be run once a week.   The project was not without challenges. There was resistance to using a digital twin from teams at the client. To overcome this resistance, it was necessary to work closely with the concerned parties. The visibility into the dynamic behavior of supply chain operations and the extra possibilities enabled by a digital twin helped gain support for the decision to use the approach.   Other challenges related to missing data and undefined logic. Because of the decision to use a digital twin approach, it was possible to overcome these challenges with assumptions.   Now, the digital twin will form the core of the supply chain, with the result that decision making can be holistic and more informed. Next steps include the integration of reinforcement learning to further help with optimization and planning for black-swan events. And, as the digital twin is built upon, it is envisioned that its capabilities will become applicable at all organizational levels: strategic, tactical, and execution.   The case study was presented by Patricio Ivan Pipp and Pablo Rodriguez Varela of Accenture Applied Intelligence HSA at the AnyLogic Conference 2021.  To learn more about digital twin development with AnyLogic, see our dedicated AnyLogic digital twin page.      Passenger Flow Simulation at Frankfurt Airport Link:  Tags: Passenger Terminals   As the operating company of several major international airports, Fraport AG is one of the main “Global Players” in the airport industry. With more than 140,000 passengers per day and over 80 aircraft movements per hour, Frankfurt airport — an aviation hub of worldwide significance — is Fraport AG's busiest airport and its home base.    Between 1980 and 2010, the annual passenger flow through Frankfurt airport has more than tripled from 17 million to 53 million. In its 75-year history, the airport has gone through a variety of construction projects in order to adapt the facilities and meet increasing demands. The result of these adaptations is a very complex structure and the necessity for airport optimization technologies. Frankfurt Airport Optimization Using AnyLogic Simulation Software Because further structural changes were limited, Fraport AG

 

Section 40 of 60
decided to develop an airport passenger flow management system in order to improve capacity planning and customer satisfaction, e.g. by optimizing airport processes and reducing waiting periods.      Using active terminal management, the passenger flow within the airport building is controlled, for example, by dynamic signage. The terminal management itself is based on measurements of current passenger flow and forecasts for the future.   Airport Passenger Flow Management (Source: Fraport AG)    Video of the airport simulation model (Source: Fraport AG)   Core to the forecast methodology is an airport simulation model that was developed by acp-IT AG on behalf of Fraport AG. This airport model is based on the acp-IT InFrame Synapse Simulation Suite and the simulation software AnyLogic. In addition to meeting simulation accuracy demands, the target was to achieve very high-performance passenger flow forecasts, looking several hours ahead and calculated within a few minutes. All the essential characteristics that affect passenger flow in the airport had to be considered. This meant that in addition to the 26 security check points, 8 boarding pass check points, 15 border control points, 90 stairways and elevators, 266 gates, 1 tunnel and 3 SkyLine stations, accurately modeling the passengers themselves. For their representation, a simple mathematical model, a trajectory model, and a, so-called, social force model, which considers the interaction between people, were implemented and compared for accuracy and performance.           The trajectory model, as well as the social force model, provided the required accuracy to forecast airport passenger flow. However, the simulation speed of the trajectory model was about twice as quick. Since it took about five minutes to perform the simulation, the performance targets were met.     Since the summer of this year, the terminal management of the Fraport AG has successfully been using the airport simulation model to optimize passenger flow. The simulation is run almost 300 times per day and about 15 GB of data is generated. With the help of passenger flow management it was possible for 5.5 million passengers to pass through the airport, marking the most successful month in the history of the airport — without any problems.   Airport Passenger Flow Simulation Model Start Screen (Source: Fraport AG)   Airport Passenger Flow Simulation Model Representation (Source: Fraport AG)  Passenger Flow Simulation at Frankfurt Airport.pdf      Passenger Flow Simulation for Railway Stations at Sochi 2014 Olympics Link:  Tags: Passenger Terminals  The Adler – Alpika-Service railway is one of the major infrastructural projects of the Sochi 2014 Winter Olympic Games in Russia. Athletes and visitors will use this railway to get to the mountain stadiums and ice palaces. The estimated traffic handling capacity of the railway is a maximum of 8,500 passengers per hour. The intervals between trains are projected to be 6-8 minutes.     NIIAS is a scientific institution that specializes in railway traffic and infrastructure management. This organization has been operating for more than 50 years in the field of railroad automation and modern IT technique implementation. NIIAS was assigned to design the stations for the Adler – Alpika-Service railway.     As a part of the traffic organization project, five railroad station models were simulated with AnyLogic software. AnyLogic’s pedestrian modeling library helped to determine the shortest possible intervals between trains and to find solutions for desired station traffic handling capacity. The modeling results also showed that ticket barriers should be installed after the Olympics so as to not slow down the expected high pedestrian traffic.     In order to reach maximum realism in the passenger traffic simulation, an Agent Based modeling method was used. It is the only simulation modeling method which takes into account individual passenger behavior. Discrete Event modeling was also used in this project to simulate train movement. NIIAS specialists noted that AnyLogic’s advantages included the support of all simulation methods and the existence of a comprehensive library for pedestrian and train traffic modeling.     Using these simulation results, NIIAS specialists continue to work on the project, while taking into consideration the newly discovered factors and limitations. NIIAS specialist Alexander Ignatenkov stated that simulation modeling is almost always used in new railroad/station planning. It was used to calculate passenger notification time at the station before high-speed train arrival and to simulate train traffic at the bridge of the projected high-speed railway Moscow-Smolenskaya – Usovo.       Pedestrian Simulation and Road Traffic Modeling at Sheremetyevo Airport Link:  Tags: Road Traffic, Passenger Terminals  Sheremetyevo is Russia’s largest airport, with an annual passenger throughput of 40 million people. The airport infrastructure is constantly expanding. Passenger Terminal B is to be commissioned in 2018, and in 2020 and 2023 it will be integrated with Terminals C1 and C2 which are under construction. Additionally, a plan is underway to expand the car parking area at the station square. These changes are expected to help increase the airport capacity and reach an annual throughput of 52 million passengers by 2024. Airport simulation modeling and optimization for terminals B, С1, and С2  Problem To evaluate the efficiency of improvements, and to understand whether the newly designed airport facilities would be able to handle the projected traffic, the airport management requested that the consultants from the Institute for Development of Transportation Systems (IDTS) develop people flow simulation models for three prospective terminals.  The consultant team was tasked with the following:  To perform people flow simulation and assess throughput capacity of the new design for terminals B, C1, and C2. To determine the optimum numbers and locations for limiting elements: check points, escalators, turnstiles, elevators, etc. To run stress tests and evaluate the stability of designed solutions under the conditions of increased loads.   Solution IDTS consultants developed a simulation model for three interconnected airport terminals and one dedicated railway terminal. Passengers in the model used different types of transportation, including car, bus, taxi, and dedicated railway train. The model considered various characteristics of passengers, such as:   Airport security screening simulation   Presence of luggage — in the model this influenced the speed of movement and time taken by passenger services. Use of trolleys — this defined the size of the area around a passenger and the speed of movement. Presence of welcomers/accompanying persons near the passengers — this influenced the space available for free movement. Use of elevators — this determined the load on escalators.  The developers also simulated, in detail, the process of passenger service at checkpoints.   Result  Airport simulation modeling allowed engineers to:  Change the scheme of traveling from bottom to top level and vice versa in two airport terminals. Determine the optimal locations for security checkpoint equipment. Determine the required width of pedestrian passageways between parking lots and terminals. Approve the decision on additional equipment procurement.  For the basis of this project, the consulting team developed an airport planning and optimization toolkit — a custom AnyLogic library for airport simulation, which allowed users to design and optimize airport processes of any complexity in a short period of time. Traffic flow simulation at the airport terminal square Problem Prior to the commissioning of passenger terminal B, the airport management decided to determine which option for road traffic organization at the airport terminal square would be the most efficient, while outlining the bottlenecks in the selected option to cope with them. To do this, the consulting team from IDTS used the elements of the Road Traffic Library in AnyLogic traffic modeling software. This helped them simulate vehicle movement in detail, integrate the airport terminal square into the existing road network, and evaluate the throughput of the roads around the terminal prior to project implementation. Solution  Consultants developed three road traffic simulation models, which were then evaluated in terms of traffic jams and the speeds at various road sections. The effectiveness of the road traffic planning solution was determined by the following parameters:  Number of entry and exit checkpoints, and service time for them; Turn angle when approaching the checkpoint (in one of the options, the cars had to make a 180-degree turn); Number of parking spots; Location of pedestrian crossings; Free waiting time near the disembarking/embarking area.  Depending on the current action, the vehicle entering the station square would change color.  Color indication helped the consultants determine the causes for traffic jams in the airport terminal square. In various cases these were:  A large number of “black” cars that were unable to find a parking spot.  This could be due to heavy traffic, high capacity at the parking entry checkpoint, or a large number of vehicles with waiting time above 15 minutes. Lots of “purple” vehicles leaving the parking lot. An insufficient number of exit checkpoints. Traffic jams on the road after the exit checkpoint.  Consultants also performed stress tests to identify the bottlenecks in the model. Using AnyLogic as a traffic jam simulator, they applied car density maps available in the software and defined changes of traffic density levels when monitoring the following variables:  Number of cars entering the station square; Number of cars waiting for more than 15 minutes; Service time at the exit checkpoint; Number of passengers in vehicles.  Result With the obtained results from the model experiments, the customer could select the optimum plan of traffic planning at the airport terminal square, which would later be implemented in the construction project.      Pedestrian and Traffic Management for the Design of a Parking Facility Link:  Tags: Road Traffic  Thungela is a leading exporter of high quality, low-thermal coal in South Africa. It has several operations in South Africa, but the one used for this project was the Khwezela Colliery, which is an open case thermal coal mine and processing plant.  Problem There are approximately 1,200 employees and 90% of them use their own vehicles to come to the mine. There is a shift system with around 300 people per shift. Half of these people work in the mining pit and, due to its distance from the entrance, they require a shuttle service to get there.  There were some challenges with the parking infrastructure that resulted in poor control and management of vehicles, people, and goods coming into the complex. So, a new design of a parking facility needed to be developed for all employees where they would park or be dropped-off in the designated areas. Then, if necessary, they could take the shuttle to their place of work.  Analyzing the pedestrian traffic through the facility was necessary so that the appropriate number of boom gates and turnstiles could be identified without creating a bottleneck at the entrance.  Confirming the parking lot's size was necessary to avoid under or over designing it.  At times, there was a problem with alcohol, so 100% alcohol testing on entry also needed to be implemented.  Finally, in order to incorporate the most recent technology, the current security facility had to be renovated too.  Solution   Parking layout overview   Creating an AnyLogic model and running a simulation was identified as the best way to design a parking facility.  Simulation using AnyLogic has many benefits including:   Providing a low-cost method to test modifications and receive prompt feedback on their potential effects on the KPIs required to be evaluated.  Relatively simple updating of parking and entrance layouts and measuring traffic and pedestrian density using the AnyLogic standard Road Traffic and Pedestrian Libraries.  The smooth integration of the Traffic and Pedestrian Libraries with standard functionalities such as Events, Select Outputs, the Process Modeling Library, etc.  Visually identifying the accumulation of pedestrians and vehicles using heat maps. This makes it easier to identify the flow of traffic and spot potential bottlenecks, which could be useful for determining where to do extra sensitivity analyses, such as adjusting the number of turnstiles or altering the pathways so that people can enter in different ways.  The ability to export the model as a stand-alone application for client testing and analysis.   Single run Initially, a single run with animation was conducted. The 3D views helped with verification and validation as well as providing a more visual comprehension of what was happening.  All information was imported into the AnyLogic database from an Excel scenario file which contained all the parameters. When the model finished running, the results were downloaded to txt files which could be opened in Excel. Then the data was viewed and analyzed.  In this single run, it was possible to determine the likely values of variables such as arrival rates or delay times for opening a turnstile.  However, the model designers did not have all necessary information, for instance, what would be the final delay when opening a turnstile. To overcome this, a sensitivity analysis could be performed.  Sensitivity analysis experiment A sensitivity analysis is when a simulation model is run multiple times while varying one or more parameters and showing how the simulation output depends on these.  In the sensitivity analysis experiment run for Thungela, all parameters were analyzed simultaneously. The number of employees, shuttle capacity, and other variables could all be configured at once. A new experiment was generated and ran in the background for each set of parameters. The number of days was set for 30. The final results could then be downloaded in detail or as a summary.  There were three output files – employee waiting, shuttle waiting, and vehicle waiting. These could all be exported into Excel to compare and analyze different variations.  Below is the Thungela pedestrian and traffic simulation model for a new parking facility from this case study.     style="border: 1px solid #e0e0e0">  Thungela pedestrian and traffic simulation model for a new parking facility Results The sensitivity analysis produced all the results for this design of a parking facility since this range of outcomes was required to determine the maximum and average times for vehicles and pedestrians entering and exiting at each gate.  Maximum waiting time at gate stop    The first result displayed the longest possible

 

Section 41 of 60
waiting time at the gate stop for various numbers of employees arriving at the same time. There was only one shuttle available to take them to the mine, but there were different capacities. So, the wait times were about 30 minutes for a 36-seat shuttle and 50 minutes for a 22-seat shuttle. From this one graph, all questions about shuttle capacity, number of employees catered for, and length of wait could be addressed.     Maximum waiting time at the gate stop with 1 shuttle   Drop and go turnstile maximum waiting time In the second result, the chart depicts the drop and go turnstile. This is where the employees are arriving on the outside. Then they have 5 seconds for alcohol testing and 5 seconds for the turnstile to open, completing the process. The maximum number of employees arriving at a shift simultaneously was about 60 (indicated by the green line in the middle).  With one turnstile, there would be a considerable amount of waiting time, with 2 turnstiles about 250 seconds, and so on. The project team could then make an informed decision about the maximum acceptable amount of waiting time for employees.    Maximum waiting time for employees arriving at the drop and go turnstile    Parking lot turnstile maximum waiting time The parking lot turnstile is displayed in the third set of results, and there is far less variation than there was in the previous results. This is because people park in different places and take different amounts of time to get to the turnstile compared with the drop and go where 60 employees could arrive at the same place at once. This insight was only recognized after running the simulation and looking at the statistics.    Maximum waiting time for the parking lot turnstile with a 5 second entry delay   Maximum waiting times for vehicles at the entry boom gate The maximum number of vehicles waiting at the entry boom gate was shown in the final set of results. The estimated number of vehicles arriving at the same time was between 30-60. As such, the maximum waiting time was 10 minutes even if there was a 10 second boom gate delay, which is extremely long. Five seconds is the more typical time for an entry boom gate delay.    Maximum waiting time for vehicles at the entry boom gate   The case study was presented by Makhehla Nkosi, of Thungela, and Jaco-Ben Vosloo, of Jaco-Ben Consulting, at the AnyLogic Conference 2022.    The slides are available as a PDF.     Pet Food Production Optimization Using Manufacturing Process Simulation Software Link:  Tags: Manufacturing  Problem Perfection Pet Foods is a US manufacturer of extruded pet food, with a focus on ultra-premium products for dogs and cats. For three consecutive years, demand for the company’s products increased annually by approximately 35%, while demand profiles changed as people switched their preferences from big dogs to smaller ones. This was the reason the company decided to optimize their production line and adjust it to the current demand. The individual stages of the company’s current production line had enough capacity to fulfill new demand profiles, but the altogether production process was not harmonized enough. It was also not sustainable because a lot of waste was generated during the process. All these factors resulted in congestions in storage facilities and great financial losses. To discover solutions for underperformance in the production process, the company’s executives needed to analyze process phases, schedule them in relation to one another, and then optimize the schedule. They tasked ITE Consult, a strategic planning and simulation consulting company, with the project, which aimed to:  Align production with demand. Optimize production phases. Minimize production waste. Maximize plant occupancy.  To reach these goals, the ITE Consult team decided to apply manufacturing simulation as a perfect approach for better production scheduling and bottleneck visualization. It would help the team visualize the manufacturing process, get insights into process phases, see how they affect each other, and test various scheduling and production profiles in a risk-free environment.  Production optimization using manufacturing simulation software   Solution The production process was complicated, so the consulting team split it into three major phases with assigned restrictions, including:  Crude extrusion, coating and drying with restrictions according to the formula type. Storage process, with storage bins capacity limits. Packing, with packing rate and package size restrictions.  The company produced various types of pet food, so the consultants considered this when analyzing the production process. To build a production simulation model, the team applied AnyLogic manufacturing simulation capabilities. Using the AnyLogic Fluid Library, the engineers created a model of the shop floor with the production line. With the AnyLogic agent-based modeling approach, they linked all production phases so that they would exchange data, and included the current scheduling and production portfolio in the model. Then, the engineers used the OptQuest optimizer, which is built in the AnyLogic production optimization software, and set a pool of strategies for each production phase, including parameters for food constraints, restrictions, and time frames for each production phase. The optimizer acted as a global agent and determined strategies that could be used together in the most efficient way, and gave users the optimal decision. End-users of the manufacturing simulation model would benefit from the easy-to-use interface when applying the model for everyday scheduling. With Excel input files, they could specify demand, production limits, and extruders characteristics. After running the model, they would get a detailed simulation-based schedule of all the processes and all the metrics presented in charts. AnyLogic process simulation software also enabled the engineers to incorporate Python analyzer into the model. It made data post-processing easier. Users could move back to the production simulation launcher and navigate to Python’s queries, which allowed for a deeper visualization of specific cases and a better understanding of the simulated process. Outcome With the help of the multi-objective AnyLogic manufacturing simulation model, built by ITE Consult, Perfection Pet Foods company managed to:  Map out a detailed and optimized schedule of all the production steps. Reduce waste by 90%. Increase production rate. Eliminate bottlenecks across the production process.  Currently, the model is used on a regular basis for detailed weekly production scheduling to maximize production capacity in the environmental uncertainty. Watch the video of Elisa Elena, Gaston Fourcade and Javier Cortes, presenting this case study at The AnyLogic Conference, or download the presentation.       Pharmaceutical Supply Chain Simulation with AnyLogic Link:  Tags: Supply Chains, Healthcare  Problem  The Sterling Simulation consulting company was hired by a pharmaceutical giant to create a supply chain model. The client was introducing one of their products to new markets and wanted to visualize the restructuring of the supply chain and how it would react to demand uncertainty. The company’s goal was to reduce lead time while maintaining fill rate and avoiding backorders or lost sales. The client wanted a flexible ‘what if’ tool for risk mitigation analysis. Solution  The client had previously built a discrete-event supply chain model in AnyLogic. However, it lacked flexibility because it could not be easily adjusted to new conditions. Sterling Simulation engineers offered a solution: build a hybrid model, which would combine agent-based supply chain components with discrete-event processes. The new model was easier to configure and use, as it allowed for run-time construction of both the supply chain itself and the processes inside each component. The model’s core included agents that represented:  Production facilities, containing one or more processes and shared resources. Production lines, which incorporated production and shipping logic. Production process steps. Types of batches with preassigned sizes. Shipment requests for product batches.  After the initial processing, the model used MRP scheduling to determine production needs. The model was impacted by demand variability as it created surpluses or shortfalls in safety stocks. Using these results, the developers reached the client’s prime goals of satisfying demand and reducing financial and operational risks.  To determine whether to produce, the model calculated the sum of current demand and desired safety stock. If it was less than current inventory, they did not produce, otherwise they charged inventories and started production. To validate the model, the client could not use the existing supply chain because it did not include the new markets. By leveraging the model’s configurability, they applied it to a different supply chain which had available data, and showed that the model's logic worked properly. Outcome The model showcased how lead time reduction techniques could reduce inventory in a pharmaceutical supply chain. The model’s hybrid nature allowed analysts to combine supply chain components, including production facilities and lines, with features of production and shipment flows, making the model highly flexible. It was largely possible due to AnyLogic multimethod modeling abilities, which combined agent-based and discrete event approaches. Thanks to AnyLogic, model building time was also drastically reduced. To include more dynamics into the supply chain model, and conduct in-depth experiments with it, the company and the contractor want to shift the AnyLogic model to anyLogistix, a specialized software for supply chain design and analytics. This approach could bring more insights to the company’s executives, including the case of supply chain expansion.  Project presentation by Scott Hebert, Vice President of Sterling Simulations:     Planning Agro-Industrial Logistics with Simulation Link:  Tags: Supply Chains  Problem Kernel is the world-leading manufacturer and exporter of sunflower oil and provider of agricultural products from the Black Sea basin across world markets. The company owns 550,000 hectares of land and more than 40 grain elevators, with a total storage volume of up to 2.8 million tons. One of the company's regular challenges is the planning, harvesting, and transportation of agro-products. Planning requires considering a large amount of input data that can influence the performance of the entire supply chain, including:  Harvesting schedule Characteristics and location of harvested products Capacity of grain elevators Elevator equipment characteristics Number of vehicles involved in harvest transportation  The company needed to meet this annual challenge without additional financial investments and to forecast supply chain behavior with a changing number of equipment. They decided to analyze logistics operations and perform logistics network optimization in a risk-free environment to avoid extra costs. They commissioned research by the Business Logic consulting company. The consultants created a digital logistics optimization model of the company’s supply chain network using AnyLogic simulation. Solution  Logistics optimization model screenshot  (click to enlarge)   The logistics operations optimization model, developed by Business Logic consultants, reflected Kernel’s supply chain, including the processes of harvest transportation from fields to elevators, its treatment and storage in elevators, transportation from elevators to ports, and its shipment inside ports. The consultants applied several methods to develop the logistics operations optimization model. The supply chain components were represented as agents, while production processes in elevators and ports were simulated with the help of discrete-event modeling. The model also reflected the interaction between various hubs and equipment in elevators and ports, including:  Vehicles unloading Operations of drying and washing equipment Goods storage in the elevator system Rail transportation of harvested products  The developed logistics network optimization solution allows users to simulate supply chain operations and also conduct experiments with its components to forecast how various circumstances will affect network performance. In the model, users can adjust equipment characteristics, speed of harvest drying process, location and quantity of elevators, different transportation strategies, and specify various characteristics of the harvested products, like its moistness.  The model is also useful for:  Operations scheduling when distributing harvest between elevators, considering storage volume constraints, warehouse capabilities, and loading/unloading points capacity. Decision-making when introducing new storage areas, and optimizing existing facilities or modernizing equipment at elevators. Planning harvest distribution and elevator capacity utilization depending on weather conditions.  Result The developed logistics operations optimization model allows Kernel specialists to:  Conduct digital supply chain-based experiments, including stress-testing, in a risk-free environment.  Reduce planning time; prior to the project, single scenario calculation took two weeks and the model helped reduce it to one hour. Determine cost-optimized supply chain configuration.  At the end of the simulation, several reports are generated. These reports contain data on storage space turnover, elevators’ equipment, vehicles’ occupancy, and other indicators required for making decisions on supply chain configuration and for scheduling daily transportation operations.   The logistics optimization model is a useful decision support tool for planning supply chain seasonal operations. The tool also allows users to plan product distribution weekly, monthly, and annually.      Planning Green Hydrogen Production and Transportation Link:  Tags: Supply Chains, Manufacturing   GHD is one of the world’s leading professional service companies operating in the global markets of water, energy, resources, and transportation. They provide digital engineering services to private and public sector clients. One of their areas of expertise is green hydrogen supply chain simulation.           Green hydrogen is made from splitting water using electrolysis powered by renewable energy, thus avoiding carbon production. When mixed with nitrogen, green hydrogen can be more easily transported over long distances by ships. However, the expenditures for plant design, logistics, market research, risk assessment, and other factors are not always apparent, and the uncertainty of levelized hydrogen production and transportation costs often become the main reasons for stopping the project.           To evaluate the feasibility of such projects from both the

 

Section 42 of 60
engineering and financial sides, GHD applies green hydrogen supply chain simulation. This helps them link engineering design scenarios directly to business values. Unlike spreadsheet modeling, dynamic simulation better captures the complexity and risks, visualizes processes, as well as accounts for the projects’ economics. Simulation models allow executives to interact with digital replicas of the projects and make more intelligent decisions.   Problem           An Australian company needed to build a green hydrogen production plant and a supply chain that would be financially and technologically efficient. Instead of building static green hydrogen logistics models, they hired GHD, who used a combination of traditional engineering design and dynamic simulation to:    Develop a hydrogen production facility design. Evaluate the economics of the project. Assess the project’s possible risks. Optimize the return on investments.           GHD chose AnyLogic as a simulation tool for the following reasons:    AnyLogic allows for capturing the dynamics and variability. This was important because green hydrogen production, as all renewable energy, is variable and depends highly on the time of day and weather, as well as plant maintenance and asset availability. Therefore, static models would be unapplicable for planning. AnyLogic Fluid Library is a perfect tool for capturing gas flows in a simulation. AnyLogic allows engineers to break down the model into components and switch them on/off in the simulation process on demand, depending on the scenario for evaluation. This adds up to the variability simulation opportunity. Extensive visualization capabilities make AnyLogic models more interactive and engaging for clients and stakeholders. AnyLogic model is not a “black box”. It is transparent, its inputs are separated from its logic, and in addition, AnyLogic provides model version control capabilities.  Solution           In the model, the production process takes raw inputs – desalinated water, air, and electricity – going to the electrolyzer plants, where different technology selections for hydrogen production are applied. Then, the hydrogen is compressed and goes to the ammonia plant where hydrogen is synthesized with nitrogen for easier transportation.   Cashflow analysis for the green hydrogen production supply chain (click to enlarge)          The precise simulation of gas production and storage was possible thanks to the AnyLogic Fluid Library. It easily captured various characteristics of flows, such as rate and throughput, to find possible bottlenecks and downtimes, and optimize operational processes. To simulate flows’ behaviors, the library used the discrete rate simulation approach. This made the modeling process more transparent and allowed users to track flow changes when they took place.           The viable options for hydrogen and ammonia logistics were also simulated, including modes of transportation and cost per km/kg, required assets, and enabling infrastructure. The cost of enabling port infrastructure to export ammonia was also considered.   Power load and power supply in the green hydrogen supply chain (click to enlarge)          To post-process the outputs and provide detailed cashflow analysis, the Python integration capabilities of AnyLogic were used. They extended the green hydrogen supply chain simulation with the financial model which included statistics for cashflows, NPVs, levelized costs, ROIs, and other financial metrics. They visualized the analytics using dashboards so that the management could compare scenarios and look at the value changes over time.           The production and transportation green hydrogen model allowed the engineers to:    Assess the capacity needed for different assets including water and hydrogen.  Assess the efficiency of different technologies for electrolysis, including the ones used at alkaline electrolyzer plants.  Test equipment set-up and monitor how changes in the set-up, including dynamic availability due to maintenance and breakages, influenced the green hydrogen lifecycle.  See how much energy was being consumed at each production step to keep the plant operating and evaluate excess energy.   Green hydrogen supply chain model - a breakdown   Result            The green hydrogen supply chain model was used to:    Assess trade-offs between renewable energy generation cost and production plant utilization.  Compare market pathways of those who are buying the hydrogen.  Find the optimal transportation method of product by distance.  Compare vendor technologies and assess their effectiveness.  Understand the key drivers of levelized costs and how to minimize them.  Calculate required storage capacity.  Test changes and see how they would influence the process.           The supply chain model combined different workstreams and allowed users to see how each stream affected the green hydrogen end-to-end lifecycle and different parts of the supply chain. It helped link engineering scenarios to business outcomes, which resulted in better planning and enterprise decision management. The simulation also brought more clarity into the interconnected processes, as well as captured uncertainty and variability, which is impossible with static models.           Although production of hydrogen at the needed cost is currently beyond reach, possible pathways to this goal were found. And the model assumptions can easily be updated as new information becomes available and costs reduce over time.            This case study is from a presentation given by Geoff Martin, Technical Lead of Simulation Analytics & Strategic Insights team, at the AnyLogic Conference 2021:       Planning a Multi-Purpose Harbour with AnyLogic Link:  Tags: Ports & Terminals  Context           During the design phase of a multi-purpose harbour, a producer must ensure that the proposed system will allow required product throughputs to be met.           The problem is complex because of resource constraints, and additional considerations such as maintenance, weather delays, trucking of TEUs, and the impact of other shipments.           Key decision to be made: Do the proposed operating parameters meet the required throughput, and what additional resources are required to optimise bottle necks?   Modelling Approach            Evans & Peck utilised a discrete event simulation modelling approach to map the transportation, warehousing, and loading to ships of product.            Key benefits of the modelling tool developed include:     the producer was able to ensure that the proposed harbour was capable of achieving the required annual throughput  a variety of scenario experiments enabled the identification of bottle necks and measures that may be implemented to increase resource utilisation and efficiencies of the port operations  the impact of different ship size mixes and number of berths could be examined   Model developed and published by Evans & Peck Australia.    Population Simulation Modeling Using Real-World Data to Battle Food Insecurity Link:  Tags: Social Processes  Problem Food security is a complex sustainable development issue, linked to health through malnutrition, but also to sustainable economic development, environment, and trade. Issues such as whether household’s population get enough food, how it is distributed within the household and whether that food fulfils the nutrition needs of all members of the household is an ongoing problem in developing countries. An International Civil Agency employed a Major Consulting Firm to analyze food insecurity in developing countries and cultivate a system that enables an economy to prepare for and halt possible food insecurity. Solution The consultancy chose AnyLogic to explore the impacts of food insecurity in developing countries by building a population simulation that leverages Household Economy Analysis (HEA) and places it into a System Dynamics and Agent-Based modelling framework. The population simulation model allowed the Consulting Firm to explore how different mitigation strategies could be employed to reduce or eliminate the human impact of a situation that might otherwise lead to widespread food insecurity. Household Economy Analysis divides the area under consideration into Livelihood Zones (LZs). A LZ is defined as an area within which, people of the same socioeconomic status make their livings in about the same way (e.g. small land hold farming, or nomadic herding, etc.). The population is then divided into socioeconomic levels, called Wealth Bands (WBs) (e.g. very poor, poor, middle, and better off). By using historical data about the existing population, as well as field work conducted by a partner firm, the consultants were able to build out a System Dynamics population simulation model showing how cash and food flowed into and out of a typical household in each WB.  These typical households were then replicated in the population simulation model as agents, keeping the relative proportion of people in each Wealth Band consistent with real-world data. The agents made decisions annually about crop production (if their LZ and WB contained crop cultivation). Additionally, each agent had an individualized level of risk tolerance and engaged (or didn’t) in pre-defined coping behaviors according to the severity of any perceived upcoming food shortage in the context of that risk tolerance.  Population Simulation Model — Food Security Risk Index Outcome In conclusion, the consultancy obtained a simulation model of a heterogeneous population that reacts to potential upcoming food insecurity in ways that are more realistic at the population level. In addition, the International Civil Agency gained the ability to deliver food and cash assistance to each wealth band, according to their own schedule. The research allowed for the exploration of a variety of aid distribution methodologies which ultimately is utilized by the Agency to battle food insecurity.    Predictive Healthcare Modeling Helps Evaluate the Effectiveness of a New Medication Treatment Link:  Tags: Healthcare  Problem A pharmaceutical company wanted to introduce a new medication treatment, which would help emergency departments (ED) improve their operational performance, in terms of efficiency and time. To evaluate the impact of their new treatment on ED operations, the client hired Sterling Simulation, a consulting company, to conduct a pharmaceutical simulation. The client’s goal was to assess various pharmaceutical marketing strategies. They wanted to find out what operational gains would be obtained for the EDs, and at what cost, if EDs switched treatment from the standard of care to their treatment. To help the client answer these questions, Sterling Simulation built a pharmaceutical simulation model of an ED, using discrete-event simulation. They utilized AnyLogic pharmaceutical simulation software and its multimethod approach, combining discrete-event and agent-based simulation approaches in the model. The company chose simulation, as it allowed them to see how the new system would perform before implementing changes and making appropriate adjustments.  Solution Sterling Simulation modeled the process of how the patients were treated, comparing three different treatments. The model included three parts:  Patients — They moved through the process. Resources — They represented what the patients needed to move through the process: registration clerks, triage nurses, ED nurses, physicians, ED beds, and observation unit beds. Process — It represented the treatment process in the model.  In this pharmaceutical simulation model, the patients had three major parameters: whether they were sick, acute, or willing to leave without being seen.  Pharmaceutical marketing model scenario (click to enlarge)  When it came to the process flow, there were three major subunits:  A patient was created. The pretreatment process, where a decision was made about whether a patient needed the treatment.   The treatment process, which included both condition and non-condition treatments, and possible observation before final disposition.   After the patient was given a condition status, it was decided if the patient was either acute or not. Acute patients would get a bed immediately and wouldn’t go through registration and triage. Once patients were assigned a bed, they would no longer be able to leave before they had been treated. The treatment process itself was split into two parts, depending on whether or not a patient had a condition. Patients without a condition were sent through a generic treatment block.  Pharmaceutical simulation model statistics (click to enlarge)  With the help of pharmaceutical simulation modeling, the development team tested and analyzed three treatment options, which were different in several aspects:  The standard of care treatment  Client’s treatment Competitor’s treatment  In the model, the standard of care had low cost, moderate treatment time, but could lead to hospital admittance or the need for extra observation time. The client’s treatment had high cost, short treatment time, and always led to discharge. Finally, a competitor’s treatment had moderate cost, long treatment time, and was also assumed to lead to discharge. To gather the data to answer their business questions, the team proceeded with patient treatment simulation throughout one year. They modeled the standard of care treatment with either the client’s treatment or the competitor’s treatment each time. They sent the exact stream of patients through the flows. To evaluate the treatment options, they used the following metrics:   Total patients treated Total length of stay Total waiting time Leave without being seen rate Admission to hospital rate Resource utilizations  Results When analyzing the pharmaceutical simulation model, the team found the total number of patients treated did not change, no matter the treatment applied. This seemed strange, because the length of stay for the patients with a condition dropped dramatically, even though the actual number of patients that went into the model didn’t change. This led to the development of the throughput rate (throughput vs. total length of stay) metric, which normalized the number of patients who left the model. The model developers discovered that providing the client’s treatment to the patients with positive condition status offered several benefits:   Reduced all patients’ length of stay in the ED by approximately 10% Improved admission rates to the hospital Reduced bed utilizations  This treatment had one disadvantage – it was much higher in terms of cost than the standard of care. Although the client’s treatment was more expensive than the other two options, it enormously improved ED operational metrics for the patients with positive condition status, and also measurably improved the operational metrics for all presenting patients, if compared to

 

Section 43 of 60
the standard of care treatment. As for the competitor’s treatment, it had a lower cost, but didn’t provide the same benefits that the client’s treatment offered. Also, people in the model with this treatment tended to leave without being seen. With the help of simulation modeling and AnyLogic, Sterling Simulation built a pharmaceutical decision support tool for their sales team. EDs, on their end, could decide which metrics considered in the model were sensitive to them and decide on the adoption of the client’s treatment. For example, if an ED suffered from crowding, the competitor’s treatment would not be the best option, according to the model. Thus, the sales team, while promoting pharmaceutical products and supporting pharmaceutical marketing strategies, could demonstrate the model to their potential customer, showing how their treatment would improve the operational performance of the ED.  Project presentation by Scott Hebert, Vice President of Sterling Simulations:       Preventing “Bus Bunching” with Smart Phone Application Implementation Link:  Tags: Transportation, Road Traffic  Overview In public transport, bus bunching refers to a group of two or more transit vehicles (such as buses or trains), which were scheduled to be evenly spaced running along the same route, instead running in the same location at the same time. Dave Sprogis, Volunteer Software Developer, and Data Analyst in Watertown, MA, used AnyLogic to confirm his thesis that preventing "Bus Bunching" would improve the experience of public transit bus riders. Specifically, he proved that long waits at bus stops would be eliminated and bus crowding would be dampened by preventing minor delays to an individual bus before they snowball into the phenomenon we call "Bus Bunching." Problem Residents of Watertown, MA, have long complained about poor service on the bus routes that service the town. Knowing that the Massachusetts Bay Transportation Authority (MBTA) that serves Watertown had published an API through which real-time bus data could be collected, Dave volunteered to build a SaaS system to collect it into a data warehouse for analysis. The results of the analysis were clear, buses lost significant efficiency servicing the route when they bunched together, typically at Rush Hour when riders needed consistency and reliability most. While the results were convincing, a solution remained elusive. Should full buses skip past riders waiting at stops allowing a trailing bus to pick them up? Should bus schedules be updated to reflect dynamically changing road conditions and rider demands? Alternatively, could simply slowing buses to prevent Bus Bunching pay dividends in the long run? Solution   Bus Transportation System Simulation  Dave had a hunch that slowing buses would pay dividends but needed a way to prove it. Proving it would require simulation because the problem is not deterministic. Dave wanted to observe the impact of slowing the bus on the overall rider experience. What are the trade-offs? For example, would reducing wait times increase the ride time for passengers and to what degree? Moreover, if the ride times are increased, wouldn't the passenger loads increase as well? Only through simulation in which variables can be tweaked and results measured, could these questions be answered.  Dave modeled an existing route using AnyLogic's GIS features. The model allows Dave to simulate the current situation and his proposed solution, collecting metrics in both scenarios and comparing the results. Dave developed the model with the following components:  Bus stops.   Buses, bus behavior, and operation constraints.   Riders and rider behavior.  Trips.  The model also includes parameters that can be adjusted before and during run-time (i.e. number of riders, rider load time, rider unload time, max bus speed, and the choice between two policies).  Outcome The model allows Dave to visualize the problem and proposed solutions. The best results were seen in the solution Dave titled “equilibrium,” which is to devise a way to maintain distances between buses. Using the equilibrium policy, buses will no longer follow the route freely, but by making continued adjustments and slowing down or stopping until sufficient space in front of them is available. When the “equilibrium” policy is used, the number of riders on each bus is more uniform and the wait time is a more predictable distribution, which eliminates excessively long wait times and dampens overcrowding of buses.  Dave recommends that the MBTA implement “uber-fication” of buses – a simple “connected” app that advises drivers when to wait based on network metrics, enforcing the “equilibrium” policy. The transition from Insight to Action is often unclear. Implementation can be costly and/or risky. Simulation, where applicable, is a great middle step, refining direction and creating confidence before investments are made. Sharing the AnyLogic model with the MBTA will assist decision makers in visualizing the problem and the proposed solution, ultimately improving service for riders.    Process Simulation in Mechanized Tunneling to Optimize a Construction Site Layout Link:  Tags: Asset Management  Overview  Big construction sites in cities are usually noisy and make traffic even worse. The researchers at Ruhr University Bochum (RUB), in Germany, aimed to make these construction sites for building tunnels better. The goal was to analyze and manage the tunneling processes by using process-focused simulation models.   Problem  In mechanized tunneling, there are always two alternating core processes: excavation and ring construction. The processes for building tunnels require a large number of machine components and logistics elements (tunnel boring machine, external logistics, etc.). Disruption of one element can lead to the disruption of the entire system.     Among the constraints, there were limited storage areas above and below ground. Also, transport distances in the tunnel steadily increased as the excavation of the tunnel progressed.    Achievable performance depended on the interaction of all construction-related processes. Therefore, conventional calculation of the tunneling time was only possible to a limited extent.     To reduce downtime, extensive analysis of the advanced support processes with the aid of process simulation was required. The aim was to achieve more robust planning of logistics and maintenance processes that could also consider uncertainties, especially in the input data.   Solution  Logistical challenges can be analyzed by using process-focused simulation. The main reason why process simulation was used, was the reduction of construction time. For this purpose, RUB developers analyzed various logistics processes of mechanized tunneling in different models.    Since optimization should always be carried out with as few variables as possible, different areas of tunneling were optimized in different AnyLogic models. The model developers needed to take into account city constraints such as noise regulation, traffic congestion, and environmental compatibility.    The goal was to decrease the tunnel building area and keep construction time as low as possible. In addition, RUB wanted to understand if process simulation could be employed in earlier planning phases of mechanized tunneling construction projects.    Two key processes, that significantly determine where these areas can be located, were the supply of lining segments and the disposal of the soil.    The model was divided into 4 main agents: 2 tunnels, the soil disposal of the muck system, and the local constructs inside. There was also an agent to track the simulation progress, but this was only for easy evaluation.    To try out the different logistics variations, a simulation model was created. From the analysis of all model variations, one optimized simulation model was developed. In this way, it was possible to optimize the logistic processes of mechanized tunneling considering a number of inner-city boundary conditions.    The basic structure of the model    The agents marked in red were modified slightly for each model variation. AnyLogic simulation software enabled RUB to model the intrinsic behavior of individual agents and the interaction of different agents. Additionally, in mechanized tunneling, it was necessary to integrate fluid flows.     On top of that, 2D and 3D modeling representation in AnyLogic simplified verification and validation of the model for developers.    Simulations of segment logistics (click to enlarge)   Results  Based on the analysis of model variations, an optimized model was created which was suitable for inner-city constraints and able to show possible construction time.    In this optimized model, the disposal of muck was implemented with trains instead of trucks. The size of the storage was increased minimally. Two histograms of initial and optimized models illustrated below, show the tunnel construction time achieved in a Monte Carlo simulation with a thousand repetitions per model.    Tunnel construction time for the initial and optimized models    The top two charts in the diagram below illustrate the results of the initial model. The lefthand chart shows the total tunnelling time, which includes excavating, ring building, and inoperability (downtime). The righthand chart shows the reasons for this downtime, including regular maintenance, equipment failures, and so on. The bottom charts give the same information, but for the optimized model.     Tunneling time and downtime for the initial and optimized models    Downtime is much less in the optimized model than in the initial model. The total construction time with the initial model was 194 days on average, while the total time for the optimized model was 169.     To sum up, the reduction of the construction time was 25 days (12,9 %). This was mainly achieved by reducing the tunneling machine downtime by 40,5 %, which then resulted in the average advance rate of 17,78 m/d (an increase of 15 %). Reduction of city traffic caused by trucks was achieved by changing the method of disposal of the soil material to train transport (30 trucks a day instead of 103 trucks).    With the help of process simulation for mechanized tunneling, specialists could already compare different variations of logistics in the early planning phase. AnyLogic enabled RUB researchers to consider uncertainties in process simulation when designing the logistics processes.      Since the AnyLogic model can be easily modified, action alternatives to varying conditions can be quickly examined. Therefore, it is beneficial to apply process simulation in the execution phase of mechanized tunneling as well.    The case study was presented by Judith Berns, of Ruhr University Bochum, at the AnyLogic Conference 2022.    The slides are available as a PDF.     Production Planning in Marine Industry Link:  Tags: Manufacturing  Italy is historically known for being one of the most exclusive countries to produce yachts and super yachts across the globe. In order to gain market penetration amongst the numerous prestigious brands, cost control and rightsizing are as important as product and process innovation. The manufacturing process of luxury yachts is complex, and the quality of the final product and craftsmanship cannot be compromised. The manufacturing process requires a huge amount of time and labor for each yacht.  Each yacht uses a multitude of different highly skilled trades, performing a large number of manufacturing tasks, some of which can be completed in sequence, and others that are mutually exclusive. To add to the complexity, the process is constrained by space, both in terms of how many yachts can be in the factory at any one time (at over 60 feet, the logistics associated with moving a yacht around the factory isn’t easy), and in terms of how many people can be working on the yacht at once (it is impossible to have a horde of people working within the same hull at the same time). Problem The managers of one of the most important Italian manufacturers, Monte Carlo Yachts, needed a new, intelligent approach that would make the planning process simpler. Fair Dynamics and DSE Consulting (now SimulAi) were approached to develop a radical new tool for simulation support planning. The objective was to give the real production planner exceptionally rich planning information, which would allow the person to test and refine a plan before its implementation. The concept of the tool was guided decision making, which means that the individual can easily refine ideas and test a plan’s feasibility over multiple simulations before rolling it out to the factory. Solution The solution was a simulation based decision support tool developed through AnyLogic, using its unique hybrid approach. Discrete events were used to model the physical layout and the manufacturing process, and Agent Based were used to model the production planner’s complex and adaptive “day-by-day” decision making. This tool could easily simulate both automatic (Agent Based decision making) and human guided planning solutions as an integral part of a 3 step aggregate plan process:  Automatic, unconstrained scheduling. Human guided review of both the master plan and the resource plan, adjusting parameters. Constrained production planning, using the updated data from stage 2, to tests its feasibility.  Thanks to the efficient Anylogic Java engine, the entire simulation process of a production season only took a few seconds!  Outcome  Strong increase of resource planning process productivity. Efficient distribution of resource tasks. Human resource cost saving. Manager’s time saving. A better management support to resource allocation concerns.  Watch Luigi Manca from Fair Dynamics and Dave Buxton from DSE presenting this project at the AnyLogic Conference 2012:        Production Scheduling Software for Metal Meshes Factory Link:  Tags: Manufacturing  Eurystic is an Argentinian consulting company that helps companies continually improve their operations by using high-impact solutions to complex problems through the application of quantitative-based methodologies and tools.  They worked on a project for a large steel manufacturer, that produces a wide range of products, especially metal meshes and drawn wire.    Production diagram  During the manufacturing processes, raw materials come in and go through the rolling process, where the products can be directly prepared as drawn wire and shipped to the clients. Alternatively, it can be used to fit the bending and welding processes to ultimately manufacture metal meshes. The factory runs mostly under a make-to-stock policy, so it is critical to have a good demand forecast and a balanced production schedule to guarantee a diverse product offering.  Problem Originally, the production scheduling was carried out manually by the planning team using spreadsheets, so it

 

Section 44 of 60
was difficult to consider relevant factors involved in the production scheduling. As a result, schedules could not be strictly detailed, and overall productivity was quite low. Given the nature of the products and market specifics, customers usually search for a variety of products. If they can't find the whole range available, they will probably turn to another supplier. Due to the high machine setup time, producing high quantities of the same products for long periods of time is reasonable. But this pattern may cause storage collapse with less product variety, lack of coordination between supply and demand, and possible slowing of sales.  So, the manufacturer sought to find a production planning and scheduling software that could generate an optimized production schedule for each of the machines in the factory, and forecast the systems behavior. The main objective was to maximize service level and fulfillment of the delivery schedule (which requires a balanced product mix), while considering resource efficiency (reducing unproductive setups, buffer saturation, etc.).  To do that, Eurystic engineers had to comply two competing strategies: commercial prioritization and production efficiency. The tool had to produce a schedule with a horizon of at least one month, while considering machine calendar, productivity and efficiency, buffer capacities, transportation means availability, raw material availability, and many more. Solution Engineers chose AnyLogic as the manufacturing scheduling software because it provides a great development experience for modeling logics of every component in the system.   While developing the simulation model, Eurystic modelers used an agent-based approach to maximize flexibility and represent the non-linear interaction between the components with their individual constraints. The model could be run at any point of time and captured the system’s current situation. Most of the inputs had to be flexible enough to allow the client to try different scenarios, so the model included all the relevant parameters:  Storage capacities Transportation availability Machine calendar Machine productivity and efficiency Raw material availability Initial stock Forecasted and confirmed demand  The greatest part of the input data was taken from SAP system and processed by the model.  The most complex components (e.g., machines) included inner heuristics to optimize certain parts of the process and adapt to changing context conditions.  To find an optimized production schedule, modelers developed a custom optimization engine by applying local search heuristics. This optimization engine runs independently from the simulation model and, in fact, uses the simulation model to test different solutions.   Construction phase process for optimization engine  It triggers multiple simulation runs using a heuristic with different production strategies. After initial simulations are finished, the production schedule with the best result is selected and used as a starting point for a local search algorithm that looks for a better solution.   Search phase process for optimization engine  Result  Simulation results usage diagram (click to enlarge)  Using AnyLogic as a production scheduling software, the company achieved a 10% increase in production.  The model runs as a standalone application and reads all input data from the Excel files. Additionally, it displays all of the results as KPIs and charts and allows users to export them in Excel files as well. This allows users to carefully analyze and compare the provided solutions. The exported machine schedules are prepared in a format that can be easily loaded into SAP and launched to the factory floor. The simulation results helped the company get valuable insights to optimize existing processes:  Partially automated and greatly reduced the time required for the production scheduling generation process from several days to in a few hours or even less. Provided a way to predict and anticipate potential problems, such as warehousing saturation, and take preventive actions to avoid them. Allowed them to anticipate the raw material requirement, which can be used to schedule production in processes from other sectors of the factory. Enabled them to make important strategic decisions, for example, to assess the operational impact of manufacturing certain products or to discontinue the ones that were not profitable. Accurate forecast production and the systems evolution have enabled the planning team to improve the coordination and the communication with other departments.  The factory staff continues to embrace a new way of working that was proposed along with the implementation of the tool, so the company expects further growth. The case study was presented by Gabriel Goyheix and Maximo Lambruschini, of Eurystic, at the AnyLogic Conference 2021. The slides are available as a PDF.     Productivity Improvement of Mining Haulage System with Simulation Link:  Tags: Mining   Overview   SPb-Giproshaht is a consulting company that operates internationally and carries out design, procurement, and construction projects for the mining industry. Their flagship project in this domain is a haulage system optimization project for the mining company Medvezhy Ruchey, one of the largest miners of copper-nickel sulfide ores. Medvezhy Ruchey partnered with SPb-Giproshaht to develop mining haulage simulation models of an ore deposit that could be used for haulage system routing and mining processes optimization. Deep-deposit exploitation: global trends and objectives   To date, mineral deposits near the surface of the earth are practically exhausted. Therefore, mining companies are forced to create enterprises of greater depth and productivity. Approximately 90% of the raw minerals extracted in open pits are excavated from mines which are more than 500 meters deep.   Increasing the depth of open pits has made raw material extraction more complicated, which in turn, has required the building of haulage systems consisting of various mining and transportation vehicles. Such systems cover all operations of the production process. At the same time, transportation costs have escalated: for deep open pits they account for 60-75% of the cost of the raw materials.   In order to optimize mining operations and transportation processes, Medvezhy Ruchey commissioned SPb-Giproshaht to build simulation models of the Medvezhy Ruchey open pit mine and the Zapolyarny mine. In both cases, a mining simulation model was required to address the following logistical challenges:  Evaluate mine transportation network throughput. Verify the equipment required to haul the rock mass. Develop recommendations on equipment selection and its quantity. Confirm the specified production capacity with the given equipment fleet is being met.  The SPb-Giproshaht team applied AnyLogic mine modeling software to develop two separate simulation models.   Open pit mining simulation and optimization model Solution   AnyLogic mine modeling software allows the concurrent application of several modeling approaches when building a model. In this case, a model was developed using the agent-based and discrete event approaches.  A fragment of the load and haul optimization model in 3D  In the model, excavators are used for unearthing ore and overburden (natural rock and soil that sits above and around the ore body), and dump trucks are used to transport the ore and remove the overburden. Ore is carried to a processing plant or to the crushing and hauling facility located in the Medvezhy Ruchey open pit, and overburden is taken to dumps. Ore dumper unloading is carried out successively, while overburden dump unloading from different trucks can be carried out simultaneously.   The input data for the mining optimization model came from the maximum transport load period of the open pit and the maximum production capacity: the 7th, 8th, and 9th years of mine operation.   The transport network was based on the site layout plans for the open pit. The 3D situational plan was set up according to the mining operations database loaded from the Micromine and Geovia Surpac software applications.   The model has 2D and 3D modes. The transportation routes were developed using standard AnyLogic material handling simulation tools, accounting for terrain specificity, the location of mining and overburden faces, dumps, and other infrastructure objects.    The model allows for two types of experiments: a simple experiment and a parameter variation experiment.   The simple experiment runs a model with predefined parameters. At model startup, such parameters like the year of mining, the ore dumping spot, and the amount of equipment are set. The mining simulation model also enables the setting of additional parameters, including dump capacity, vehicle speed, and the duration of loading and unloading. At model startup, the rock transportation process is displayed. The results are presented in graphs, diagrams, and dynamic text: the number of vehicles involved, the total ore and overburden excavated (during shifts and for the entire simulation period), and other characteristics of the transportation process.  The parameter variation experiment enables the user to evaluate the type and the degree of influence of certain parameters on model behavior. Users choose the parameters needed and set the number of automatic simulation runs for which to vary the values of the selected parameters. The experiment results are displayed in diagrams showing the dependency of model efficiency on the varied parameter. For example, this experiment provided insights on how the transported overburden volume is related to the number of dump trucks. It helped define the minimum quantity of dump trucks needed to provide the required performance.   Result   The open pit optimization model allowed engineers to:  Calculate traffic flows and the maximum capacity of the open pit road network in order to prevent congestion. Confirm that the entire extracted rock mass can be transported to the processing plant and overburden dumps. Using the parameter variation experiment, calculate the required number of dump trucks more accurately and reduce the quantity of required dump trucks, depending on the type of rock mass and total quarrying time.  Zapolyarny underground mine simulation model Solution   Mining processes in the Zapolyarny underground mine are characterized by a number of features. For example, the mining operations generating the spoil tip operate on a closed transportation network and dump trucks inevitably interfere with each other when navigating the network. Also, with advancing field development, the position of spoil tips changes along with the locations of the raw material extraction. As a result, at various development stages, the dump trucks should be loaded in different places, which results in numerous haulage routes.   These features make the rock mass transportation process non-linear. When developing the model, a discrete event approach was chosen to reflect the listed features. The AnyLogic Material Handling Library provided components that reflected how vehicles moving in a transportation network could influence each other.  Transport network model  The model consists of “Path” and “Nodes”, standard AnyLogic objects displayed on a scaled raster background with existing and planned mine constructions. At specific points on the transportation network, components determine where the dump truck loading and unloading points are situated. The simulation and the parameter variation experiments are also available in this model.   When conducting the simulation experiment, the mining productivity improvement model runs with parameters that determine dump truck operations during one working shift. The minimum time interval of the transport network operation is one calendar month. Each month corresponds to a certain area of mining operations, while the process of the rock mass transportation remains unchanged. The model requests information from the database about the areas where the operations are being performed for the specified date and activates the corresponding loading points in the transportation network. Taking this data into account, the routes for the dump trucks are built and accidents are monitored and eliminated.   For the parameter variation experiment, the year and the month are set as variables, while the following settings act as parameters:  The number of dump trucks The weight of the rock mass transported per trip The speed of the dump truck (loaded and empty) The loading and unloading duration The dump truck working times during the shift  Results are displayed in a histogram that shows the ore production volume for each month of the given time frame. Results   The underground mine model assisted engineers in calculating the equipment quantity required to achieve the desired performance for different years. Based on simulation results, it is planned to increase the number of dump trucks in 2018-2030 and in 2033-2047 by two units, and in 2031, 2032, and 2048 the number will be increased by one unit.    Rail Yard Capacity Modeling Link:  Tags: Rail Logistics  Client            Aurizon is an Australia’s largest rail freight operator, managing more than 700 locomotives and more than 16,000 wagons. Aurizon is widely engaged in coal, iron ore, and mineral transportation. The company is the world’s largest rail transporter of export coal from mine to port. In order to increase operational efficiency the company decided to move one of their rail yards to other town. This rail yard was mainly engaged in wagon and locomotive maintenance and locomotive preparation.   Problem            Aurizon approached Evans & Peck consulting company with the following task:    To determine the capacity of a rail maintenance yard and if additional roads were required when additional services were moved to the yard.  To develop the model in a relativity short time when compared to current modelling methods used at Aurizon.  To build a model which can be reused in a larger network model if required.   Solution            The consultants employed AnyLogic’s Rail Yard Library for simulating operations within the rail yard (including locomotive preparation, locomotive maintenance, wagon maintenance, reliability examinations of trains, wagon stowage, locomotive stowage and passing traffic). They had to construct the model in a modular fashion to allow components to be reused and the yard model to be connected / incorporated into other models.            The model permitted the testing of the following:     Train configurations Train movements within the yard Track utilisation Facility utilisation Stowage times within the yards Scheduling of activities Impact on mainline services            The model is a good demonstration of AnyLogic’s ability to build complex models without extensive Java

 

Section 45 of 60
coding.   Results            The model was used to determine:    If additional roads were required within the rail yard If operations at surrounding yards could be moved to the yard in question What effect the shifting of these operations had on the Train Master Plan How activities can be coordinated better within the yard            The model was constructed in a way that allows it to be placed into a larger network model, which Aurizon is developing with the assistance of Evans & Peck.            Watch Martin van Holten from Evans & Peck presenting this project in detail at The AnyLogic Conference 2012 or download his presentation:       Railway Maintenance Yard Design and Simulation for An Australian Railcar Maintenance Provider  Link:  Tags: Rail Logistics  Problem When carrying out rolling stock maintenance, companies face various challenges. For instance, if a maintenance facility’s total load increases, it might lead to further disruptions in working processes. To address the issues, including traffic congestions and fleet mix inefficiencies, and at the same time maximize throughput, railway yard redesign may be needed. For this purpose, the current facility layout may be simulated to later analyze its malfunctioning parts, and then test innovations in the model to make the layout more efficient and sustainable. Advisian, an independent consulting arm of the WorleyParsons Group, with subsidiaries in 19 countries, was tasked with modeling one of Australia’s largest rail maintenance centers. This center is used for the planned maintenance, repair, and stabling of both regional and suburban rolling stock. The depot suffered from considerable congestions and the managing company wanted to find out the reasons and work out the techniques for congestion elimination. The managing company also wanted to test train depot design at its maximum capacity, taking into account infrastructure constraints and optimize fleet mix.  Advisian’s role was to develop a railroad simulation tool which could be used to:  Determine the overall efficiency and effectiveness of the depot. Analyze the effect of new fleets on the facility. Identify constraints on operations. Test the impact of depot infrastructure changes.  With simulation, Advisian was able to create a digital copy of the maintenance facility and its operations, analyze its dynamics, and test different scenarios of improving the workflows in the depot. Solution  Maintenance facility layout simulation (click to enlarge)   Advisian chose AnyLogic rail yard management software to simulate the depot operations. The company made extensive use of the AnyLogic Rail Library, a toolkit for detailed simulation of railyard operations. It not only provided the ready-to-use objects with predefined logic for maintenance facility simulation, including railcars, locomotives, and rail tracks, but also allowed for creating realistic 2D and 3D animation for the model.  The facility was utilized by different train types, which is why the simulation team mirrored the functioning of fleet mix in the model, identified how the processes inside the facility were changing in real time, and gained insights concerning possible structural changes.  Railway yard design simulation (click to enlarge)   The simulated operations were based on data from actual train timetables. AnyLogic data input capabilities also permit the connection of a model with a database and feeding the model in offline mode, or with real-time data. The detailed model also helped uncover hidden interactions to gain an understanding of the railcar repair yard processes and improve their efficiency. The major insights, however, were driven by model statistics. It helped capture the behavior of the simulated system and analyze indicators, which were important in the depot’s functioning. For instance, for congestion avoidance and other purposes, it was vital to understand the utilization of railways in certain areas of the depot. This led to gathering utilization statistics per each area and displaying it on graphics for further analysis. The statistics data could then be easily exported in Excel format.   Statistics from railyard design simulation model (click to enlarge)   Outcome With AnyLogic, the simulation team was able to identify:  The depot’s maximum capacity, considering infrastructural and process constraints. Bottlenecks within the facility’s current layout. They were the cause of significant congestion across the facility.  Simulation allowed for a deeper analysis of depot operations, as well as for implementing, testing, and assessing the effect of potential changes in both the fleet mix and the facility layout.    Reducing the Misinformation Effect by using AI Systems and Computer Simulation  Link:  Tags: Social Processes  There is a lot of misinformation that exists regarding COVID-19, especially on social media sites. It is important to understand where this misinformation comes from and how to understand it in order to overcome it. The Norwegian government decided to track emotion and misinformation on social networks, especially Twitter.  Problem There is a huge amount of data that exists online and collecting it, sorting through it, and providing an analysis and solution is time-consuming.   ALAN Analytics needed to collect a lot of data that was normally considered as qualitative and then integrate it in quantitative ways using a data pipeline integrated with AnyLogic simulation. This could help policy makers to understand the spread of misinformation and emotional contagion, and how it affected human behaviors during the COVID-19 pandemic.  Solution ALAN proposed to use a multi-agent behavior simulation AI model. This is an agent-based model, where the systems inside of each agent are specifically programmed to try to be as psychologically realistic as possible or as required for the actual application. This approach was chosen to look at COVID from the level of human society. Three questions they dealt with:    Who was going to follow government regulations?   When was misinformation more engaging in order to copy those patterns and get an edge on social media information overload?   What effects did anxiety have on political beliefs and identities of individuals, and how that might affect misinformation and the motivation to follow regulations?   Structural equation model backbone of the architecture at the agent-based level (click to enlarge)   In the behavior simulation model, there were three key inputs and outputs – behavioral, psychological, and digital. Behavioral data came from survey data. Digital data was taken from online sources, such as Twitter. Psychological data was a combination of this survey data and digital data.  The survey was done over two COVID waves to understand changes over time. Questions about people’s beliefs regarding misinformation showed two clusters of information and this was integrated into the model.  One cluster showed people who support misinformation or conspiracy theories, and the other illustrated those who do not. Both clusters of information tightened from the first to the second survey. This was used to try and better parameterize the behavioral aspects of the simulation with regards to reactions on different policies.  In the model, semantic networks were used to represent personal and social thoughts and behaviors, as well as personal and social identities. Slight differences in social identities could be reconstructed using AnyLogic   multimethod modeling, adding   system dynamics over time with the agent-based model.  Data was analyzed from Twitter to get information about personality, concerns, and fears especially connected with political leanings. The analysis was performed using CulturePulse API in all languages, including emoticons and then imported into AnyLogic. It was important to note that the CulturePulse Pipeline is not just a machine learning pipeline – it uses both a lexical and machine learning approach.  The benefit for ALAN of using AnyLogic was that the simulation was taken care of by AnyLogic, and they also had statecharts available from some of the models in AnyLogic. Essentially, they could bring over the data analytics from CulturePulse and build a single data pipeline for creating a psychologically realistic multi-agent AI model.    Data pipeline connecting with AnyLogic    Results The most significant result from this study was that different geographic regions have their own cultural characteristics. Misinformation in Scandinavia can be different from what is misinformation outside of Scandinavia.   In fact, trying to apply something that might work in the Middle East or North America, could have the opposite effect in Scandinavia. Therefore, there is no global way to combat misinformation. The campaign to counter misinformation must be culturally attuned.  In the future, there is a possibility to try widen this platform and use it to understand how to implement new policies that could reduce the threat of political violence. This approach lends itself well to agent-based modeling and the kinds of predictions that can be made using this type of modeling.  The case study was presented by Dr. Justin E. Lane, CTO, ALAN Analytics, at the AnyLogic Conference 2021.  The slides are available as a PDF.     Research of Traffic Flow in the Transport Hub of Volokolamskaya in Moscow Link:  Tags: Road Traffic  Problem The Moscow subway station and transport interchange, Volokolamskaya, is situated in the northwest part of town. The main road traffic in the area comes from the neighboring districts and the suburban parts of the Moscow region. ITS Consulting was contracted to test the efficiency of the planned roadway network during the peak traffic hours (morning and evening) with due regard to perspective increases of traffic within a timeframe until 2025. Solution  The company decided to build a road traffic simulation model that would include public and private transport of various sizes and movement logic: signalized junctions with various sets of lights, unsignalized junctions with "main road" and "give way" signs, uncontrolled pedestrian crossings, and bus stops with several boarding/debussing lines. For modeling, the consultants used AnyLogic Road Traffic library that was in the preview mode at this time. The simulation model was used to test possible scenarios of road network development within the timeframe until 2025.  The modeling process included three stages that you can find in the table. Users can work with the model in the interactive mode by editing and tuning such parameters as:  Traffic flow density. Signal phase duration. Probability of stopping at parking lot. Ratio of buses and fixed-route taxis.  Traffic Congestion Areas  Results At stage one, the model showed the traffic-congested area. The two exposed bottlenecks were located in front of signal lights and had the queue length of 65 and 80 vehicles in a lane. The running model at stage two revealed that with perspective traffic growth, the situations at traffic-congested areas will go from bad to worse. The traffic flow will grow from 65 to 80 vehicles in section one, and from 80 to 140 vehicles in section two.  Based on simulation modeling results, several measures were suggested to the customer to improve the roadway system performance. In both sections, the consultants offered signal phase optimization, adding a lane and a bay. In addition, they proposed redesign of the entrance to the parking lot in the section 2. At stage three, a new model was built to reflect the reconstruction plans and perspective traffic flow. The running model showed that road system capacity will increase more than twice within 10 modeling years (until 2025). At section 1, the queue length will be 25 vehicles in one lane, and at section 2, this queue will be 20 vehicles in one lane. The road traffic simulation model allowed the client to officially approve the network reconstruction project.    Resource Optimization in the Oil and Gas Industry Project Management Link:  Tags: Oil & Gas, Asset Management  Overview YPF is the largest oil and gas company in Argentina, with a 43% market share in oil and gas production, and 58% in gasoline. As the third largest company in South America, YPF employs 72,000 people directly and indirectly, and holds 92 production blocks and 48 exploration blocks in basins around the country. Problem  Project Management: Distribution of Tasks.  YPF was aiming to reduce its costs associated with oil wells’ maintenance downtime and equipment breakdown. The analysis showed that the root cause of inefficiencies was the lack of a robust maintenance scheduling process. The main problem was that the scheduling process was decentralized, with planners independently allocating hundreds of work orders each month. Multiple planners were assigning tasks for multiple teams without proper coordination between each other. This prevented them from creating optimal schedules, resulting in downtime losses and inefficient resource utilization. YPF approached Simcastia, the simulation and optimization business unit within Continente Siete (an Argentinian business data science company), to develop a tool for scheduling, resource optimization and project management at all YPF facilities. The pilot project was carried out for Rincón de los Sauces, an oil field located in Neuquén, Argentina, with 700 wells (including water injection and oil wells), about 100 working crews, and more than 100 weekly maintenance orders.   System interface: resource availability screen.  Solution To manage an oil field maintenance system that included many interacting parts, custom policies, constraints, and time-dependent events, using spreadsheets and analytical optimizers was not sufficient. Simcastia consultants developed a simulation-based optimization solution to manage this complexity. They chose AnyLogic simulation software for its unique flexibility, which allowed them to model specific resource behaviors and custom process rules. The simulation model included sites with GIS-referenced locations, preventive and corrective work orders consisting of multiple tasks, and resources (staff and equipment) with skills and working hours. All of these elements were modeled as agents with unique properties and behavior patterns. The costs calculated in the model included:  Wells’ production loss costs, including unplanned and scheduled interruptions in operations.  Resource-related costs (both regular and extra work hours). Travel-related costs.   The solution developed by Simcastia was based on an AnyLogic maintenance process simulation model and custom optimization algorithms. The simulation-based resource optimization used algorithms to allocate resources to work orders and complete these work orders the fastest possible way. The interface of the software solution allowed the planners to adjust model parameters, such

 

Section 46 of 60
as associated costs, weather conditions impeding some processes, resource availability timetables, and prioritization rules. By changing model parameters, the planners could feed the model data relevant to the changing environment. Combining simulation with optimization, the tool produced operational plans for 9, 12, and 30 days. It also provided various statistics displayed in dashboards, including operational plan details, schedules by   and site, costs and tasks by type, resource utilization rates, extra hours worked, and distances covered.    Project Management System Dashboards: Resource Optimization and Statistics.   The resulting solution was integrated with the client’s databases and SAP, becoming the part of the company’s planning software infrastructure. Results This project provided the Rincón de los Sauces oil field with a project management and decision-support tool for maintenance scheduling, which helped improve the site’s operational efficiency and resulted in:   Work order execution increased by 11%. Preventive maintenance fulfillment increased to 95% in six months.  Corrective maintenance backlog reduction of 56%.  Unplanned downtime oil production losses reduction of 50%.   The direct economic impact of the project included annual yearly savings of $18M at Rincón de los Sauces. As a second stage, implementation for Mendoza assets (much bigger and more complex) have already started, and the roadmap aims for nationwide implementation by the end of 2018, with expected savings of $234M per year. To learn more, watch the project presentation at AnyLogic Conference 2016 or download it.     Sales and Marketing Automation System Rollout Link:  Tags: Business Processes, Marketing  Overview Vendasta is a Canadian-based technology company. It helps small and medium businesses to manage different digital channels. Vendasta provides a platform where local experts (digital experts or IT companies) can get marketing and sales tools, outsourcing services, task tracking, invoicing, and business training.  Problem Vendasta created an automation system to minimize the manual work of both its customers and its internal teams. Automation systems are powerful but can quickly overwhelm a company's infrastructure if something goes wrong. Vendasta needed to understand the following before releasing the feature:   How many users will there be?  How long will it take those users to adopt automations? Are those users internal Vendasta teams, Vendasta's customers, or both? Is there a risk that the automations will rapidly activate paid features or overwhelm Vendasta's infrastructure? Are there benefits or costs that Vendasta missed and should explore further?  Vendasta needed a way to quickly, yet carefully answer these questions. The company also didn’t know how the platform’s user profile would look like. Vendasta had only predictions but needed qualitative research.  Solution Vendasta developers built a system dynamics model with AnyLogic software to explore different scenarios. The modelers designed several diagrams in the model including partner adoption and retention, feature uptake, automations running, etc.   Partner adoption and retention  System dynamics modeling gives the opportunity to observe a lot of dependencies and make the right findings for a real system application. Simulation with AnyLogic is good for what-if analysis and risk management even without historical data.  The AnyLogic software supports the design and simulation of feedback structures, such as stock and flow diagrams, and array variables (subscripts). Frequent patterns may be saved as library objects and reused within a simulation model or across different models.   The system dynamics model of the platform rollout (click to enlarge)   The company analyzed how many partners would use the platform and how many automations these partners would create and run. Vendasta divided partners into "good-fit" (9-15%) and "poor-fit" (1-15%) and explored the adoption scenarios.  Vendasta wanted to include in the models all stakeholders: development, support, sales, and R&D teams. The modelers looked at processes in the sales and support teams to determine how much work could be avoided by using automation. The R&D teams used automations to implement features, so it was important to know what infrastructure and time they would need to accomplish it.  Results The simulation modeling outputs related to external automations were almost the same as the initial predictions. For example, the quantity of running projected automations was 350, and the simulated value was 361. The number of projected automation users was 400, while the actual amount was 319. The difference in both cases was only around 3%.  The outputs of the model confirmed the predictions  Vendasta also considered the benefit of internal automations. The company managers realized that even if the internal automations didn’t save much time or money, they could affect employee satisfaction and productivity. For example, the sales team had daily routine work which automations could remove.  Also, using the simulation results, Vendasta analyzed the work of the support team and prepared specific detailed documentation for their needs. This might improve the quality of employees’ lives and their performance in the company.  Watch the video about the case study presented by Vendasta at the AnyLogic Conference 2021:      Scheduling for Underground Mining and Construction Projects Using Simulation Modeling Software Link:  Tags: Mining   Problem        When planning for complex underground works, whether it is in construction or mining, specialists have to consider many factors and their impact on the whole project. Such works are characterized by limited operating resources and space, as well as close interconnections between processes. As a result, specialists inevitably face difficulties when developing schedules.       The capital construction project management company IBCON was tasked with creating a new scheduling method for underground mining works. Using simulation modeling software, they developed scheduling technology that considered:    Unexpected occurring organizational and operational constraints (e.g., a certain time limit when the work should be completed, lack of machinery). Competition over operational resources (e.g., competition for machinery in case of its breakdown). Resource performance fluctuations (e.g., the machinery’s performance can decrease if the rock formation drilling speed goes down).  Solution        To solve this scheduling problem, IBCON engineers had to change the approach to scheduling. Usually, underground construction projects are managed via Gantt charts, which schedule events that are known in advance. However, every project is subject to uncertainties. For example, managers may know about potential problems but be unaware of the time and probability of their occurrence. To understand when such events would happen and their impact on the work schedule, the IBCON engineering team opted to use simulation modeling software.   2D Ore Mine Model              (click to enlarge)  3D Ore Mine Model              (click to enlarge)       First of all, using the input data and AnyLogic’s capabilities, the team developed and visualized a simplistic ore mine simulation model. The developers used the elements of the Fluid Library and the Process Modeling Library to describe the rock volume, the mining process, and the process of rock transportation to the surface. With the agent-based modeling capabilities, the team set the operation logic for each element of the underground mining system, including:    Warehouses, which also served as base locations for transporters and loaders. A garage (where self-propelled drilling rigs and load-haul-dump (LHD) machines were located) and stopes (excavations in a mine from which rock is extracted). Storages for rock from the stopes and whether the rock was transported to dry backfilling or mine car loading areas. A car dumper point, a bunker that received rock from the cars, etc.       Moreover, in the ore mine model, the IBCON team considered the amount of rock that was mined, workers and equipment availability, car speed, conditions for aligning work in several stopes, transportation machinery loading factors, time between equipment maintenance, winding intervals, etc. Together, modeling these considerations made the model highly accurate.   Experiments with the underground mine model        After developing, running, and debugging the model, the developers performed the parameter variation experiment to see how the project completion time would change.   Mining process at one of the stopes       In the first experiment, the IBCON team wanted to determine if the general project implementation schedule would work with the initial parameters of machinery, equipment, stopes, and other elements of the underground mining system. As a result, the team found out that the project’s turnaround time could be reduced by three months. However, certain types of work would need to be carried out for longer than initially scheduled.        The goal of the second experiment was to minimize the project’s project turnaround time. The developers wanted parameters that could speed up work as much as possible without compromising its production and safety. As a result, they were able to reduce the turnaround time by seven months.        In the third experiment, the IBCON team considered a case where some initial parameters had been wrong, drilling rigs and LHD’s failed regularly, and the rock from one of the stopes had to be moved to a more distant dry backfilling area. The result of the experiment showed that the work completion time took one and a half months longer than scheduled. Following this, the IBCON team conducted a factor analysis based on the experiment results and determined the most subversive factor so that its influence can be minimized in the future.        As a result of experimenting with the model, the developers produced a log with the timings for events. Using this data, they drew up a cyclogram, a monthly schedule by day, and a Gantt chart, which had different levels of detail for work planning.        Engineers could update the model data to keep it up to date as events unfolded. They could then run the model again and use the results to adjust the work schedule. They could repeat the process as many times as needed until the underground mining project was completed.   Result       While developing this solution for underground works scheduling, the IBCON team built a simulation model for broken rock extraction and transportation at an ore mine. Based on the modeling results, the engineers made:    A project schedule for the entire underground construction period. A monthly schedule with a daily work plan. A cyclogram with a three-minute step.       At the same time, they used the AnyLogic model to analyze how various factors affected turnaround time compared to the initial project schedule. This took into account uncertainties and helped in choosing the most appropriate strategies for the underground mining project.        The technology developed by IBCON is applicable in every industry dealing with tunneling work. Simulation modeling is a flexible solution that enables the calculation of the dynamic pace of stope construction, the creation of utilization schedules for each piece of machinery, the optimization of workloads, testing of “what-if” scenarios with varying parameters, and more. The ability to understand and forecast activities is a powerful support tool that helps inform management decisions.      Selecting the Best Inventory Policy Using Gojii Link:  Tags: Supply Chains  Problem Product inventory and WIP decisions are important sources of leverage on bottom-line results. Inventories and WIP represent large investments put at risk. These investments ripple through to the P&L in the form of direct costs, revenue impacts, and business growth. Supply line investment, an operational decision that is made every month or quarter, compounds result on the P&L. This important decision is typically addressed through "ad hoc" processes because there are few policy tools designed to support this management decision. Existing supply chain and S&OP tool sets do a great job of managing supply to meet a selected "forecast." However, there is no single "correct forecast" of future demand, and existing tools are not designed to select the best demand level for the business. There is a "tool gap" between forecast inputs and selection of the best demand signal (aka "forecast") to drive your S&OP system. Solution Gojii fills the tool gap with a scenario based planning tool that helps stakeholders select the best demand signal to drive the S&OP system. Gojii works by identifying the inventory and build policies that will position your product line to take advantage of opportunities while protecting against adverse consequences.   Place of Gojii in Supply Chain Analytics A system simulation model using AnyLogic Simulation and Modeling Software is the heart of the Gojii tool. The model includes information about supply physics that capture many structural and inventory control dynamics. "The AnyLogic Company provides the perfect tool for this situation -- their "multi-method" AnyLogic simulation engine, which we (DecisioTech) use in all our projects. The Gojii simulation uses components based on discrete-event methods, system dynamics approaches, some agent elements, and bits and pieces of Java code. This all comes together seamlessly in the AnyLogic simulation engine leaving the modeler free to worry about solving the problem rather than managing the simulation technology," describes Lyle Wallis, President at DecisioTech. Using Gojii, powered by AnyLogic, DecisioTech models the interaction of the supply system with the market. Gojii captures market feedback as part of the cost curve calculation and generates a profit denominated risk-reward trade-off visualization for use by decision stakeholders to choose a particular strategy.   How the Decision Support System Works  Outcome The Gojii tool, powered by AnyLogic software calculates a cost curve whose inputs include a range of market scenarios and alternative supply policy options (build rate, line loading, safety stock, etc.). A simulation for every set of scenarios is run and captures material flow, simulated cash flow, time series data, etc. The outcome is then visualized making it easier for stakeholders to select the policy that makes the most sense for their firm. The cloud based implementation enables companies to use Gojii via a browser, requiring no software installation. Ultimately, Gojii allows various stakeholders who have different opinions to collaboratively explore and analyze market scenario data.   Learn more about Gojii by watching Lyle Wallis' presentation at the AnyLogic

 

Section 47 of 60
Conference 2013:     Shaping Healthcare Policy Using Simulation Link:  Tags: Healthcare  Overview An initiative by the Department of Mechanical and Industrial Engineering at the University of Toronto, the Centre for Research in Healthcare Engineering (CRHE), was in response to the immediate and compelling desire for efficiency and quality improvements in the Canadian healthcare system. Problem Policy makers were motivated by the overall ranking of healthcare in Canada. A study from 2010 showed low rankings in quality of care, effective care, coordinated care, patient centered care, timeliness of care, efficiency, and equity, in comparison to Australia, Germany, the Netherlands, New Zealand, and the United Kingdom.  In order to test and visualize policy changes and other possible solutions, the CRHE planned to build a decision support tool to observe behavior and select appropriate policy changes that could ultimately increase life expectancy by increasing access to healthcare, increasing patient satisfaction, and changing the perception of healthiness. Solution AnyLogic simulation modeling was chosen to build the decision support tool due to its multimethod modeling capabilities. System dynamics modeling was used to study aggregate behaviors (interactions between major groups), and agent-based modeling was used for adaptive behavior.   Healthcare System Model Structure  The model included descriptive data from the Irish healthcare system codified through content analysis, and quantitative data from the Central Statistics office of Ireland, OECD health statistics, and Eurostat health databases, which were codified through statistical analysis. Data from Ireland was used because the country experienced substantial strategic shifts, external economic shock, and could provide extensive, transparent documentation. The documentation from the healthcare systems was compiled, run through a content analysis process, moved through a UML for structure, and plugged into AnyLogic as a platform to build the model and run scenarios. The model structure of the healthcare system included elements, goals, and strategies. Elements included patients, physicians, other clinicians, hospitals, clinics, and corporations, including insurers and regulators. Goals were outcomes such as life expectancy and mortality, access to healthcare, and other determinants of health, as well as subjective outcomes such as satisfaction and perception. Strategies and policies that were considered included:  Insurance- equity in accessibility  Sectoral- primary care and elder care  Capital investment- facilities and technology  Regulations- gatekeeping and models of care Governance- public/private care and mergers  Results The ability to visualize the large amount of data and derive patterns was the greatest benefit achieved from this project. Variables such as life expectancy, expenditures, consumption, etc., could be plotted and observations could be made dependent upon country. In future research, the CRHE will test policy change scenarios and work to improve the healthcare system of Canada and other low ranking countries. Watch the presentation of the project by Neil McEvoy from CRHE:      Shipyard Capacity Analysis Link:  Tags: Manufacturing             As the West Coast’s largest new construction shipyard, General Dynamics NASSCO is a market leader in the design and construction of ships for the U.S. Navy and commercial customers. Playing a key role in maintaining this strong presence in the U.S. shipbuilding industry is NASSCO’s emphasis on pre-production planning and analysis. With hundreds of thousands of parts flowing through the shipyard each year, however, traditional spreadsheet analysis methods can only provide modest assessment of how those parts interact with labor, machinery, and workspace to govern NASSCO’s overall shipbuilding throughput and capacity.      With AnyLogic simulation software as the centerpiece, NASSCO utilizes a custom-built analysis system called the Large Scale Computer Simulation Modeling System for Shipbuilding (LSMSe) to provide highly detailed and accurate capacity analyses for both current production and potential new work. Developed over a four year period, the LSMSe includes:    A simulation model of the entire shipyard modeled in AnyLogic Software utilities to define facilities, processes, schedules, labor, and products Links to product design, scheduling, and business systems software Optimization software to control parallel scenario analysis over networked computers           Implementation of the LSMSe has reduced NASSCO’s recurring analysis labor costs by 75% compared to the former spreadsheet analysis approach with increased detail in the analyses. While significant, an even greater impact is achieved with the ability to identify and eliminate bottlenecks across the integrated shipbuilding system, develop optimum labor assignments across the shipyard, and “right-size” facilities to meet production demand with appropriate capital investments, potentially saving millions of dollars with implementation of the analysis results.            AnyLogic’s object-oriented approach to model building and use of the JAVA programming language were instrumental in meeting the design goals of the LSMSe, providing a modular framework for ease of maintenance and future growth, as well as, supporting the programming capability to model and simulate the hundreds of thousands of entities and resources defined in each analysis scenario.    General Dynamics: Capacity Analysis with AnyLogic.pdf (221.61 KB)     Simple Simulation Model Helps Intel Avoid Production Plant Downtime Link:  Tags: Supply Chains, Manufacturing  Based on revenue, Intel is one of the world's largest and highest valued makers of semiconductor chips.  Intel factories used a particular type of equipment that often broke down, which caused capacity constraints. If the equipment was repairable, it would be fixed by Intel factory technicians or, if the problem was too complex, it was sent to the supplier’s repair center. These expensive parts were used in critical factory operations, and the repairs took significant time, so it was necessary to have extra spare parts on hand to avoid downtimes. Broken parts caused constraints at some of the factories while other factories over purchased spares.     Broken part repair loop at Intel factories   Intel’s commodity managers worked to determine the root cause of the problem and ways to solve it. They needed data to understand how many spare parts the company needed to keep the factories running without overbuying them.  To support the negotiation process with the equipment supplier, they built a simple simulation model. The model was a standalone application with a user interface that could be utilized to input various parameters and test “what-if” scenarios.  The model helped the managers understand how many spare parts had to be purchased to avoid equipment downtime due to lack of these parts, as well as to avoid significantly overbuying the spares, while taking into account various failure rate scenarios.  As the repair centers sometimes got overwhelmed with incoming orders, the managers were also able to understand how changing local and at-supplier repair center staffing and capacity helped solve the problem.  This simple model was built and used for only a few days. It was used to support negotiations with the equipment vendors to convince them to provide Intel with additional spare parts’ consignments at no cost. This allowed Intel to achieve significant savings with little effort.      Simulating Bicycle Sharing System with the Help of AnyLogic System Dynamic Modeling Link:  Tags: Social Processes  Preface           Sustainable bicycle transportation systems have generated broad expectations and have recently attracted focus in the scope of urban planning by alleviating traffic congestion and reducing noise and air pollution. Such bike sharing systems are widespread and easy-to-use. Customers arrive to the bike stations, take a bicycle for a short amount of time and return it at the same or another bike station. The bikes are mostly used without the commitment and the responsibility of owning it. These systems are generally promoted by the public administration and operated by advertisement companies, transportation operators and, and public organizations.   Problem   In Mexico such systems exist, although they are rather underdeveloped for big cities. That is why this study was applied to the city of León, Guanajuato, the fifth biggest city in Mexico. After dividing the city into 23 macro-areas to suggest the number of people who will use the applied system in each area, the developer tested the model on big macro-areas. Three danger-prone zones were primarily excluded.            Modeling helped not only define the specificity of each area, but also how people would behave while using a bike sharing system. The study was performed with the assistance of Monterrey Institute of Technology and Higher Education (ITESM).   Solution                           The project was designed using a system dynamic approach. At the first step, the developer used a dynamic simulation model for evaluating and determining the ideal number of bicycles and identifying the stations with the highest occupation levels. The government plan for improving bicycle infrastructure was used while planning input parameters, as a municipal committee had targeted to build 220 km of bike paths by 2030.                            After determining the factors that measure the environmental impact of the bike sharing system, the developer dimensioned it using a dynamic system perspective to define its weaknesses, capacity and associated costs. The input parameters for the model included:   Initial capacity of bicycles per each station Number of parking slots Bikes per station by the end of the day                           The first parameter was especially significant for the busiest stations because the absence of bikes or slots for bikes’ parking would affect negatively on the loyalty towards a bike-sharing system. Among the fixed parameters were a simulated 16 hour day, travel time, and average speed (15 km/h). Three scenarios were tested with expected, high, and low demand. Simulation tools in AnyLogic helped improve graphic animation, while multi-method modeling provided model’s high flexibility.   Outcome                            The experiments with model helped:   Analyze the number of different demand scenarios Identify the busiest stations in the system and utilization of each station Visually display performance metrics Define the number of bikes at each station                           The two northern parts of Leon, both student areas, would be then chosen for the system’s implementation.   Project presentation by Karla M. Perez (ITESM)     Simulating Delivery Operations for a Retail Company Link:  Tags: Supply Chains, Transportation  Overview Lojas Renner S.A. operates all over Brazil, as well as Argentina and Uruguay, in the segments of fashion and home decoration. Its fashion and lifestyle ecosystem is formed by the brands Renner, Camicado, Youcom, and ASHUA. Combined, there are more than 600 stores in operation. Renner is the largest fashion retailer in Brazil in sales.  They deliver a high-quality experience and implement a standard of excellence in fashion and lifestyle to the medium and high segments with an innovative and sustainable ecosystem. Lojas Renner S.A. includes a financing institution, Realize CFI, which supports the retail by offering and managing financial products.    Lojas Renner ecosystem   The objectives regarding omnichannel operations were to improve inventory accuracy and assortment availability, to make faster and cheaper deliveries, etc.   Problem  The delivery operations included the management of a few large distribution centers, as well as retail stores. They dispatched packages from distribution centers and retail stores via third-party couriers. Therefore, the company had high costs and a long lead time. Another issue was that e-commerce order tracking depended on the couriers.  Lojas Renner S.A. planned to change the delivery operations by managing dedicated last-mile couriers, using retail stores as transit points for last-mile delivery, and standardizing e-commerce order tracking.  While designing the last mile operations, the company needed to answer the following questions:   How to use retail stores as transit points?  How many couriers should be used per transit point?  What is the cost of these operations?  How long will the couriers work?  How to account for failed deliveries?  How to handle special sized deliveries?  How to simulate the operations considering actual delivery routes?   Lojas Renner S.A. was not able to find the answers without proper logistics software. Deploying such software would give them an advantage in the market. The company assumed a fixed set of stores as transit points, a fixed coverage for transit points, and regular transfers of packages from the hub to transit points.  They aimed to simulate random delivery locations, time periods, and failures. Each transit point had fixed couriers.    The delivery routes   The challenge was to simulate the delivery routes to random package destinations. At the end, they needed to solve vehicle routing problems (VRP) for each depot.   Solution  Lojas Renner S.A. designed a general courier workflow that they used in simulation.    General courier workflow   After loading packages, a new route must be defined for each courier.  There was a probability of failure for each delivery. Also, the time to deliver followed any chosen probability distribution.  When the route was finished, the courier had to return to its depot only if there were still available packages to be delivered, or if there was any failed delivery.   AnyLogic could easily simulate the last mile operations for given routes. For these needs, the retail company used agent-based simulation modeling and the Process Modeling Library. The model developers used histograms for visualizing statistics. GIS features were very intuitive and easy to set up by engineers. Moreover, AnyLogic could be easily expanded with Java that allowed Lojas Renner S.A. to use Java based, open-source toolkit jsprit for resolving vehicle routing problems.   The AnyLogic simulation model (click to enlarge)    Results  One of the modeling outputs showed a lack of couriers at a depot, explaining why some deliveries were not completed. Hence, Lojas Renner S.A. needed to increase the number of couriers there.  AnyLogic provided various modeling alternatives for the same problem, so the engineers just needed to focus on the company’s final goal to find the best solution.  Watch the video about the case study presented by Lojas Renner S.A. at the AnyLogic Conference 2021:       Simulating Ice

 

Section 48 of 60
Cream Production: Recognizing Constraints and Manufacturing Capacity Planning Link:  Tags: Manufacturing  Problem Conaprole, the biggest dairy production company in Uruguay, produces more than 150 SKUs in their ice cream plant, using five production lines, and up to five different packaging configurations for each line. The company plans ice cream manufacturing capacity on a 12-month rolling basis as part of the Sales & Operations Planning process, and the demand plan varies a lot due to seasonality. The factory management needs to prepare the production lines for the peak season during the low season, taking into account product shelf life and warehouse freezing capacity and costs. The factory was often unable to meet the high season demand which generated stock-outs, and the management found it very difficult to quickly reschedule their detailed plans due to the challenges they faced.   Manufacturing Capacity Planning — Model Logic  Other factors, including bottlenecks and constraints in the manufacturing processes and variations in human resource availability that can occur randomly, made capacity planning analyses even more difficult. The management’s challenge was to be able to reformulate their plans in order to balance supply and demand and make sure they would avoid stock-outs in key products. They also sought ways to optimize the use of their manufacturing capacities. Ite Consult found simulation modeling to be the best tool to carry out manufacturing optimization and provide Conaprole with the solution to these problems. The objectives of the manufacturing simulation model were:  To analyze various production scenarios for the following twelve months of changing demand.   To carry out manufacturing optimization to avoid stock-outs for all SKUs.  To optimize a production line’s capacity using the optimized production.  Solution Using AnyLogic’s discrete event modeling capabilities, the consultants designed and developed a manufacturing optimization model. It was integrated with the company’s S&OP planning platform and SAP Material Management and Production Planning. The created solution included three experiments with the model of the production system. Each of them addressed one of the objectives above and helped solve the business problems questioned. In the first experiment, the model examined the initial manufacturing capacity plan, detecting stock-outs and backorders that could be expected if production followed this plan. It allowed the management to explore production needs based on demand and initial inventory. This experiment also gave users the ability to find out, by manually modifying parameters, how different situations could impact performance, for instance: the need to close lines during certain periods, the necessity to modify equipment efficiency, extend resource availability, or change human resources’ schedules. Users could manually change priorities of SKUs and analyze the expected impact of such actions on revenue (costs associated with stock-outs’ differed by SKU). Additionally, they could define minimal used manufacturing capacity and some other policies.  The parameter variation experiment ran the model of the system 100 times and searched for the solution to fulfill the demand and keep products’ shelf life as long as possible while minimizing warehouse costs.  The last experiment optimized the use of lines by freeing production capacity in peak periods. Manufacturing was scheduled as close to the beginning of planning periods as possible in order to leave free capacity in all manufacturing lines as a buffer. Manufacturing Capacity Planning — Model Statistics  The input data included:   Demand per SKU Inventory levels per SKU  Batch size, priorities, life shelf, warehouse space occupation  Overall efficiency and mean absolute percentage error (MAPE), if needed  Costs  The system considered the following:  SKUs’ allocation by lines and sublines Lines’ and sublines’ capacities  Line and packaging restrictions  Warehouse limitations  Production times  Production schedules  Outcome All simulation results segmented by month and SKU were exported to Excel. Additionally, the model presented changes in demand and inventory levels by month in histograms. It also gave information about stock-outs, in case they happened.  By using the model, the Conaprole management was able to:  Discover the processes in each production line by SKU.  Optimize manufacturing capacity planning to better meet demand while maximizing product shelf life and minimizing warehouse costs. Improve production line utilization to secure additional manufacturing capacity in case of an increased demand.  The simulation model provided the management with the insight to choose the solution that would increase revenue and minimize the risk of stock-outs.    Simulating Off-Site Construction Factories to Deliver Energy-Efficient Modular Buildings Link:  Tags: Manufacturing  In the United States there is a significant divergence between manufacturing productivity and construction productivity. From 1994 until 2011, productivity in manufacturing almost doubled, but in construction it remained flat. In addition, buildings are one of the most carbon-intensive and energy-consuming industries or sectors in the US.  The challenge for the construction industry, and especially the building sector, is to improve productivity in the construction process to deliver energy-efficient, low-carbon, and affordable products. The  National Renewable Energy Laboratory (NREL) is going to undertake this over the next 5-10 years, together with the    US Department of Energy (DOE), and various funding agencies.  Problem The US DOE is pushing an initiative called  Advanced Building Construction (ABC) to deliver affordable, appealing, energy-efficient, and low-carbon new buildings. In addition, they are looking at methods to integrate more energy efficiency in the highly productive construction sector of the US. The US DOE is investing in new technologies, while at the same time engaging with stakeholders from both the public and private sectors.  The NREL, together with the US DOE, have two specific approaches to achieve the set targets. The first is permanent modular construction, which is when buildings are constructed off-site before they are transported and assembled at the final site. The second is simulation environments and a  digital twin, where a model is combined with the current state of a live system. Existing tools such as AnyLogic can be used to achieve this.  In an off-site construction factory in the US, there is a manual workforce and a large material handling system. The final product is shipped to the required destination, where it is assembled with minimal work. It is 40% faster to build in this way, however, it is unclear if the process is more energy-efficient and leads to more energy-efficient and low-carbon buildings. This is where a digital twin and simulation environments can be used to understand the processes involved, make better decisions, and deliver these buildings.  Solution The NREL uses a data collection strategy for the off-site factory, which in this case is the physical environment, to integrate with a near-real time representation of a simulation of this factory. This data includes every process and discrete activity, as well as all manual workforce or construction labor, along with the material handling systems. This leads to the digital environment, or the factory information model.    Near-real time simulation of off-site construction factories with every process and discrete activity involved   Off-site factories still require a lot of manual labor, and so it is difficult to understand the implications of labor productivity, cycle time or even downtime. This led to using AnyLogic   multimethod simulation modeling.  An agent-based   approach can simulate actions and interactions of autonomous agents, which in this case are the construction workers.   Discrete-event simulation modeling works better with simple assembly lines and conveyors. In   system dynamics, layers of the material handling system and material flow can be added. To really understand the labor productivity, a multimethod simulation model can be created, but with a strong focus on the agent-based approach.  The inputs in the model are the worker movements and activities that are taken from real world off-site factories. These inputs can then be placed in the AnyLogic environment. The video below shows the modeled factory floor. Here, human resources are allocated with those construction labor models seen in each station. As the work continues, specific workers, schedules, movement patterns and downtime, and even time taken for lunch can be assigned. All of that greatly influences what happens in the factory.   Video of the factory simulation model    In the model there are different stations, and in each station there are workers allocated. As the modular unit moves across these stations, those particular stations’ resources are activated and the exact schedule of the workers from the real-world situation is added as an input in the factory model. Therefore, a baseline for the existing condition of the factory can be created.  The entire factory can be simulated in great detail and in the model it is possible to zoom in to really understand the different stations. The modeler can really understand the effects of the material handling system.  Looking closer at four different areas of the created model (see diagram below), numerous research questions can be identified using the simulation environment. These questions can then be mapped to different methods within AnyLogic. Answering these questions allows for a better and low-carbon efficient product to be delivered. Model builders, using what-if scenarios, look at the trade-offs between productivity and new activities, and can then make informed decisions prior to implementing these strategies in the factory.    What-if scenario modeling in AnyLogic (click to enlarge)   Next Steps This model is a work in progress and as near-real time data is continuously received from deployed cameras and sensors in participating factories across the US, it continues to be improved. The current model shows how to represent an existing factory, but in the future, once there is an established baseline, what-if scenarios can be created and AnyLogic capabilities will allow this to be done. The future work will involve what-if scenarios for trade-off analysis and productivity vs new resources and activities.  The case study was presented by Ankur Podder, of the National Renewable Energy Laboratory (NREL), at the AnyLogic Conference 2021.  The slides are available as a PDF.     Simulating Port Berth Capacity with AnyLogic Link:  Tags: Ports & Terminals  Context     A port authority was facing the following issues:     the port was utilised by multiple clients and a total of 16 products passed through the port  the shipped tonnage of all products from the port was expected to increase by 80% in the next 5 years  there were multiple upgrade options available (eg. creation of new berths, relocation of existing, dredging)    Key decision to be made: How much additional product could be exported without upgrading the port?   Modeling Approach     Evans & Peck utilised a discrete event simulation modelling approach to map the port operations for a single berth.     Key benefits of the modelling tool developed include:      the port authority was provided with a tool to manage the first steps along the upgrade path  delays in port operations were identified which provided management with options for increasing production beyond current limits without capital expenditur.  The same decision modelling method can be applied to:  Analysis of efficiencies in logistical operations such as transport of products from mines by road or rail to a port Analysis of production scheduling in manufacturing and processing industries  Model developed and published by Evans & Peck Australia.     Simulation Based Digital Twin for Well Construction Process Optimization Link:  Tags: Oil & Gas  About Transocean   Transocean is one of the largest offshore drilling companies that provides rig-based well construction services worldwide. It operates a fleet of versatile, mobile, offshore units comprised of midwater, deep water, ultra-deep water, and harsh environment floaters. The company’s dedication to innovation and constant performance improvement led them to search for the best technical solutions. Problem Offshore oil and gas well construction is a complex process that takes a considerable amount of time. It demands certain sequences of both manual and semi-automated operations carried out in unison, as safely and efficiently as possible. Different kinds of equipment are usually created and operated independently. However, at the rig, all technical units are integrated. Any kind of delay in machinery increases the critical time path, which reduces the efficiency of the overall performance, and can lead to financial losses. The critical time path can vary depending on various factors, including:   Equipment manufacturers and configurations Experience and fatigue of operations personnel Rig type and drill floor layout Weather conditions Equipment maintenance Wellbore conditions   It is hard to track the dependencies of such variations and resulting inefficiencies throughout the overall process. For this purpose, the engineers needed to focus first on the different states of equipment; decomposing the process to the simplest levels, then combining these states, and finally seeing how they work together in the scope of other factors, including high-safety standards.  To handle possible variations and inefficiencies of the well construction process, Transocean engineers needed to collect and assess measurements at dozens of rigs, including machine and crew timing. A tremendous amount of data had to be collected and then analyzed, and if they managed it manually, it would be too time-consuming. That is why engineers decided to build a well optimization simulation model based on the machine data, including control signals from the equipment. Simulation would help engineers build an oil well optimization digital twin, which would reflect and help analyze the interdependencies among various operations to reduce downtime. Simulation outputs would enable the managerial staff to determine the real reasons of time loss and find solutions. Solution When building the oil well optimization model, engineers focused specifically on the tripping stage of the well construction process. It usually takes 20-30% of the overall well construction time, and it is semi-automated, so there is tremendous variability in the equipment performance. The process consists of operations such as block retracting, block hoisting, and bringing the pipe to the well center, some of

 

Section 49 of 60
which are done simultaneously.  Well creation process for further oil well optimization (click to enlarge)  Using AnyLogic oil well construction process simulation software, and leveraging state machine algorithms and discrete event process modeling, Transocean was able to analyze the tripping operations in real time. The whole process was decomposed into a hierarchal system of four main operations that were also split into functional elements down to the simplest levels. This allowed the engineers to create the state machine model. This well construction optimization model was integrated with a discrete event process model, which altogether represented the system of the entire tripping process with the built-in machine logic of 4-5 devices.  Transocean Builds Digital Twin for Well Construction Optimization  Statistics for oil well simulation and optimization model  The AnyLogic simulation allowed engineers to put in machine data to both models in real time and receive a range of results that could be presented in dashboards and colored charts to identify machine states and process activities, capture descriptive statistics, and analyze the time critical to operations for its minimization.   The application output the data to an SQL database, and then to BI-tools. This permitted the management and operations teams to have the necessary visual aids, and with the given information, look for the exact reasons of downtime and performance inefficiencies. Outcome  With AnyLogic oil well construction process simulation, Transocean engineers were able to represent the whole tripping stage of the well construction process. With real-time machine data and detailed representation of operations, the oil well optimization model performed as a digital twin, aiding in analyzing well construction activities. Model statistics were fed back to operations personnel and rig managers, allowing them to assess how well crews were performing and identify causes of time loss. The initial results indicated that over 20% of time could be saved by implementing the digital twin. Future implementation of the oil well construction process simulation for this project might result in assessing a new well profile and predicting the oil well’s performance. The profile could be run through the model, essentially drilling a virtual well, and could provide future insights into performance of the well, considering material handling, resources needed, and logistics around the rig.    Simulation Model of a Coal Bulk Terminal Link:  Tags: Supply Chains, Transportation, Ports & Terminals  Overview Daltransugol coal terminal, also referred to as the Vanino Bulk Terminal, is located in the deep-water Muchka Bay. It is the most important transshipment point on the way to the markets of the Asia-Pacific region.  It is situated at the extreme point of the Baikal-Amur Mainline, and is one of the newest and most modern coal terminals in Russia.  The terminal is equipped with an automated wagon unloading system and a coal warehouse with a capacity of up to 1.2 million tons. The current pier is capable of receiving and handling Capesize vessels. In 2020, the terminal handled 23 million tons of coal. It has an automated conveyor network with a length of more than 4 km. The length of railway tracks is more than 50 km.  Problem Daltransugol needed to increase the transshipment capacity of the terminal to 40 million tons of coal per year. At this stage, the specialists of Dilibrium conducted an analysis of the Daltransugol coal terminal.  They specified parameters of the technological process, made full-scale measurements of individual technological operations, conducted interviews with various specialists and terminal services, analyzed historical data on the operation of the terminal, and found statistical patterns. Then they used this information to build a model.   End-to-end business process modeling logic showing transport logistics (click to enlarge)   The entry point was the arrival of laden wagons at the Terminal Reception Park. The exit point was the departure of empty wagons after the Terminal Sorting Park, and the departure of loaded vessels from the water area.  Simulated processes included:   The formation of applications for the supply of goods to the terminal by rail, and applications for the shipment of coal by bulk carriers.  The movement of trains at the adjacent station, and internal railway logistics of loaded wagons: arrival, sorting, sawing and defrosting of cargo, and filing for unloading.  The unloading of wagons by using an automatic system of wagon dumpers, taking into account the peculiarities of the process at different times of the year and weather conditions.  The transportation of goods through the conveyor network, especially the work of a virtual dispatcher for marking a warehouse for unloading coal, and building routes, considering the priorities of cargo, coal grades, season, weather, workload, and degree of operating time of the main technological equipment, etc.  The loading of coal onto ships, allowing for their deadweights and restrictions on individual berths for receiving ships, queue priorities, as well as types of vessels for which loading should proceed with different quality control and speed, etc.  The movement of empty wagons along the internal railway network, including sorting, rejection of wagons and sending them for repair, and formation of trains for shipment by rail.   Solution To solve the problem, a simulation model was developed with AnyLogic software. The development process consisted of the following stages:  1. Model design At this stage, information about the object of modeling was summarized and the concept of a simulation model was formed. The goals and objectives of modeling were clarified, and modeling boundaries were defined. The structure and technological architecture of the simulation model, as well as the main set of capabilities that the simulation model implemented, and the simulation results were also described.  2. Development of the model “AS IS”  First of all, Dilibrium decided to develop the “AS IS” simulation model of the terminal. This was important in order to set accurate values of the parameters of individual agents and algorithms on the constructed and sufficiently detailed model. The purpose was to verify the model in accordance with historical data. As a result of the verification of the terminal simulation model, it was possible to achieve high reliability in comparison with historical data for 2019-2020.  This result became possible due to the high degree of detail of technological processes in the simulation model.  To simulate technological processes and change the state or behavior of model objects, the following simulation approaches were applied:   A method of agent-based simulation to implement the behavior of individual agents.  A discrete-event method for modeling the technological processes of the terminal.   3. Refinement of the model with “TO BE” scenarios Several experiments were added to the verified “AS IS” simulation model with fine-tuned processes and parameters to test hypotheses for calculating the maximum throughput of the terminal. These included modified terminal spatial planning, a new layout, and new types of main technological equipment. In total, five different options for the configuration of warehouses and sets of basic technological equipment were implemented in the model. They corresponded to various options for the future development of the terminal.  Using the AnyLogic Rail Library, all internal railway logistics were modeled with a fairly extensive system of tracks and technological processes. The pipeline network was developed on the basis of the AnyLogic Fluid Library. Additional technological processes were modeled using Java code.   End-to-end business process modeling logic showing railway logistics (click to enlarge)   In addition to modeling railway and conveyor logistics, there was a difficult task to develop a cargo routing algorithm taking into account warehouse planning. In order to select and build a route, a large number of interrelated conditions and parameters had to be considered:   The stage of modernization chosen before the simulation. Weather conditions (modeled according to historical data and with specified coefficients of deviations). Season. Availability of each key unit: equipment (more than 80 units), employment on the line, maintenance and repair, breakdown. Current order from the dispatcher for unloading or loading. Volume and types of cargo in wagons at the terminal and in transit. Volume and brand of cargo in the warehouse. Current marking of the coal stored in the warehouse in certain stacks. Operation priority (unloading/loading). Deadweight of bulk carrier in the queue for loading.  The algorithm was developed using the AnyLogic statecharts (click to enlarge)   The model simulated a full calendar year, including seasonality and weather conditions. During this period, both operational data and statistics by year/month/day for all major processes and pieces of equipment were collected in virtual time mode. At the end of the simulation, the outputs were uploaded to a separate Excel file for further analysis of the results of the experiments.  Stochastics were implemented into the model, for example, random deviations from statistical data on breakdowns, weather conditions, and the coefficient of errors or inconsistencies when submitting trains and ships to the terminal. Conventionally, a ship could come to the pier, at which not all of the required grades of coal were in stock and the rest was still delayed on the way to the terminal.  Results The simulation model was verified through the “AS IS” stage. Dilibrium obtained extensive statistics for maximum productivity and for the indicators of each equipment node: maintenance and repair, operating hours, CTG, KPI, and other values. It became possible with the help of multiple experiments for each variant of the modernization stage. In addition, with the aid of the experiments, bottlenecks for different operations were identified.     Simulation Model of the Sorting System for a Warehouse Link:  Tags: Warehouse Operations  Overview DHL Supply Chain is a division of Deutsche Post DHL Group, which operates globally. DHL Supply Chain is the world’s leading contract logistics provider. Their integrated logistics solutions drive efficiency and improve quality. To learn more about DHL warehouse modeling in AnyLogic and the optimization tool for order picking, read the following case studies:  Optimizing e-commerce warehouse operations Warehouse cluster pick optimization  In these case studies small and medium size e-commerce orders were considered. Problem  The sorting system (click to enlarge)   DHL Supply Chain wanted to install a box sorting system for the new facility. Compared to the previous cases, there would be much larger orders. Thus, DHL needed automation to support the sorting process. This system was supposed to include a reject conveyor, eight divert conveyors, and a recirculation conveyor. Orders would be sorted and palletized by a narrow belt sorter. DHL planned to sort boxes by their weight. It was essential that the heaviest packages go to the bottom of the pallet. In doing so, they would not damage the medium and light boxes. A screen at the entry point would notify operators about available lines and boxes on the recirculation conveyor. There were four palletizers at the end of divert conveyors. Each had two sorters. The feeding conveyor would be switched after every pallet completion. DHL required a simulation method implemented in their planning strategy. They needed to define the following:  If the system can handle peak day volume. If yes, then what are the optimum operating parameters for the system.  Solution To meet these goals, they decided to use AnyLogic simulation. DHL used AnyLogic Material Handling Library for the simulation model. The developers used historical data as inputs. This data provided the arrival sequence of boxes, their order numbers, and their dimensions.  Input data for the simulation (click to enlarge)   They decided to have three entry points for weight classes: heavy, medium, and light. Three employees would sort the boxes and drop them on the conveyor. There were two variable parameters X and Y, which DHL engineers were not sure about: capacity of the recirculation conveyor (X), and the time after which orders were flushed from the sorter (Y).  Simulation logic (click to enlarge)   The values of the variable input parameters X and Y could be adjusted to identify the best scenario.   The spreadsheet identifying how the value of X and Y affected the outputs (click to enlarge)   Results DHL developers made sensitivity analysis while varying input parameters for several scenarios.  Analysis outputs (click to enlarge)   At the end of the simulation, engineers shared the results of different scenarios with their management and operation team. They decided to use it as a reference for future operations.  Watch the video about the case study presented by DHL at the AnyLogic Conference 2021:       Simulation Modeling Based on Healthcare Routine Data Link:  Tags: Healthcare  Decisions made by health care professionals require tools for planning, testing and assessment of new technologies or interventions. The complex structures, interactions and processes involved in health care, make change and innovation an ongoing challenge. Patrick Einzinger and Christoph Urach from DWH Simulation Services and Vienna University of Technology in partnership with the Austrian Association of Social Insurances (AASI) were given an opportunity to analyze public data for the purpose of critical future decision making.  The AASI assembled routine care data upon reimbursement of heath care providers, which includes drugs prescribed, services rendered and diagnosis. Typical statistics and mathematical modeling were considered as a tool to analyze the data, but simulation was chosen to ensure the mass amount of data could be fully utilized, thus increasing the accuracy of the analysis and results. Furthermore, AnyLogic Simulation and Modeling software was employed for its multimethod capabilities. The models built include an agent based simulation of multiple reimbursement schemes in the extramural healthcare, a system dynamics model of the consequences of group practices, and a microsimulation model for health technology assessment of screening for Abdominal Aortic Aneurysms (AAA).  Comparison of Reimbursement Systems Problem Patients may be receiving unnecessary procedures due to AASI current reimbursement system which reimburses the physician for single services. The AASI is considering other reimbursement systems such as per visit, per episode of disease or per episode of care, but before any change can be implemented, the options must be tested in order

 

Section 50 of 60
to realize the consequences involved in the proposed reimbursement systems. Solution   Healthcare Reimbursement System Modeling: Consultants' View of the Problem  The consultants built a model that includes data imports of patients, medical problems, medical providers, medical services and reimbursement types using AnyLogic Simulation and Modeling software. AnyLogic was chosen for its agent based capabilities which allow the incorporation of different parameters for a high number of subgroups, unlike a top-down approach where it would be necessary to split the patient population into subgroups. In AnyLogic, agents simply have attributes that determine the subgroup and therefore the parameter. Then, as multiple simulations are run, the consequences of each reimbursement system can be realized.  Outcome Due to the number of diseases to consider, it is not possible to complete extensive literature searches and gathering of expert opinions on each disease. The approach used shows how analysis of routine health care data can serve as input for a complex and comprehensive model structure as the data provides accurate prevalence and incidence rates for the diseases. Even though assumptions are necessary for certain portions of the research (i.e. treatment pathways) the optimal selection of providers to their service portfolio leads to plausible and reasonable results. The model is not only useful for the comparison of reimbursement systems but also for testing assumptions and will assist with formulations of new reimbursement systems, due to effects that are logically inherent and show quickly in simulations, but are not typically straightforward or immediately obvious. Consequence of Group Practices Problem AASI would like to know the effect of executing group practices, where physicians with different specialties combine practices, for example, pulmonologists and internist. The group practice is complicated by the number of cases, consultations and individual services which, depending on the physician, may be reimbursed otherwise. Will the physicians increase or decrease the amount of services provided dependent upon their workload and salaries? How does the behavior of the group practice differ from the physicians who remain as single practices?    Group Practice System Dynamics Model Structure  Solution To provide insight for AASI, the consultants developed a system dynamics model using AnyLogic simulation and modeling software, where nearly every variable is an Array Variable due to the 4 groups of physicians (single practice pulmonologists, single practice internists, group practice pulmonologists and group practice internists). The model’s input values include fixed parameters, (data analysis, OOEGKK, and internist/pulmonologists) strategic assumptions and assume the number of referrals and new cases would increase. Outcome The parameters of the model were given to the AASI for simulation, but DWH were unable to view the results due to privacy policies. The output of the simulations included total fees that were paid, number of cases, number of consultations, number of special services, number of double services and the value of each case. The model also allows the AASI to change input variables and run multiple scenarios. Abdominal Aortic Aneurysm Screening Problem Abdominal Aortic Aneurysms (AAA) are prominent in the male population of Austrian men over 65 years of age. If detected at an early stage, the occurrence and risk of rupture decreases significantly versus detection at a later stage where expensive and dangerous surgery is the most probable outcome. Ultrasonography is used for early detection of AAA, which is noninvasive and a moderate expense.  The AASI employed DWH to find out the following:   What would happen in the Austrian population if there was organized screening for individuals over 65 years of age?  Which people would benefit the most from organized screening?  What are other optimal screening strategies, for example, once per year or every two years?  Can we change the screening approach dependent upon the person’s risk factors and increase screenings when other risk factors occur?  Solution The consultants utilize agent based modeling in AnyLogic software, giving every person an aorta whose diameter growth depends on age, sex, smoking habits, history, and other diseases. Furthermore, they run simulations of multiple screening strategies to find the optimal solution.  Outcome   Abdominal Aortic Aneurysm  If regular screenings over age 65 years is introduced for about 40% of the population, the death rate will decrease by about one third. Taking other measures to prevent AAA was also considered. For instance, decreasing the amount of smokers in the Austrian population from 38% to 20% decreases the amount of AAA similar to the decrease seen in implementing a regular screening process. Its modular design allows integration of additional knowledge on AAA for further analysis and evaluation of screening strategies or other interventions within a short time-frame and the disease progression module can be adapted without changing the whole model structure. A study of this magnitude on patients is often impossible due to temporal, logistic, ethical or other reasons. Utilizing simulation modeling gives insight on possible consequences of interventions, which results serve as decision aid for health care decision makers, plus the design can be adapted to evaluate other diseases with similar progression. To learn more about Einbzinger and Urahc’s work in AnyLogic software, view the AnyLogic Conference 2013 presentation:      Simulation Modeling of an Offshore Offloading System for an Arctic Oil and Gas Condensate Field  Link:  Tags: Transportation, Oil & Gas  Problem       The Novoportovskoye oil and gas condensate field is located in the Yamal peninsula and owned by Gazprom Neft, the fourth largest oil producer in Russia. Oil from the field is transferred via 100km pipeline to the sea terminal at Cape Kamenny, where it is loaded into arctic tankers for further transportation. The full-size field development will start in 2016 and continue for several decades.       The main issues in planning transportation in an arctic region are the harsh ice environment and difficult sea conditions. Most of the year, the ships are operating in a 500m ice channel in shore fast ice of Ob’ Bay over 2 m in thickness. Some months of the year, the open water area of Kara Sea is almost completely covered with drifting ice.    Define a sufficient amount of arctic tankers and the demand for icebreaker assistance. Calculate the expenses for the tankers' fuel and freight of icebreakers in different ice conditions.  Design a temporary scenario for oil shipment during 2016-2017, when tankers of low capacity and low ice reinforcement will be used. Gazprom Neft plans on gradually introducing new arctic tankers of greater capacity, in accordance with the increase of cargo traffic. Consultants needed to define the system capacity during 2016-2017.  Define the capacity of a shore-based storage facility to be sufficient for usage in ice conditions of different severity. Any storage overflow should be eliminated. Consultants needed to calculate the minimum volume of shore-based storage which will meet capacity requirements within all periods of field development. They had to take into account that the more severe the ice conditions, the more difficult it would be for tankers to provide the required rate of transportation and avoid the storage overflow.  Solution  Logistics planning model interface      By the order of Gazprom Neft Novy Port LLC, experts from a state research center incorporated ship calculation modules, GIS environment, and a logistic simulation model developed using AnyLogic under common interface. The simulation model reproduced the dynamics of shore-based storage loading, logics of ships’ motion, and interaction in probabilistic weather conditions, taking into account ice channel freezing.       Tankers were modeled as independent agents moving in the bay and guided by the logic blocks of the simulation model: interactions between tankers and icebreakers, choice of tanker speed in correlation with storage fill, other tanker locations, ice conditions, and other factors.       Using GIS technology inside the simulation model allowed them to do an analysis of a transportation system sensible to geographic factors, including bathymetric conditions, navigating channels, protected waters, and shore line.       Since ice channel conditions significantly influence ship traffic, the state center experts added to the model the following parameters:    Characteristics and number of tankers in the channel Time after last pass Air temperature Wind and wave conditions Terms for canal laying  Result  Logistics planning model: changing parameters in different ice conditions      Based on multiple model runs, the state center experts defined the optimal storage volume to be sufficient in ice conditions of different severity.       Application of simulation modeling technology allowed them to design a transportation system adjusted for dynamics of ice channel freeze-up, tanker traffic, and storage fill. No analytical tool can consider these kinds of dynamic factors.       In the course of the modeling project, the state research center experts also developed best practices to eliminate the risk of storage overflow. The model also showed the optimal amount of channels in land-ice for different ice conditions, approximate dates of canal laying, and the volume and terms for icebreaker support for tankers. Analytics defined the dynamics of outwards pilotage, expenses for fuel, and icebreaker support in various scenarios during all periods of field development. The model also helped plan operations during the temporary usage period of small tankers of low ice-class.      Simulation of Behavioral Economics Theory as Applied to Organizations Link:  Tags: Social Processes  Classical economic models assumed that people were rational and predictable with well-defined preferences, which enabled them to make decisions. In contrast, behavioral economics is a new method of economic analysis that combines elements of economics and psychology to better understand people's behavior in the real world.  Within this new field, theories such as “Prospect Theory” have emerged. This theory shows that human decisions are not always optimal and how we make our choices is often based on how the options are presented. These selections may depend on context and driven by biases, e.g., one-model thinking bias, herd behavior bias, etc. These biases can be targeted in order to achieve a positive or intended outcome. Furthermore, rational outcomes can be achieved by reframing or changing the context of choices.  Problem  Organizations consist of people who make decisions which impact the performance of the company. People’s rational and irrational decisions create both long-term and short-term effects. In reality an organization is also composed of systems and subsystems. This is where   Astellas Pharma used AnyLogic simulation software to combine behavioral economics with system science to generate simulations. These could then help identify what impact people’s decision making has on the company’s performance.  Solution  In this case study, the researchers applied agent-based modeling to simulate employees. At first, the researchers utilized behavioral economics to determine how the agents (employees) made decisions. After that, they used system dynamics approach to identify any unintended consequences, along with the emerging behavior of the organization.  The model was initially built as a project to help learn to ask the right questions. A large number of these questions related to building trust within teams and the role of psychological safety, along with team productivity and organizational performance. This model looked at the interaction between different teams, how those teams worked within themselves and how they worked in collaboration between different teams.    Agent-based model to evaluate organization's performance   In the model, the researchers could control the size and the number of the teams. With various team sizes, the dynamics of trust developed differently over time. In addition, the nature of the questions asked varied. Some teams worked on research-based approaches where the uncertainty is quite high and the level of success is fairly low, while in other teams, the success was a little bit more predictable. Finally, the researchers of this model looked at how established teams vs new teams played a role in the development of trust over time.  Findings Initially, the researchers changed the penalty of interacting with people (negative result penalty), applying the behavioral economics model. So, in this first finding, the model showed variation in the productivity of the team, which illustrated that the experiment was working as expected. Following this, more findings could be verified.    As agents begin interaction, productivity varies as trust changes   In the second finding, the model sought to answer questions related to team size. If there is a variation in team size, does the trust or productivity change. The results showed that with small teams, trust is extremely high, but as the teams increase in size (for example 50 or 100), the trust drops relatively. Productivity follows the same trend.    Variations in team size can affect the trust between employees (click to enlarge)   The third set of findings dealt with how the type of tasks could affect the interactions amongst the employees. The questions asked were:   What happens if the teams receive tasks that are either collaborative or individual?  Does a collaborative task require a lot of trust or do the types of tasks have an impact on trust or productivity?    The researchers found that a small team with a lot of paired tasks works well because they have enough trust which enables them to have a high level of productivity. However, as the teams begin to increase in size, the trust goes down. When you include the complexity of paired tasks, the trust falls further, and the productivity follows in the same way.    Percentage of collaborative vs individual tasks affect trust between employees and overall productivity of the team (click to enlarge)   Essentially, paired tasks and larger teams revealed that the trust would be lower and in order to improve productivity, team sizes or the types of tasks should be varied.  Next Steps This model helped to identify which targets could be changed and if nudges could be created or not. Overall, the model showed some important findings, but as it is still in development there are a number of results that could be shown in the future. Such results include determining optimal team sizes, simulating the impact of bad employees, simulating

 

Section 51 of 60
changes in strategic directions, and even interdepartmental collaborations and cross-communications.  Finally, the model will be able to see any unintended consequences of changes an organization makes as well as improving strategies, structures, and processes by focusing on influencing the decisions of individuals.  The case study was presented by Akansha Saxena (Astellas Pharma) and Kurt Kreuger (Kreuger Consulting) at the AnyLogic Conference 2021.      Simulation of COVID-19 Spread in Hong Kong Link:  Tags: Healthcare  Problem Researchers wanted to create a model which could show the spread of COVID-19 in Hong Kong and identify vulnerable areas. However, Hong Kong has very unique conditions, especially in terms of COVID-19 and so this was harder to do than first thought. It is a hyperdense urban environment, a major transport hub, and the economic engine for eastern Asia.  There were also political changes taking place at the time of this pandemic, so government interventions may not have worked as intended. Finally, Hong Kong had experienced SARS previously and the population had concerns about infectious diseases already.  Solution Using secondary data in Hong Kong and a 3D agent-based model the researchers aimed to:   Simulate how COVID-19 spread in Hong Kong.  Identify which areas or population were the most vulnerable.  Evaluate the effectiveness of COVID-19 non-pharmaceutical interventions and vaccinations.    The researchers used a large array of data, including from the Hong Kong Travel Characteristics Survey 2011, points of interest (POI) data, building data, and COVID-19 data.  The first agent-based mobility model created included two main agents – mobile and static.  Mobile agents were considered to be the younger generation which went to school, the office, and such places, while the static agents spent more time at home. Four major locations were used – restaurants, shopping malls, schools, and workplaces. There was also a separate smaller category for other places.    Mobility model   The second infection model was an extension of the SEIQR (Susceptible-Exposed-Infected-Quarantined-Recovered) model. In this model, the researchers assumed that 2 meters was close contact, and exposure could be the result. They also considered the duration of stay in the same location and ventilation.  In addition, parameters and data were taken from literature such as publications or from government reports. This information came from Hong Kong, mainland China, or east Asia.   SEIQR model (click to enlarge)   Results From the mobility and SEIQR models, researchers created a scenario analysis, and five different simulation models were configured:    M0 – the default simulation model without any interventions.  M1 – 50% of the population were assumed to work from home.  M2 – face mask interventions where 80% of people wore a mask, with the protective level of those masks being 65%.  M3 – where there were vaccinations and 70% of people would be vaccinated and the vaccine had a 70% efficacy level.  M4 – the implementation of M1 + M2 + M3.   The population of the model was 10% of the total Hong Kong population. This amount was chosen in order to have a faster model run time, while also having enough infections in the model. The number of POIs was around 250,000 and the model period was for 1 year.  When the model was run, the different parameters which could be adjusted were displayed. Then the results were calculated and displayed graphically. These could then be downloaded to Excel using AnyLogic functions.  In the illustrations below the effectiveness of the interventions can be seen: M4>M3>M2>M1>M0. Based on M0-M2 the second wave was predicted to occur if there were no vaccinations.   Overall results (click to enlarge)   Overall results graphically illustrated (click to enlarge)   District level results could be displayed to identify which areas were riskier for infections. The top three high risk districts included the center of the business and commercial areas of Hong Kong.  Other results included age and gender, and the effective reproductive number.  A final conclusion was that despite the effectiveness of the COVID-19 vaccination, non-pharmaceutical interventions still needed to continue, possibly for years.  In the future, a number of other factors could be considered, including using new types of data, using more geographic approaches,   multimethod modeling with  system dynamics and  discrete-event models, other social changes, and participatory modeling working with specialists from the public health. These additional elements could result in a more realistic model.  The case study was presented by Peter K. Koh Phd & Ken K.C. Tang, University of Hong Kong, at the AnyLogic Conference 2021.      Simulation of Disease Progression for Pharmaceutical Forecasting Link:  Tags: Healthcare  Acute myeloid leukemia (AML) is a type of cancer in which abnormal cells in bone marrow and blood interfere with normal blood cell production. This is a very serious cancer and can progress rapidly, resulting in death within weeks or months if left untreated. It is, therefore, essential to diagnose it early and ensure treatment is given as soon as possible.  Problem Treatments are available but depend on a number of factors such as the subtype, morphology, patients’ preferences, access to care, and numerous others. AML is very complex in how it can be diagnosed, how it progresses, and how it is treated. This complexity provides an opportunity for an epidemiology model based on systems science to be developed.  Solution  System science based models can create complete systems of disease and treatment pathways to help decision makers understand how health conditions develop and their consequences. This allows the patient to be treated, understand the progress of the patient, and finally how the patient is interacting with the system.     The benefit of systems science methodologies is that there can be integration of data and evidence from many different sources at many levels of analysis. The model below shows this in the model breadth and at a lower level – the interventions. This creates a deeper understanding of the patient and the entire market overall.     System science based epidemiology model (click to enlarge)    Astellas used an agent-based model to simulate how patients progressed through screening, diagnosis, disease progression, and treatment. External factors that could impact this model were also considered.    Agent-based model to follow the journey of patients   The model used publicly available data to be able to ensure accuracy and adjust for the real world. This methodology could help optimize forecasting and product planning by identifying risk.  Results The model matched the published literature at a very high level by using different data sources. By aggregating the results into one model, the researchers could see how patients would progress from diagnosis to palliative care or a relapse group, or other alternative progressions.  The assumptions were visualized using the private cloud feature of AnyLogic. Internal decision making such as forecasting, sensitivity analysis, and running Monte Carlo simulations could also be done here.  Astellas could then understand how changes in the market could impact patients, and then simulate how to better treat them in the future.   The case study was presented by Alexander Chettiath of Astellas, at the AnyLogic 2021 Conference.      Simulation of Maternity Ward Operations Link:  Tags: Healthcare  Problem This model simulates the maternity ward in a hospital currently under construction. Since the new hospital building will replace an existing ward and since the new maternity ward will be staffed by current personnel, the model also simulates current facilities. The purpose of the model is to support discussions related to which resources, capacity, and work methods are required on the new ward. One relevant discussion is whether to apply an “integrated philosophy” - where the mother and child stay in the same room during their entire stay - or whether dedicated rooms for antenatal care, delivery and postnatal care are preferred, as used in the current system.  The project was carried out for Karolinska University Hospital in the Stockholm County, Sweden. Solution  Model of the current ward  Since this problem is on a micro and operative level of abstraction, Discrete Event Modeling is naturally the preferred modeling choice. This enables the handling of resources, processes, patients, etc. in the best way. Further, since this issue requires the comparison of a two distinctive alternatives, it is advantageous to run these scenarios/alternatives in parallel, instead of in sequence. In this way it is possible to pinpoint differences in performance given the same demand. From a modeling perspective, the mother-to-be is “cloned” and sent (and her clone) simultaneously to the two different process alternatives. This method was also chosen especially in this case to support the discussions during two workshops. This process model focuses on the physical resources. A number of variable parameters enabled the users to experiment with relevant scenarios.   Model of the projected ward                    The parameters include yearly demand, number of rooms of different categories for the existing ward and future ward, relevant patient categories and their traits (such as minimum, maximum, and average time for delivery, postnatal care, etc.), proportions/probabilities for forms of care, and prioritization (when several resource types can be used for the same care process). The process description excludes human resources. To do so the staff schedules, personnel categories, skill levels, planning strategy, etc. should be included. Given that the purpose of the model was to focus on physical resources and investments and support the discussion process, this was unnecessary. The model therefore assumes that there are always enough personnel. The caregivers are animated but are never limiting.  Outcome The primary purpose of this model was to stimulate and support the discussions and conclusions in a workshop format. The simulation “provoked” participants into better insights into their situation. From a clearly skeptical outset, the model enabled participants to see that the future scenario was in fact realizable and envisage how they could start to prepare for this.  The outcome can also be seen in the light of that fundamentals from operations management and healthcare management engineering are much easier to understand for those lacking a strong background in these fields if communicated and presented with the help of a visual simulation model. Examples of such fundamentals include:  Dividing a total need over several dedicated resources will always cost something in terms of effective capacity compared to having the same number of resources but fully flexible.  A need evaluation must always be made per resource type – and the amount of resources per type should roughly have the same relationship as the relative needs.  Historical output, result and production figures can seldom be used to take decisions for systems in the future (with different traits and circumstances).  The output and results from the simulations were summarized in a result window. Indicators were presented for the existing ward and the future ward both numerically and graphically. This enabled the evaluation of the strengths and weaknesses of each simulated scenario.    Simulation of the Construction of a Tunnel with a Tunnel Boring Machine Link:  Tags: Asset Management   Developed by Ruhr-Universität Bochum and SimPlan AG.  Problem Definition Construction of a tunnel with a tunnel boring machine (TBM) is usually a long-lasting project with complex interdependent processes and a comprehensive supply chain. Monetary losses due to equipment breakdowns and insufficient supply chains can range around 20% of the whole project cost, and that often means millions of dollars. The cost of one hour of down time is very high and project managers have to do their best to avoid unnecessary delays. The aim of the simulation project, which was carried out at Ruhr University Bochum in Germany, was to create a simulation model that would be capable of determining the bottlenecks in the established processes in order to minimize the possible monetary losses.  Solution Construction of a tunnel with a TBM is determined by the process-related excavation of soil and the construction of the tunnel itself. Furthermore, a steady flow of excavated soil must be propagated through the machine. However, the construction process is often disturbed for many reasons and then the processes must be suspended or delayed.   Since AnyLogic software makes it possible to combine several simulation techniques, the modelers were able to reflect all the peculiarities of the system by simulating the process-related works with the help of state-charts (Discrete Event Modeling), and the continuous propagation of soil with System Dynamics. The System Dynamics method was also used to memorize the current status of the excavation in case of a disturbance. The multimethod approach also enabled the modelers to easily simulate disturbances of certain machine components. The assembly elements of the TBM were modeled separately in Active Object Classes in order to provide a flexible and reusable simulation model. Besides the TBM, other works were simulated (back up system, tunnel works, on-ground construction site, and external supply chain) which allowed the model to be completely aligned with real-life and its uncertainties.  Outcome The developed simulation model can illustrate and quantify the impact of disturbances on the progression of a specific tunneling project. The simulation of on-site processes and external logistics will also allow a project manager to reduce weak spots and plan required back-up stock. The minimization of unnecessary standstills and performance losses will shorten the duration of a tunneling project and optimize its budget.    See the video of Tobias Rahm from Ruhr University Bochum presenting this project at The AnyLogic Conference 2012 or download his presentation.        Simulation of the Telecom Market in Argentina Link:  Tags: Marketing  Problem:           In 2008 there were three huge companies in the telecommunications market in Argentina. Each company sold some of the four products (known as “quadruple play” when combined), including high speed Internet, cable TV, telephone, and cell phone services. Many households purchased individual services from the different companies. The aggressiveness of the players made “product bundling” a key marketing strategy, but there was not a clear understanding of how to use this strategy, or the impact it might have on the market.           Each company wanted to enter the markets that

 

Section 52 of 60
they hadn’t covered yet, but each one knew that penetrating the competitor’s niche would force them to reply in the same manner. The consequences of such actions had to be estimated. That’s why Telefónica (one of the competitors) decided to employ Continente Siete, a consulting company, to build a model of the market using AnyLogic multimethod simulation software.           The challenge was to build a model that would allow the client to analyze several scenarios, taking into account the whole telecom market (with its 3 major players) and the “product bundling” effect. Scenarios had to cover the telecom market evolution for 24 months and issue an integrated set of metrics (market share for each product, revenue, brand effect, etc.). The model had to consider all 100+ products potentially available in the market. It also had to work with a previous conjoint analysis made specifically for this purpose, as well as include historical information on complementary products and macroeconomic variables.   Solution:           The core of the model was consumer choice. Households were modeled as agents (1 agent simulates 1000 people). The whole consumer decision process (Know-Evaluate-Decide-Implement) was embedded into each consumer agent, based on the results of the conjoint analysis. Many factors influenced consumers in their decisions. For example, the conjoint analysis allowed the estimation of each product’s attribute value (including both price and brand) for each consumer. Besides those preferences, the consumer had to be familiar with each new product and keep it in mind before buying. Also, wallet restrictions were added, as all of the agents were divided into three groups by their income level. Other barriers, such as needing to buy a PC to have Internet, going through the hassle of changing the service provider, and even the service provider’s retention strategies (making a better offer to a customer who wants to quit the service), were also taken into account.           The companies were modeled as agents too. They dealt with different levels of the four basic products (for example, low-speed Internet vs. high-speed, etc.), bundles, promotions, and retention offers. Prices, promotion lengths, retention policies, and even the launching of new products were all controlled by the companies.            The whole model reflected the current state of the market, including the companies, their income, their number of customers (structured by geographical and socio-economic properties), technologies sold by the companies, government regulations implied, and PC market evolution over time. All macroeconomic variables were simulated with the System Dynamics approach. An adapted innovation diffusion Bass model was used to model PC evolution over time.           Scenarios were built to understand possible outcomes of launching the new basic products for each company. A particular emphasis was placed on possible delays between the competitors’ product releases. Price escalation and promotion removal scenarios were also tested. Another key question was the subsidization of buying PCs for low income consumers. Scenarios were built to understand the impact of this policy.   Outcome           It was the high flexibility of AnyLogic simulation software that allowed the consultants to grasp the system in its complexity. It allowed them to build a truly multimethod model using System Dynamics for reflecting the market as a system and Agent Based Modeling for simulation of the behavior of each household and of the companies.           The use of the validated model for scenario generation allowed Telefónica to build up their strategic plan for 2009 to optimize their performance in the market.            Watch the video of J. Pablo Rodríguez Varela, the co-founder of Continente Siete , presenting this case study at The AnyLogic Conference, or download his presentation.        Steel Pipe Manufacturing Scheduling with Simulation Software Link:  Tags: Manufacturing  Problem        Manufacturing and economic conditions are increasingly complicated and unstable with the result that traditional scheduling methods are becoming ineffective in the face of disruption. A major Eastern European steel pipe manufacturing plant, relied on traditional spreadsheet-based manufacturing scheduling and began facing the following problems:    Persistent and spontaneously occurring bottlenecks that were unaccounted for in the manufacturing schedule and hindered its implementation. Manufacturing schedules didn’t include order routing and instead relied on the experience of foremen to make decisions case by case. Lack of coordinated production stages resulting in downtime, equipment overloading, and order sequencing disruptions.       The plant’s management team opted to improve manufacturing scheduling using simulation software. For this purpose, they approached Focus Group, a company that provides services for implementing simulation-based decision support systems. Focus Group specialists developed a manufacturing simulation model that would help fulfil three main goals:    Complete manufacturing scheduling automation, to exclude human error. Developing a precise specification for pipe production routing through stations. Identifying free production capacity to enable additional orders.  1. Elements of the model in AnyLogic Solution        The Focus Group team developed a manufacturing scheduling model based on a discrete event approach and the manufacturing processes were represented as flow diagrams. The developers opted to develop their manufacturing scheduling simulation model with AnyLogic because the software offers several key advantages:    Discrete event modeling. Industry-specific library — the Material Handling Library. Model can be exported as a standalone Java-application. Data handling capabilities, including input and output. Opportunity to create a fully functional digital twin of the manufacturing enterprise that integrates data from actual manufacturing facility operations.       In addition, AnyLogic not only enabled the modeling of processes, but also the ability to set the logic for components in the production line and their rules of interaction.        For the material handling model, the Focus Group team included objects such as Order, Pipe, Prokat (Metal-roll), and Station. In the model, they set the logic for the whole production line, from a damper warehouse to finished order release. The input data (weekly order schedule, performance, machine units working hours, etc.) was uploaded into the model via Excel-templates.        The model could test out the feasibility of the weekly manufacturing schedules created by the pipe manufacturing plant specialists. Moreover, the engineers developed a specific algorithm that enabled the model to suggest optimal manufacturing schedules. By applying analytical methods, and testing the results with dynamic modeling, the Focus Group team ensured that the production level in the model was not lower than in schedules made by the plant team. Moreover, the model helped reduce the number of the equipment changeovers and provided more detailed planning.   2. Elements of the model in AnyLogic Result        The developers created a steel pipe manufacturing model that considered all production stages on the shop floor. The plant's performance improved because, based on the model, the management could:    Test the reliability of the manufacturing schedule developed by the plant in a risk-free digital environment. Safely test if new orders can be integrated into existing schedules. Determine the utilization rate of major stations and machine units.       Additionally, the model helped calculate expected order lead times and sequence optimization, provided detailed stage-by-stage order routing, and could develop the weekly manufacturing schedule itself.        As a result of the simulation modeling, the manufacturing enterprise minimized gaps and daily fluctuations in the handover of orders, identified bottlenecks, and refined order routing. They also combined short downtimes into longer intervals and used them to enhance line utilization. Furthermore, schedule adjustment time was reduced from three hours to three minutes. Overall, modeling the production line and developing schedules with it lead to a forecasted additional production output of 678 tons and an increase in revenue of $813 thousand per week.        In the future, there are plans to further extend the model to function as a digital twin of the shop floor by integrating it with the production control systems. Management will then be able to set detailed shifts or daily tasks for each machine unit, as well as connect planning and production. This would enable the plant to adjust its schedules based on a global view of the production site.      Steel Plant Simulation Helps Increase Unit Throughput Link:  Tags: Manufacturing  Overview Tata Steel is the second geographically diversified producer in the steel industry that operates across five continents. The company has been recognized as one of the largest steel manufacturers and suppliers with an annual crude steel capacity of 33 million tonnes per annum. Tata Steel focuses on a wide range of segments like automotive, consumer goods, infrastructure, etc.  Problem       One of the company’s steel manufacturing units showed potential for increasing the overall unit throughput by optimizing the internal logistics systems. The opportunity was in improving crane and ladle handling by introducing optimized process flows and testing different layout configurations.        The steel melting team saw that the utilization rate of cranes was unequally distributed throughout the production process. As a result, the throughput was impaired: there was an underproduction of heats (batches of steel), the final product of this unit. The possible reason for this was that most of the crane handling was done manually and independently. Therefore, less human decision making was required.        In the steel manufacturing unit in question, the production process starts with tanks (torpedoes) with hot metal arriving at the plant, specifically at the torpedo station. This metal is then transferred to a ladle which, in turn, is placed on a transfer car. Then a torpedo bay crane picks it from there and transfers it to the desulphurization station. After the metal is purified, it is moved by the transfer cars to the charging bay, picked by another crane, and taken to an LD converter, or vessel. Once the metal is poured into the vessel, the crane returns the empty ladle to the torpedo station.    Steel Manufacturing Unit scheme       The objective of the project was to find out if changing the existing rules of handling the internal logistics system would increase throughput. The engineers wanted to find a simple rule of thumb to operate cranes and optimize the overall production process by decreasing human dependency in decision making.       One of the reasons the team decided to go for simulation modeling as a solution was the complexity of interdependencies between the elements of that system. In addition, there were many variables with much randomness (e.g. processing time of vessels and desulphurization stations, downtime of cranes and transfer cars, etc.) that only simulation models could handle.   Solution        The first stage was to collect the input data. The team conducted a field study, where they collected the data on every piece of equipment in the unit for the past year. Furthermore, they analyzed the pattern of failure, specifically MTBF and MTTR, and process flow.          Using AnyLogic as plant simulation software, they built a model representing the process from hot metal unloading into ladles at torpedo stations to vessel charging. To set up the model’s logic, the team used the following parameters collected from the field study:    Vessels are available 87.5% of simulation time. Metal processing time at a desulphurization station is ten minutes. After that, a ladle with the purified metal leaves the station. Vessel charging time is around ten minutes and the value varies according to a probability distribution (maximum time is 15 minutes). A ladle with hot metal should be transferred from a desulphurization station to a vessel within five minutes.       The team created three different what-if scenarios with the aim to reduce waiting time for the vessels and modify the crane operation logic:     Both charging bay cranes are used for vessel charging. One crane works between the desulphurization station and vessel areas while the other crane mainly unloads metal from ladles into the vessels. If the second crane is unavailable, the first one takes over its functions.  Scenario 2 combined with a possibility to hang empty ladles at the height of nine meters above the floor.      For each scenario they designed 90 experiments, with variations in desulphurization station and vessel processing time, to observe how the quantity of heats produced per day, crane utilization rate, and waiting time of vessels would change. Every experiment simulated ten days and gathered statistics.   Outcome        Statistical analysis of the experiment results allowed the team to determine the optimal scenario from the standpoint of crane utilization, waiting time of vessels, and throughput in heats (1 heat = 1.65 tonnes). It showed that scenario 3 had a significant advantage of a minimum two heats/day which could save the company several millions of dollars per year. In addition, although vessel waiting time decreased and the throughput improved, the charging bay crane utilization was still around 80%, which was unwanted. To avoid unplanned downtime, the engineers would need to introduce a preventative maintenance plan.   Line charts with key results        Due to the opportunity to experiment with the model in a safe digital environment, using AnyLogic as plant simulation software, all the necessary changes could be implemented without disrupting production.        To further optimize the unit, the team’s goal is to incorporate the scrap charging part of the production process into the simulation. As a result, they would be able to use simulation modeling to improve the company’s steel manufacturing system as a whole. Furthermore, plans exist to incorporate AI into the model to improve policy making.   Watch the video about this case study presented by Tata Steel at the AnyLogic Indian Conference 2019     Supply Chain Forecasting and Bullwhip Effect Evaluation Using Simulation Software Link:  Tags: Supply Chains, Manufacturing   Infineon Technologies AG is one of the world’s largest semiconductor manufacturers. In 2021, Infineon reported revenue of more than €11 billion with a workforce of 50,280 people worldwide. Following the acquisition of the US company Cypress Semiconductor Corporation in April 2020, Infineon is now

 

Section 53 of 60
a global top 10 semiconductor company.   The semiconductor industry in general is characterized by capital intensity and high demand volatility. Semiconductor demand is unstable, even before accounting for COVID-19, highly dependent on innovation cycles, and is prone to the bullwhip effect.    Supply chain engineers at Infineon have been using AnyLogic for many years because simulation is an effective tool for solving production demand and supply chain problems. At the AnyLogic Conference 2012, they talked about their project on investigating the bullwhip effect in their market. At the time, they combined agent-based and discrete event modeling approaches to build a model of their multi-tier semiconductor supply chain. The supply chain model helped them better adapt to demand fluctuations and reduce the bullwhip effect.   Problem   These days, the problems of volatility in demand and the bullwhip effect are even more challenging than they used to be. During the COVID-19 pandemic, automotive demand dropped significantly which resulted in too much inventory. Demand for cars fell because people were working from home and commuting less. Later, the market rebounded, and the increased demand coincided with a global computer microchip shortage.    In the chart below, you can see the correlation between the growth of the global economy and the semiconductor market growth. In 2008-2009, due to the global crisis and economic decline, the demand for semiconductors dropped dramatically. However, in 2009-2010, economic growth recovered, which immediately affected the semiconductor market. This is a great illustration of how the bullwhip effect works and why it is a major concern in the industry.   In 2020, during the COVID crisis, growth in a semiconductor market no longer followed global economic growth patterns. It did not vary as dramatically as GDP because demand for semiconductors fell in some areas while it increased in others. Nevertheless, reducing the bullwhip effect still has high potential.   *I-Ref-M = Infineon reference market = Total semiconductor US-Dollar-based market revenues excl. DRAM, NAND Flash, MPU. – Real GDP = Inflation-adjusted (real) Gross Domestic Product of all countries of the world; a total of local values converted within each case current US-$ exchange rates. World real GDP is from the chain-weighted index. Quarterly data (year-over-year growth rates)  Solution   In contrast to their 2012 project, the supply chain engineers at Infineon decided to apply system dynamics tools to study the bullwhip effect. They wanted to compare the new results with those from the 2012 study and, this time, to understand the impact of end-market scenarios on the bullwhip effect through the whole supply chain. . Simply put, the engineers wanted to look at the same problem from a slightly different angle. System dynamics is used primarily at the macro level, where general patterns are more important than small details. Using system dynamics tools helps apply systems thinking for the purpose of identifying feedback loops, understanding fundamental problems, and looking at their symptoms.    The engineers had three main goals:    Simulate the recovery in demand from the COVID-19 crisis in the automotive semiconductor supply chain. Understand the impact of bullwhip effect for different end-market recovery scenarios. Provide a tool for evaluating collaborative efforts and habits.  To achieve their goals, the engineers completed four key tasks:    Identification of end-market demand recovery scenarios: U-shape, V-shape, L-shape, etc. Creation of a system dynamics supply chain model in AnyLogic. Testing of model using historical data. Sensitivity analysis to see which parameters have the biggest impact on results.  The end-to-end semiconductor supply chain structure can be seen in the picture below:   From right to left, the structure contains four echelons, each echelon describes a member of the supply chain:    Echelon 1: OEMs (original equipment manufacturers) Echelon 2: Tier-1 supplier Echelon 3: Tier-2 supplier Echelon 4: Semiconductor supplier  All these echelons are on a globally aggregated level, which means that the OEM represents all the original equipment manufacturers on the global level, semiconductor manufacturers describe all the global manufacturers of semiconductors, and so on. As you can see in the picture above, the information flow of this supply chain propagates upstream, while the physical flow of products propagates downstream the supply chain.    The four echelons were specifically modeled in the simulation. Each echelon passes inputs through several control loops before outputting to the next stage. Different echelons have different parameters for the same components, including forecasting, capacity, work-in-process, stock, backlog, and supply line management.    In this supply chain model, we assume that the semiconductor supplier reserves are infinite because they are guaranteed by silicon supplier.   The basic structure of the system dynamics for each echelon had several loops:   For the supply chain model’s data input, the engineers used historical data. This provided a base for model verification and for scenario testing.   Light vehicle sales faced a harsh drop during the crisis while electric content per car is gradually growing. Both factors significantly affect demand in the semiconductor market.   The end-to-end supply chain model has a simulation dashboard for choosing different scenarios and the variation of a wide range of different parameters when conducting scenario analysis:   Result   In the supply chain model results, we can see how demand drop affected other parameters.   After demand for semiconductors collapsed during the pandemic orders quickly rebound due to end-market demand recovery.   The results of the simulation model show a clear amplification of the change in the end market for light vehicle sales. The more upstream in the supply chain, the larger the drop in the received demand signal during the crisis. The recovery phase in end-market demand shows high amplification of demand increase. The incoming demand for the semiconductor echelon exceeds end-market demand by about 40% with a doubled amplification compared to Tier-2.  Inventory recovery is challenging due to long cycle times and high demand during the recovery. The high demand coming from increased vehicle production and a greater number of semiconductors in new vehicles. The result is the semiconductor shortage.   Inventory at the semiconductor echelon rises due to the cancellation of orders from downstream supply chain partners. Inventory at the semiconductor echelon cannot be made flexible due to long cycle times.  In the recovery phase of the crisis, the inventory level of the semiconductor echelon is insufficient. Due to capacity restriction and high demand from the downstream echelon, the inventory level only recovers slowly.  Sensitivity analysis for different echelons and parameters indicates a future direction for increasing overall supply chain performance. Different behavioral parameters show diverse influences on the backlog level towards Tier-2 suppliers, hence, the chip shortage for the whole supply chain. More up-to-date and lower time lag in information flow reduces the backlog level.   As a result of the testing and analyses, the supply chain engineers from Infineon obtained multiple insights:    The amplitude of the demand and its oscillation (the bullwhip effect) in the supply chain varies according to the recovery scenario and behavioral parameters of the individual echelons. Upstream members of the supply chain suffer the most from the disruption and restoring equilibrium might last longer. Reaction during demand drop affects the ability to cope with recovering demand. Collaboration and trust are important to master the bullwhip effect. Communication in terms of ordering behavior, inventory coverage, and lead time communication can improve the situation. At Infineon, a VMI KPI improvement concept has been developed to give more insight into downstream/upstream root causes.  The case study was presented by Abdelgafar Ismail and Hans Ehm of Infineon at the AnyLogic Conference 2021.   The slides are available as a PDF. For comparison with other modeling techniques, read the 2012 Infineon semiconductor supply chain study.      Supply Chain Network Design Using Simulation and Vehicle Routing Optimization Link:  Tags: Supply Chains, Transportation  Problem One of the world’s largest book distributors faced three major challenges.  Firstly, over the past decade, to meet growing demand, the distributor experienced significant growth in the number of depot locations they used to service customers. This resulted in an increasingly complex and costly distribution network. Secondly, the distributor also predicted that the uncertainty and volatility in future demand and supply would likely result in both over- and underutilized warehouse space at various locations.  And lastly, increased real estate costs and a highly competitive driver market made the current network design unsustainable and fragile to any negative stressors. The company hired two different consulting companies to do a high-level network optimization study to identify the most optimum network design and distribution practices. Both consulting companies found that using the Drop-and-Hook method and 40-foot trailers instead of straight trucks, would reduce depot locations by 25%-30%, with a Project Net Present Value (NPV) of an estimated $18m. However, both studies used calculations based on averages, raising management concerns about whether the predicted benefits would be realized under real-world conditions.  Solution Goldratt Research Labs (GRL) was contracted to validate (or invalidate) the key assumptions of the two studies as well as the predicted operational and financial performance improvements using dynamic simulation modeling.  They were also asked to use the simulation model to determine, using scenario comparisons, if the operational and financial results could be improved with different supply chain network configurations, transportation practices, and fleet setups. To fulfill the objectives, GRL engineers developed a fully self-configurable Supply Chain Digital Twin (SCDT) simulation model that could consider all critical system interdependencies, constraints, complexities, and demand and supply variability. This model was designed to determine the range of likely outcomes for a single scenario and provide sensitivity analysis and direct scenario comparison for different supply chain configurations.  The alternative configurations evaluated included different customer and depot locations, customer-to-depot allocations, and alternative fleet configurations, e.g., smaller straight trucks versus bigger trailers and tractors. The resulting SCDT model gave the company’s management team a low-risk, low-cost way of evaluating the likely operational and financial impact of all the proposed changes and scenarios BEFORE investing significant CapEx and time. This would ensure they would avoid the risk of reputational and financial damage if the project did not deliver the expected benefits. GRL chose AnyLogic as the best-in-class simulation modeling software to develop the SCDT model. The main reasons for their selection included:   To replicate the customer’s real-world complexity, both agent-based and discrete event simulation methods were needed. The model had to be developed in a short amount of time and offer a wide range of system configurations. Considering the large number of scenarios to be evaluated and the model's size, the customer wanted to run the simulation model on their own computers and have access to a cloud option. Not only can AnyLogic models be exported as a standalone app, but they can also make use of the AnyLogic Cloud to complete resource-intensive model runs much faster.  Considering the range of scenarios to be evaluated, the simulation model had to incorporate a Route Scheduling optimizer to determine, for each scenario, the optimized final mile routing of vehicles from depots to customers. AnyLogic’s use of standard Java libraries allowed GRL to develop a simulation model that was fully integrated with a third-party app like PTV’s Route Optimiser, which could generate the optimum routes in a remote server and send the results back to the model to be stored with the scenario file for later use.   The full SCDT model setup included the inputs, the outputs, different analysis options, and the scenario analysis resulting in the final model reports and logs. The model is completely data-driven from an Excel file, making it really easy to change scenarios right inside the file. The scenario files contain the following input data:  Geographical information Depot setup Customer order data Fleet setup and assignment Financial and operational parameters  Simulation model setup  Using these inputs, four main analysis options can be conducted:  Distance Calculator The distance calculator was built to pre-calculate all the distances required during any of the other analysis options to reduce the execution time.  Allocation Optimizer This feature enabled the company to optimize the customer to depot allocation using a simulated annealing algorithm. This algorithm reduces the stem miles while considering the limited capacity of depots. Route Scheduler In this mode, the model would generate realistic final mile routes for delivery of products from the depots to the allocated customers using the PTV Route Optimizer. Scenario Analysis This last step in the analysis process is the culmination of all the inputs from the previous steps of supply chain simulation The final result from this analysis is a detailed operation and financial set of results for every location, vehicle type, and customer location.     Some of the option’s outputs can be used as the input to other analysis options.  Simulation model interface  Results The AnyLogic-based SCDT simulation model results revealed a number of key findings for this project. The model identified several flawed assumptions in previous studies. The most consequential being that the drop-and-hook method was not as cost-effective as predicted. In fact, it showed that the predicted NPV of $18m would never be realized. The NPV was likely to be negative $5m. That would mean that the CapEx investment would never have been recovered. With this unexpected and concerning conclusion, the new project objective was to identify an improved supply chain network design and fleet setup that could improve their current performance, while adding resiliency and responsiveness to the expected uncertainty in future demand and supply.  Using their new SCDT model with the embedded Route Optimizer, the team identified a solution that would result in an NPV of $47M, even higher than originally

 

Section 54 of 60
thought possible. In addition, the ability to easily modify parameters inside the model and the detailed output files, made it a decision support tool to additionally check assumptions on:  Required driver’s class (for small or big trucks) Required number of drivers (depending on the seasonality, per month and per location) Number of vehicles per day  Details regarding the number of overnight routes, driver hours, and overtime  The customer has indicated that the model will continue to guide their strategic decision-making in the industry and continuously changing environment. The case study was presented by Dr. Alan Barnard and Jaco-Ben Vosloo, of Goldratt Research Labs, at the AnyLogic Conference 2021. The slides are available as a PDF.     Supply Chain Optimization: Choosing the Right Location for Warehouses Link:  Tags: Supply Chains  anyLogistix supply chain optimization system success story. Problem Eldorado Company, a huge electronics retailer in Russia, with stores in 350 cities, needed to determine the optimal number of warehouses, and where they should be situated, in order to better fulfill customer demand and minimize delivery and storage expenses. The analysis showed that the problem could be solved with introduction of AnyLogic Logistics Network Manager (now part of anyLogistix supply chain optimization system). Input data provided by the customer described potential warehousing points: rent cost, investments for building new or modernizing old warehouses, average level and cost of storage, overall costs for staffing and security, etc. The simulation model included in the solution also considered the warehouse and retail store GIS coordinates, and distances between cities.  Solution        The introduced system allowed the client to simulate, in detail, several kinds of activities:   Daily basis (model time): goods are sold in stores, and losses from the shortage of demanded goods are counted. Weekly basis: inventory is supplemented to target level, transportation costs are counted, and deferred payments to suppliers are planned. Monthly basis: warehouse levels are renewed according to monthly sales levels of stores, transportation routes from warehouses to stores are generated, and franchisee shipments are planned.  Monthly sales numbers conform to average sales numbers, while daily sales are generated stochastically.    Users can carry out several experiments with the model. Parameter variation experiment checks all possible scenarios of warehouse positioning, taking into account “fixed” warehouses and their maximum number. The result of such an experiment is the best combination of warehouses that cost the least amount of money. Based on this information, optimization experiment calculates in-store warehouses’ floor space. In a simple experiment, a user manually chooses warehouses from the list, and launches the model with this combination in order to receive statistics about it.  Outcome The solution allowed the customer to choose the optimal positioning variant of a warehouse network out of 63,000 combinations. The software implementation costs are paid off during the first two months of work when using the distribution network system recommended by the model. The decision support system is expected to operate for a long time, as it allows the users to find new optimal distribution system setups in case the market situation changes (change of transportation tariffs, warehouse parameters, amount of stores and sales, etc.).     Supply Chain Planning for Vaccine Manufacturer with Simulation Software Link:  Tags: Supply Chains, Healthcare  Challenge GlaxoSmithKline (GSK) was the world’s sixth largest pharmaceutical company in 2014. The company was launching a new vaccine product on a new market that needed a distribution network different from what they had before. Therefore, the company needed to design a new supply chain and align manufacturing processes with it.  The supply chain of a vaccine is a complex system because it is geographically extended globally, and it uses a large amount of resources, including manufacturing plants and warehouses. Production and distribution start with the preparation of “bulk”, where many different components are used. This stage is then followed by the “filling” step, where many containers, such as syringes and vials, have to be managed. Then the product goes to final packaging (with specific requirements for all different regions, in compliance with local laws). In addition, in the GSK case, the vaccine could only be produced and released for distribution in batches, which made further supply chain planning even more difficult.  Process Logiс in the Model  A specific issue for the company was how to handle large amounts of components with specific expiration dates, while complying with quality control procedures (both internal and external at “single dose level”), and commercial rules. In addition, GSK implemented various inventory management policies using corporate transactional systems, including SAP. Above all, their supply chain faced a large amount of specific settings and balancing problems.  All of these special traits were the reason that GSK choose dynamic simulation as a decision-support tool for the vaccine supply chain design. Fair Dynamics Consulting developed the supply chain’s simulation model for the company using AnyLogic.  Solution The model designed by Fair Dynamics simulated GSK’s supply chain, including the manufacturing and the distribution parts of it.  The manufacturing part was based on the discrete-event modeling method and it simulated business processes involved in the vaccine production. These included three levels of processes that could be intervened by each other:   Manufacturing processes  Quality control processes (product testing)  Quality assurance processes (production process control)  The model had to take into account the different policies the company implemented and production constraints. For example, the model included shifts of human operators at the plant, their operation times depending on experience, as well as different sourcing policies that affected the production schedule.    Model Statistics  The manufacturing part of the model was integrated with the distribution part that simulated the US market supply chain, orders to the manufacturing part, and received goods from it.  The supply chain in the model reflected the real-world supply chain design, and included warehouses replenished from the main European distribution center, wholesalers (product distributors), and hundreds of clients, all with defined geographical locations. Clients ordered goods from wholesalers, but were supplied directly from GSK’s warehouses, due to the sensitive nature of the product and to avoid delays caused by intermediaries’ participation.  One of the important metrics taken into account was maintaining high service levels, because the product’s selling proposition was guaranteed 24 hour delivery.  The model was able to simulate both the steady state of the supply chain, and situations with disruptions and emergencies, to see how “what-if” scenarios would affect supply chain performance.  Outcome The model allowed GlaxoSmithKline to determine the optimal vaccine supply chain design from the standpoint of costs and service level.  The model also served as a part of an operational decision-support tool for the supply chain planners. The tool allowed them to determine optimal production/distribution policies for the next week/month.  The uniqueness of this project was the combination of manufacturing and distribution processes in one simulation model, which was possible due to the multimethod modeling techniques available in AnyLogic. This approach gave GSK the ability to achieve more accuracy in simulation, thus allowing for more precise forecasting and more profitable decision making.     Tackling Retail Out-of-Stock with AI Link:  Tags: Warehouse Operations  Challenges       Element AI turns cutting-edge AI research and industry expertise into software solutions that exponentially learn and improve. The company was looking into potential ways to combine simulation with AI and identified three challenges to address with the assistance of AnyLogic:    Can simulation help generate valuable datasets to pre-train an AI model? Can AI help improve the behavior of agents in the simulation? Can reinforcement learning techniques be applied to real industry use cases?      It’s important to underline that deep learning usually requires very large datasets for model training to be successful. Rather than building extensive rule-based systems, neural networks, a family of models used in AI, learn from vast amounts of data by making decisions that are in accordance with as many data points as possible. However, the data can be affected by several issues:    be insufficient or missing, be biased, be sensitive/private and thus require anonymization, take a long time to procure access to (for client engagements), require cleaning, require pre-processing to match the AI model.      For all these reasons, when an AI team wants to engage with a client or build a core capability, it needs to figure out data requirements early in the project, either through a data audit phase or a data gathering and labelling phase. This step can take a long time and be quite costly. Therefore, it was the first challenge for Element AI to tackle with simulation.       As a second challenge, Element AI looked at how AI can help simulation make better decisions and discovered a few interesting applications:    AI can act as a brain for the simulation agent so it behaves with better insights, Simulation can be a testing ground to compare AI models, Simulation can help build a visual demonstration of the added value of an AI solution, Simulation can help prepare AI for irregular operation scenarios at a much lower cost, Simulation can be used by domain and technical experts to reach a common understanding of a problem and de-risk a project early, Simulation can be combined with AI in a digital twin environment.      Finally, because of the untapped value listed in the first two challenges above, Element AI decided to delay industry testing of reinforcement learning techniques for a later date and case study.   Solution   Element AI store simulation model        In order to tackle the identified challenges, Element AI selected an industry use case that would benefit from the use of the simulation ideas highlighted above. The company focused on replicating the operations of a grocery store; more specifically, the focus was put on product demand forecasting and employee task prioritization for shelf replenishment.       The first objective was to generate 5-years’ worth of minute-by-minute product demand data with significant variability, noise and irregular events. The key was to create enough data for the time series forecasting (AI) algorithm to learn from at a complexity level that warrants the use of AI over traditional rule-based formulas.       A known risk with this approach is that an AI model can overfit to the simulated data and fail to generalize when new parameters are inserted or when compared to real-life data. This is generally overcome or minimized with domain randomization.       The second objective was to use AI to guide employee task prioritization in the simulation. More specifically, help virtual employees (agents) who are responsible for the grocery store shelf replenishment tasks know what product they should prioritize to avoid or minimize costly out-of-stock events.       To investigate these two objectives, the grocery store simulation model included three basic agent types, each with a set of randomly seeded parameters, to ensure the desired level of complexity.       1. Clients modelled as pedestrian agents with various:    walking speeds product shopping lists arrival rates (per hour, day of week, month of year, year to year, null on national holidays)  cart abandonment tolerances      2. Product categories with various:    demand distributions availability restock thresholds time required to restock prices & margins physical locations in the store weekly promotions      3. Employees with various:    roles availability schedules task priorities  Finally, Element AI synchronized their AI models with the simulation execution.       The developed solution involved four simple steps:    run a few minutes/hours of the simulation pause the simulation to output the stock levels of each product to a text file raise a flag for the external AI module to:   process the information  return a list of prioritized products to restock (tasks) in a text file    resume the simulation with the employees working based on the new priorities      The main benefit of this approach is that it is agnostic to the level of complexity of the AI and the coding language (Python in this case), at the cost of having to briefly pause the simulation at regular intervals.   Outcome       The 5-years of data generated from simulation allowed Element AI's scientists to train time series forecasting models for minute-by-minute product demand. This was done using a split of the data, where the first four years were used to train the AI, and the fifth year was used to test the accuracy of the forecast.       The baseline for hourly product demand forecast was set as Lag-0 which predicts that the previous hour will repeat itself. The accuracy of other time series models was then evaluated against that baseline.   Time series model comparison for hourly product demand forecasts. Baseline Lag-0      The results listed in this table illustrate that a store manager trying to predict what will be sold in the next hour would be 61% accurate if he/she used the past hour as a reference. This same store owner would be up to 80% accurate if he/she leveraged an AI demand forecasting tool.       However, this conclusion comes with a caveat. Although there were multiple sources of variability and complexity introduced in the simulation model, the generated data still lacked realism.       For example, there were no irregular events that could force the store to close for a period of time, there were no employees who failed to show up for work, there was always enough product in the back store to refill the shelves, and ultimately the demand forecast problem was simplified by avoiding the introduction of new products.       For these reasons, it could prove tricky to use the data generated by this simulation for data augmentation because the AI has not learned to handle additional noise and could be unable to adapt to real-world data. This is where domain randomization techniques and recent progress in sim-to-real or transfer learning would be beneficial.       However, even if it can’t guarantee the results once exposed to real data, the AI trained using this simulation approach can aid

 

Section 55 of 60
researchers in ruling out unsuitable forecasting models and evaluate whether a use case would benefit from additional sensors or data sources.       As identified in the second objective, the simulation model also allowed Element AI to compare the impact of various AI policies for task prioritization based on a set of metrics per defined time period (day, week, month, year):   Sample results for the daily profits of the store  total revenue total profits total clients wait time total out of stock events total revenue lost from out of stock events total abandoned carts & items      The queue policy means that products are restocked in the order that they run out. The other policies are then compared to that baseline to evaluate the optimal policy for profits but can also be used to optimize for other KPIs like employee time utilization. As it turns out, under the defined parameters, the prioritization of tasks has far less impact than the ability to forecast demand, but different parameters or datasets might lead to a different conclusion.       In the end, simulation was used to generate data to improve the forecasting ability of the AI and as a testbed for different AI agent policies. Once deployed in a retail store, this solution could help a store manager gain a better understanding of how many products are expected to sell per hour and where employees should focus their shelf replenishment efforts.       Overall, this project allowed Element AI to become familiar with the world of discrete event and agent-based simulation, to engage clients in a new way, and to generate data for internal teams. Most importantly, the use of simulation gave Element AI a building block to be leveraged when tackling more complex future projects that involve reinforcement learning, sim-to-real, transfer learning, and digital twinning.   Discover more about combining simulation and artificial intelligence.             Tire Manufacturing Simulation to Increase Productivity Link:  Tags: Manufacturing  Fate is a leading Argentinian company in tire manufacturing and exporting. They have the largest and most modern tire industrial plant in Argentina, with more than 2,000 employees producing more than 5 million tires a year.   Problem Fate wanted to adjust their transformation processes to increase productivity. So, they needed a flexible tool which would be able to analyze the performance impact of different scenarios. The problem was that this tool needed to be developed in a complex system considering standardized logic and criteria throughout the plant.  The project included the passenger production facilities and three production sectors: material, tire-building, and curing.    Fate plant production sectors   In general, they were not looking for a solution to an exact problem, but rather they wanted to take a holistic view of the plant. The model would allow Fate to analyze production volume and needs, workforce availability, workforce task and machine assignment, bottlenecks, and so on.  Solution    Fate contracted Eurystic to develop their model and simulation. AnyLogic was chosen to build this model and run the simulations because it could deal with complex and time dependent systems. Also, it was flexible, and dashboards could be built quickly. In addition, it had random components such as machine failures. AnyLogic enabled developers to run multiple iterations and custom experiments in order to compare different scenarios.     From a high-level perspective, the digital representation of the production system took an input to process and configure the simulator. The simulator then ran and showed the current and cumulative results through the user interface dashboards. This information could then be exported to another program.  The model used a combination of agent-based and discrete-event processes. Agent-based relied upon statecharts for agents such as machines and transportation staff and actioncharts were used for complex internal material handling logic. While discrete-event was the best approach for modeling the conveyor belts.  A simulation of this scale needed an extensive set of inputs and parameters as can be seen below.    Inputs and parameters (click for more details)   Some of the inputs required were stored in their enterprise resource planning (ERP) software, but others were collected manually, organized, and given a structure for the model.  It was important for this model to be constantly updated with the company’s latest information through ERP and reporting activities so that it could be used regularly.  The production schedule for all of the machines, except for in the last process, was determined by the logic and parameters of the model. The logic needed to be collected and standardized for each machine and the final rules of the model followed.  Transportation logic was similar to the production scheduling where rules were defined and priorities given in order to optimize the product flow.  Once the simulation was complete, the information was exported to spreadsheets and then compared with real-world results. After this, further analysis could be performed with the help of business intelligence software.    Model integration with business intelligence software   The model for the presentation used imaginary scenarios to protect confidentiality. Below are examples of the user interface.   User interface examples from the simulation (click to enlarge)   Results In many cases, multiple iterations needed to be performed to arrive at the final answer. Results and potential outcomes included the following:   Predicting production planning completion.  Visualizing bottlenecks.  Determining plant capacity.  Evaluating human resource allocations, impact of bottleneck improvements, and improving bottlenecks.  Optimizing internal logistics.  Testing unification of materials and components as well as testing and assessing different strategies.   Thanks to the simulation many ideas could be tested. Those with the highest impact and lowest cost could be further analyzed and potentially implemented in the future.  The case study was presented by Damian Marino of Eurystic and Alejandro Paz of Fate at the AnyLogic Conference 2021.  The slides are available as a PDF.     Transfer Hub Passenger Flow Simulation at Moscow Ring Railway  Link:  Tags: Passenger Terminals  Problem            Moscow Ring Railway, built in 1908, crosses city outskirts, residential districts, and industrial areas. It is currently being used for freight traffic only. The launch of passenger trains on this railway is planned for 2016, and is expected to drastically improve the road traffic situation and congestion on public transportation. Many of the newly built stations will be connected to subway and commuter rails by huge transfer hubs. Passenger flows, at one of the future hubs, were simulated using AnyLogic software by the ITS Consulting Company for JSC Moscow Ring Railway.            The aim of the simulation project was to evaluate the working performance of the Cherkizovo hub at peak passenger load times in order to discover the construction elements limiting the capacity of the hub (corridors, stairs, escalators, ticket barriers, ticket windows, fare boxes, etc.). Hub performance was considered acceptable if passengers didn’t experience difficulties in moving along the hub and buying tickets during the peak hour of 8:00 to 9:00 in the morning.   Solution            The Cherkizovo hub model used both the Pedestrian and Rail Yard libraries of AnyLogic. 3D animation capabilities of the software were used for system visualization. The model integrated the projected railway station and the existing subway station and featured the following elements:     Northern and southern railway station terminals.  Northern and southern subway station halls.  Northern and southern crosswalks between railway and subway stations.  Subway and railway station platforms.            Passengers could arrive at the terminal by foot, or change between railway, subway, and ground transport. Passenger movement algorithms can be found in the figure below.   Entrance and exit algorithms.            To reflect the system correctly, the modelers had to take into account the following peculiarities:     Nonuniform distribution of passengers between station terminals. Train arrival timetable. Nonuniform distribution of passengers in trains (coaches closer to exits carry more passengers). Percentage of passengers with or without tickets. Percentage of passengers buying tickets at ticket windows and fare boxes. Consideration of people eligible for buying reduced fare tickets (available only at ticket windows).            The input data used in simulation included:    Characteristics of trains, coaches, escalators, platforms, and ticket barriers.  Passenger flow distribution in entrances, exits, ticket windows, fare boxes, and distribution of passengers in trains and at platforms.  Service time of one passenger at ticket window and fare box, ticket barrier usage time, train stop time.  Passenger speed and speed of train moving along the platform.   Outcome            Simulation showed that the planning concept of the Cherkizovo transport hub operated well with peak passenger loads, and all the construction elements had a reserve capacity. However, if the number of passengers increased, bottlenecks were expected to appear close to ticket windows because the ticket windows were situated along the main route of passengers on their way to ticket barriers.            To optimize the ticket windows’ load, and decrease passenger service time, the client was advised to encourage passengers to buy tickets at fare boxes or use other methods to purchase tickets. It was also recommended that the client decrease the number of fare boxes, as some of them would not be used even if some passengers switched to them from ticket windows.            The Cherkizovo model will be employed by JSC Moscow Ring Railway for finding optimal parameters of other transport hubs. The model will also be used for researching passenger behavior in emergencies and during evacuations.     Transport Planning Simulation Tool for Suburban Public Transportation Scheduling Link:  Tags: Transportation, Rail Logistics  Problem        In CIS regions, passenger railway traffic is a highly specific field. Suburban railway routes are often cut due to their unprofitability. This may have a negative impact, not only on the passengers, but also on the social and economic conditions in the regions.        The Russian Central Federal District authorities faced the rail routes reduction problem. They decided to launch alternative bus routes simultaneously with the cancellation of the railway routes in order to reduce adverse impacts.        The districts authorities entrusted the planning of alternative routes topology and determining the economic effect of their launch to Focus Group company’s consultants. The consultants applied transportation simulation to test new routes and evaluate their effectiveness. The simulation would allow them to formulate and analyze various transport topology options, observe its operations in runtime, and evaluate their financial efficiency in a safe digital environment.        Thus, the consultants required a transportation optimization model that would serve as a decision support tool, instrumental in:    Reflecting the current transport situation in the region. Scheduling alternative routes, taking into account the passenger traffic. Predicting the transport situation in what-if scenarios. Assessing the economic effect of planned changes on the regional transport system.   Solution       Having analyzed the simulation software market, the consultants opted in favor of AnyLogic transportation optimization software for building the transport and logistics model due to several reasons:    AnyLogic is the only commercial transport planning software that supports agent-based modeling. GIS maps can be integrated in AnyLogic-models and applied for animation. AnyLogic transportation optimization model can be exported as a standalone Java-application. AnyLogic allows for the use of Excel templates and databases for data input and output.       With an agent-based simulation approach, the consultants were able to prescribe the behavior of such typical model objects like localities, trains, buses, railway stations, and bus stops, and set rules of interaction between them. In addition, objects location was specified in the model with Excel sheets. This approach significantly expedited the model building. All objects were located within the region GIS map. Information on real rail and road routes loaded in the model automatically.   Transportation simulation and optimization                model interface (click to enlarge)  Transportation simulation and optimization model:               railway stations and bus stops (click to enlarge)       When the model was set up, the consultants tested out various transport network topology options, considering financial indicators. In that way, they managed to figure out which alternative routes would be unprofitable, and which would be cost-effective.   Result        Six alternative bus routes were worked out as a result of the transportation optimization project. The economic analysis indicated that four of these routes would be commercially unfeasible. This information would enable the customer to make informed decisions concerning the launch of new routes.        The model was handed over to the customer as a customized standalone decision support application for transportation system analysis. The input data, such as traffic flows, can be adjusted, alternative routes can be tested out, and their effectiveness can be compared with the effectiveness of the existing ones. This application can also be reconfigured for the scheduling of transport routes in other regions.        This is a simplified version of the project's model:       Transportation Company Integrates Intermodal Railway Simulation and Management System Link:  Tags: Rail Logistics  Problem       In 2012, a railway operator decided to create a strategic railway network to embrace the need to increase the scale and volume of cargo traffic. This network was comprised of three railway ports and six transport and logistics terminals (TLTs). The network was aimed at enhancing the efficiency of cargo transportation. The first stage of launch would increase cargo traffic by 100-120 million tons per year.     TLT Bely Rast         The first pilot TLT Bely Rast was constructed in the Moscow Region. Bely Rast became a large-scale multifunctional hub occupying an area of 180 hectares. The anticipated cargo turnover reached 11.5 million tons per year. The work of the TLT involved consolidation, transit, storage, distribution, customs clearance, and temporary storage of containers and cargo. The TLT included

 

Section 56 of 60
container and liner train terminals, customs facilities, terminals for processing large and heavy cargoes, building materials, vehicles, a system of warehouses, and a business center. So, the TLT was both a multifunctional system and one of the elements of the modernized railway network. This meant that Bely Rast was a complex facility with a great number of internal and external functional links.       The success of the project depended greatly on the local logistics efficiency and the proper operation of every TLT element. Basically, there are no universal railway yard models for TLT design and analysis. That is why when creating a TLT, engineers applied the experience of already existing terminals.       The railway operator entrusted the examination and testing of the proposed solutions to the engineers of SPC Infotrans. This is an enterprise engaged in the full cycle of creating technical facilities to diagnose the railway infrastructure. Infotrans experts suggested applying the AnyLogic railway simulation software. Simulation modeling allowed them to track many parameters, both at the design stage and at the operational stage.       The TLT functional model was implemented to enhance internal relations at TLT Bely Rast. This was achieved by:    Technological solutions efficiency check Layout solutions efficiency check Diagnosed faults correction      The built-in AnyLogic Rail Library for detailed rail operations simulation permitted engineers to model the micro level of train performance. The AnyLogic railway simulation software also provided options for modifying and expanding the models with the help of Java programming language, allowing them to adjust the TLT model for the specific project tasks and setting.   Solution       The railway simulation was applied to enhance the efficiency of the continuous cargo traffic inside the TLT and to find the best logistics solutions. This determined the choice of discrete-event modeling.       The TLT model included the following basic facilities:    Intermodal terminal Inert cargo terminal  Loading/unloading vehicles terminal Specialized warehouses (cross-docking facility included), providing for adjacent checkpoints and their entry points Local road infrastructure, providing for the checkpoints and parking places    Railway simulation model logic and animation (click to enlarge)        It was possible to set parameters at different logical levels of the railway simulation model and conduct experiments. With the model, the experts were able to manage train arrival schedules, as well as change:    The number of wagons in the train. Their load rate and the percentage of unloading. Technical specifications for elevating transfer vehicles (load capacity, quantity, efficiency, etc.). Parameters for separate infrastructure facilities (storage capacity, number of gates for loading/unloading, etc.). The structure and purpose of the container flow.   Railway simulation tool interface with parameters for railway optimization (click to enlarge)        The additional option of entering source data into the model using third-party resources, for example, databases and Excel data sheets, enabled users to consider not only simple numerical parameters (the density of daily train flow), but also complex structural data (train schedule).       The dynamic modeling made it possible to track changes in local indicators for terminal transport (number of tractor units, maximum queue size, delay time at the checkpoint, etc.), warehouse and terminal capacities load, and the number of operations performed with containers by type of operation.       Random events algorithms (equipment breakdowns, queues, weather conditions) and ‘what-if' options helped predict the consequences of unforeseen situations and evaluate the interrelated dynamics of the TLT indicators.   Result       As a result of the railway simulation and management model implementation in the company’s workflows, the engineers managed to:    Elaborate technical specifications of storage areas. Coordinate the flow of containers and terminal transport. Define technical requirements for different elements of the TLT information support system. Create a system for planning and dispatching traffic flows. Create a tool for choosing optimal solutions for further operation.      Model implementation also disclosed critical limitations of the infrastructure. The insufficient traffic capacity of the checkpoints, transport cross flows, and the points of the adjacent road network junctions were identified and corrected. Such restrictions potentially could lead to queuing and overload in particular TLT zones, which would significantly reduce the efficiency of the entire TLT as a whole.       Apart from that, experts came to the conclusion that the railway planning model should be used for further needs:    To integrate the TLT Bely Rast in the system of external logistics links.  To optimize the adjacent transport infrastructure. To create a macro-model for monitoring and optimizing the interaction between all TLTs of the planned network, as well as railyard management.     Transportation Optimization with Simulation Software – Port of Long Beach Pier B Rail Support Link:  Tags: Transportation, Rail Logistics, Ports & Terminals  Problem The Port of Long Beach, California, planned to invest up to $1 billion dollars in its Pier B Yard rail facility. TranSystems, with over 25 years of experience in the transport and supply chain space, worked with the port to quantify system requirements to meet capacity goals and to gain stakeholder consensus. The proposal was to reconfigure, expand, and enhance the existing Pier B rail facility to support growth and provide a more efficient environment for on-dock rail at the port’s shipping terminals. Rail cargo movement via on-dock rail is the most efficient method for moving containers to long-haul destinations. To support projected capacity requirements, it is important to utilize on-dock terminal tracks and improve air quality in the area. Modeling the rail system of the port was necessary to demonstrate to the stakeholders the required changes. However, previous modeling attempts had failed because the tools and approach had not captured the system appropriately. Success depended on reflecting the real challenges in the port rail system and showing how a new rail facility would improve operations and provide the necessary capacity. In addition to the need to capture the current dynamics of the system, was the requirement to filter the many design concepts produced by the engineering team. To meet these requirements, TranSystems selected AnyLogic because of its ability to:  Represent the real-world operating constraints and situations of trains within the port. Visualize train movements to communicate system operations.  Schematic System Level Animation. Solution To support the modeling, TranSystems conducted site visits and interviews for all the marine terminals and rail operators at the Port of Long Beach. Furthermore, they reviewed operations, both in the port and regionally. As a result, they gained an understanding of the challenges that needed to be met. These challenges included:  Insufficient rail cars to support import/export imbalances. The ability to support multiple rail carriers and terminals. Storage and switching activities. Longshore union work rules. Activity support: inspections/repair, brake air testing, fueling, crew changes.  When developing the model, a multi-step approach was taken: Step 1: Screening of major alternatives by sizing the facility based on the project goals, using a quick and unconstrained system. Step 2: Conceptual layout, constrained to determine the operating rules and tradeoffs needed for realizing the capacity. Step 3: Visualization to port stakeholders. Finally, to be true to the real world, systems level analysis with multiple facilities and operators was carried out. This ensured all the facilities would work together. AnyLogic was ideally suited to integrating the wide variety of systems and variables in one model. The appropriate modeling approach (agent-based, discrete event, system dynamics) could be used for each element and seamlessly integrated into the system.    Measure of Success: Operational Sustainablility.   Custom rail and fluid libraries are available to assist the modeling of port systems. TranSystems also harnessed the flexibility of the software to create their own custom libraries and further ease the modeling process. Outcome The measure of success for the rail system was its operational sustainability. This required testing the ability of the system to deliver and remove containers while avoiding unstable build-ups. The results allowed the project to move forward. After success at different buildout and volume goals had been demonstrated, stakeholders approved the necessary design. Furthermore, as the project entered the environmental review stage, the model was successfully redeployed for air quality reporting.  To learn more, watch the project presentation at AnyLogic Conference 2016 or download it.     Transportation Provider Carries Out Warehouses’ Layout Optimization with Simulation Software Link:  Tags: Warehouse Operations  Problem Delovye Linii (DL) is one of the largest carriers in Russia. The company has 180 sites throughout the country and arranges shipments to 1,500 towns. In total, it uses approximately 540 thousand square meters of warehouse space and this figure is still rising. Clients' base widening and cargo traffic increase resulted in the need to enhance the efficiency of the company’s logistic processes. The company’s management defined two main tasks:  Increasing the warehouse operation speed and efficiency Increasing goods rotation in the warehouses  The warehouse space and operations optimization would initially be implemented at the company's largest distribution centers. Solution Distribution Center in St. Petersburg A large warehouse complex in St. Petersburg was the first site to be upgraded. Elevating the cargo reception process was crucial for improving the efficiency of the whole warehouse. It was assumed that the warehouse’s layout optimization would solve the problem. Simulation modeling was going to be applied in order to analyze various possible scenarios without interrupting the warehouse operations. Management realized that conducting experiments in a digital environment would be much cheaper and faster than experiments with a real warehouse. Two models of the warehouse were designed in AnyLogic simulation software. The first model reflected the current structure. It allowed engineers to verify the accuracy of the simulation and the input data. The second model displayed potential warehouse’s layouts. The engineers altered the warehouse's topology and conducted experiments until they figured out the optimal warehouse layout option. Taking into account the experiments’ results, they put forward a proposal to move the packing area closer to the unloading gates. The warehouse layout optimization enabled DL to reduce the empty running. As a result of this the handling of goods cost was cut by 10%.  Distribution Center in Novosibirsk The distribution center located in Novosibirsk is another large warehouse complex of DL. It covers a total area of 20 thousand square meters. Long distances impeded the rapid dispatch from the warehouse and contributed to staff being overburdened. It was crucial to reorganize the storage of the goods in the warehouse in order to increase the dispatch speed and the warehouse operation efficiency.  The operational support services offered several appropriate configuration options. Simulation modeling was applied to test these hypotheses. Models of all warehouse’s configuration options were developed with AnyLogic simulation software. These models thoroughly reflected the warehouse operations including parameters such as empty running, staff productivity, and use of the gates. As a result, the optimal warehouse layout was selected, and the cargo slots were redistributed as close to the overload gates as possible. With the new warehouse topology, both the technological operations speed and the goods rotation increased. It was also instrumental in optimizing the loaders work, and its productivity increased by 10%. Outcome Simulation modeling implementation enabled DL to enhance the efficiency of its distribution centers in St. Petersburg and Novosibirsk. Having carried out several successful projects, the company began to implement AnyLogic simulation software for modeling new warehouses in addition to the existing ones. Now, in order to design a warehouse, engineers create its simulation model with several topology options and specified technological processes. The model allows them to consider all operational aspects of the planned warehouse, including processing of goods and loading of the areas, cells, and resources. Then they calculate indicators for each option, compare them to select the optimal warehouse layout and improve the technological processes coordination scheme.  Warehouse logistics is not the only branch where DL intends to use AnyLogic simulation software. The company is determined to use simulation modeling for financial planning and business process management.      Truss Factory Simulation to Improve Productivity Link:  Tags: Manufacturing  Overview  Bryden Wood is a global company of creative technologists, designers, architects, engineers, and analysts. They have integrated expertise in the theory and practice of Modern Methods of Construction (MMC), the Platform approach to Design for Manufacture and Assembly (P-DfMA), and automation in construction. Problem  The FASTtruss project is aimed at developing a Design for Manufacture and Assembly (DfMA) solution for off-site manufacture of steel-tube lattice-trusses. FASTtruss is a system of standardized truss systems, parameterized for mass customization and scalability across a wide range of applications.   The problem is that lattice trusses are often bespoke and manufactured with intensive labor input, manual handling of materials, and are limited in repeatability.   The construction industry is typically slow to adopt new processes and technologies. Thus, it has had little improvement for decades, whereas other sectors have increased productivity. Traditionally, everything is focused on-site using bespoke elements and processes. This means poor efficiency and an inability to repeat and scale.   Truss fabrication and assembly can be done in an off-site factory with high levels of automation, including robotic cells. Storage, shipping, and on-site installation are faster and more efficient, with reduced working at height improving safety, when using automation. As a part of the digital workflows, simulation could play a vital role in these projects to make them faster and smarter.  Solution  Bryden Wood,

 

Section 57 of 60
in collaboration with Tata Steel and the Advanced Manufacturing Research Centre, developed a simulation model of the factory system. Bryden Wood believes that simulation is a core approach for the future of the construction industry.   For the previous research, Bryden Wood used some assumptions and data. Their early-stage initial model considered only core process timings with no constraints on the availability of resources (e.g., people, additional equipment) and no equipment breakdowns or maintenance. Also, no changeover times were considered between truss types in each welding stream. Brace and chord grinding areas as well as painting bays were assumed to have unlimited capacity without bottlenecks.   Then Bryden Wood decided to create an optimized model. AnyLogic was chosen for the simulation because of its built-in libraries, which provide a fast and flexible platform for development. Factory simulation software enables stakeholders to analyze manufacturing systems, evaluate the impact of system changes, and make informed decisions. As the simulation is built using knowledge from existing mathematical models, it allows further dynamic analysis.   In the picture below, you can see the optimized model for manufacturing that was a part of the automated design workflow.   Automated design workflow (click to enlarge)    The simulation structure was based on the latest process flow and facility layout. The model scope was from raw material entry to finished trusses.   Product demand is the throughput that must be met by production. Trusses consist of chords, braces, and connections. These trusses can be assembled in 2 different ways: rafter or transfer.   In the picture below, the approximate factory layout is demonstrated.    The truss factory layout (click to enlarge)    Inputs for the model included the following:    Process flow, cycle times and timings  Truss types, dimensions, and quantities  Production schedules  Logistics   Outputs were the following:    Accumulation and sizing of buffers  Number of processing stations  Factory throughput  Indications of bottlenecks   The Process Modeling and Material Handling libraries were primarily used for building the model. The developers used conveyors for the main material transport routes, with conveyor stations representing processing equipment (e.g., cutting, drilling, inspection). Additionally, welding stations were modeled using a series of nodes. Also, cranes and forklifts, as well as painting and storage areas, were included in the simulation model. In the AnyLogic model, the developers could easily represent the correct capacities and dimensions.    The parts of the factory layout in the AnyLogic model    In the process of transferring materials to agents, Bryden Wood decided to simplify the model by batching raw materials and components together. In the real system, a single truss type (including all of the chords, braces, and plates) will be converted in individual batches with the correct parameters and dimensions. It will also be generated in the model based on the production schedule. They will be processed through the different streams depending on the type.  Results  Using AnyLogic simulation software, Bryden Wood was able to test a few different scenarios. They excluded transport times to get a ‘best case’ boundary output. Also, depending on the lifting methodology of the final truss system, the production schedule needed to be changed. Whether it was lifted from above or below, it required a different production schedule for a factory.   Schedule completion time and daily throughput (click to enlarge)    As the model developers expected, the inclusion or exclusion of process times made the most difference in the results. One of the other indications was how much of the demand could be fulfilled by one factory such as this.   The variation of 20% in process timings only resulted in slight output variations, indicating the potential acceptable range when considering equipment and process options. The exclusion of process timings provided the largest difference in outputs and highlighted the potential to investigate layouts and transport or transfer methods further.   Bryden Wood also looked at the sizing of the hold, priming, and intumescent painting areas. They found out, at an early stage, that the production schedule had an impact on the sizing of these areas, which was important for facility sizing, resource requirements, and cost.   Some area sizing and throughput figures were similar to what had been predicted by the project team. However, the simulation indicated that other areas could require additional stations or bays (e.g., painting). This highlighted areas for further investigation.   As for the future steps, an iterative review and update of input data and modeling results are planned. Additionally, Bryden Wood aims to further investigate production schedules, facility layouts, as well as reliability, downtime, and working time constraints.   The expansion of scope could be done for resource usage, raw material receipt and storage, finished truss storage, packing and dispatch, as well as the wider supply chain.   The case study was presented by David Reader, of Bryden Wood, at the AnyLogic Conference 2022.  The slides are available as a PDF.      Trust-Based Resource Sharing in Distributed Manufacturing Link:  Tags: Manufacturing, Business Processes  Problem Globalization and changes in consumer demand in distributed manufacturing have resulted in many challenges for companies. To overcome this, companies can keep extra capacity, but this may result in excess stock in quiet periods. A second option is for companies to join together to create clusters, or federations, for sharing resources.  A resource sharing mechanism between manufacturing companies needed to be created, and at the same time the trustfulness of companies in decision making was to be considered. An  agent-based simulation was identified as a solution to model the complex communication between partners to run the experiments effectively.   Solution EPIC Centre of Excellence developed a model where companies are members of a federation. They receive orders and if they do not have the necessary resources, they can send requests to the federation center/platform. Companies which have free resources, can send offers to this same center.  The federation center matches the requests with the offers and sends the different possibilities to the requester agent. The requester agent then chooses the best offer, and a contract is made between the requester and the offeror. The aim of the platform is not to find the best solution but to find good alternatives.  Trustfulness is calculated by ratings given after each contract. Trust is an internal value based on direct interactions between companies. However, trust is not the only consideration. Reputation is a public perception value influenced by all the interactions made by the company and is similar to google reviews. These two ratings are given by the requester about the offeror after each contract.    The platform-based resource sharing model    The aim of the simulation model was to provide an experimentation tool for investigating different scenarios and in this case 10 company agents were placed on the map of Hungary and the individual resource utilizations and reputation values were visualized during the model running.    The simulation model (click to enlarge)   Results The goal of the experiments was to create an effective mechanism that could be applied in real life and test the effect of realistic scenarios.  The first experiment compared the platform-based solution and direct communication between companies. The experiment considered different numbers of federation members and two different KPIs illustrated in the diagram below.   In the diagram on the left, the platform-based approach performed better by about 30% in terms of average resource utilization. In the diagram on the right, surprisingly, the direct exchange performed better in terms of service levels.   Platform vs. direct exchange-based resource sharing    The second experiment tested the effect of a negative event that affected trustfulness. In the experiment, the performance of the company did not change, but the public perception or reputation value decreased as can be seen in the diagram below and represented by C09 – the red line.  The reputation value rose back to the original level relatively quickly, but the resource utilization of that company – illustrated by the gray line chart remained low for a long time because the partners made their decision based on individual trust and public perception values. The performance and the accuracy of the company did not change and because of this after some interactions the reputation level rose. In the case of the individual trust values, the increase was much slower because of the lower number of individual interactions, and this is why the resource utilization level also increased much slower.    Effect of a negative event that affects trustfulness    In the future, there are plans to apply multi-criteria decision making and have different types of companies with different types of preferences, such as price, trustfulness or even sustainability.  Another direction for the future is to extend the company agents with a discrete-based factory model as agent-based and  discrete-event modeling  can be combined in AnyLogic.  Finally, more experiments should be run as the platform which is responsible for matching and providing reputation values for the companies has a global view in the system. This means that it can optimize, for example logistics roots and then the platform can see all the interactions, all the offers, and all the requests.  The case study was presented by Adam Szaller, EPIC Center of Excellence, at the AnyLogic 2021 Conference. The slides are available as a PDF.     Using Pedestrian Library to Create a New Look for an Art Gallery Link:  Tags: Passenger Terminals  Problem           Fair Dynamics, a Milan based consulting company with a distinct aspect towards simulation modeling, was engaged in developing The Ametria project, a contemporary art exhibition held in the Athens’ Benaki Museum.   The general purpose of the project was to steer visitors’ attention by designing brand new personal experiences and novel curatorial approaches at the exhibition. To achieve that, they created a maze-like area in the museum, instead of ordinary canvas filled rooms, to affect the visitors’ moods. The contractors were supposed to provide a layout where artworks would be placed in hidden areas, so that it would be difficult to find them. That’s where AnyLogic simulation modeling was advantageous. It let engineers see where the artworks could be placed, as well as show how people could move inside the gallery. Different layouts were tested, including:    Presence of critical areas (too high or too low density of visitors)                   Percentage of artworks that could be observed                   Pathway of each visitor inside the gallery                   Visitors’ satisfaction, measured in terms of artworks observed   Solution The movement of visitors inside the gallery was modeled with the help of the AnyLogic Pedestrian library. This library allowed modelers to simulate the behavior of agents (representing visitors), moving according to predetermined rules, and adjust distance and speed according to the crowd density. Thanks to the library, it was possible to join the advantages of pedestrian dynamics with the power of the agent-based modeling approach, providing visitors with specific features and behaviors, and allowing visitors to interact with artworks. This approach helped define:    Involvement rate, which was based on time spent per artwork and inside the gallery.                   Mood, affected by anxiety, proximity of other visitors, achievements in observing artworks, and other variables.                   Physical state of a visitor (fresh, tired, relaxed), and preferences for some artworks rather than others.   In the end, engineers were able to reproduce the behavior of a person, roaming inside an area they didn’t know, with some simple goals to be reached, but with no specific supports on how to reach them. It should be mentioned that the agents’ parameters were taken from surveys of people who visited other galleries, to make the model more accurate. The visitors’ walkthrough is described below:    Visitors entered the gallery according to a defined schedule, without knowing the artworks exhibited.                   Visitors joined a group that has not yet reached its maximum size and move along the gallery’s areas, trying to observe a greater number of artworks.                   While moving along the gallery, visitors established connections with the artworks, which are modeled as agents as well. To consider an artwork observed, visitors had to follow the rules particular to each artwork. (For example, look at the masterpiece from a particular location).                   Visitors demonstrated different involvement degrees towards the exhibition, and this affected time and effort spent in the gallery. While people walked through the artworks, they got tired, until they reached a threshold level that forced them to stop as soon as they reached a rest area.   The last step deserves greater attention. According to exhibition organizers, it was vital to consider visitors' specific features, including, preferences, goals, interest in the exhibition, mood, and others. It also implied that spending too much time without observing artworks moved a visitor to a progressive state of dissatisfaction that, at its maximum peak, forced them to exit the gallery. The contractors paid particular attention to the model’s interface. Providing a high level of usability, it had a material-design look, was easy to read, and had adjustable performance indicators.  This was achieved through the flexibility of AnyLogic in integrating the components, provided by the Java Swing class. Most of the interface elements were easily adjustable, so the clients could introduce any dynamic input parameters or empirical data into the model, including:   The opening and closing time of the exhibition   How much time the visitor should spend at each artwork or area   Percentage of visitors interested in the exhibition   After setting up and launching the model, one could see people entering the hall. When people started observing a particular artwork, represented as a dot, it changed color. Visitors tried to complete the gallery, following behavior patterns (involvement degree, mood, goals, etc.). While people were moving, the legend on the left describes what was going on in the gallery. Thanks to the density map provided by AnyLogic, it was possible to look into the people saturation of the entire area. The model also

 

Section 58 of 60
allowed enhanced statistics, including the following:   Number of people in the gallery and the distance they covered  Distribution for groups and group members  Average time a person spent in a certain area of the gallery  Number of artworks observed and visitors per each artwork  Pathways for each visitor  Modelers also took emergency situations into account, so the statistics could show total time spent on safe evacuation.   Outcome Not only did AnyLogic agent-based simulation modeling help relocate the paintings and showcase the exhibition layout, but it also provided detailed statistics for each agent/group of agents (both artworks and people inside the halls). It also determined the reasons for crowdedness in front of particular paintings, which allowed the designers to eliminate them using a density map. Apart from this, the model allowed the customer to test various layouts to find a suitable one for future exhibitions. Much was done to make the model more customer oriented. The contractors tried to improve the user experience through intuitive interface, visual indicators, and documentation tools. For instance, organizers could gather multiple pathways’ information to produce 1000 different exhibition booklets. The model also offered the possibility to test the maximum number of visitors that could concurrently visit the exhibition, avoiding overcrowding problems. This feature helped in modeling a plan for safe evacuation.  Project presentation by Luigi Manca and Roberto Grugni      Using Predictive Modeling in Healthcare for Simulation of Clinical Trials  Link:  Tags: Healthcare  Problem   Peripheral neuropathy is a condition caused by chronically high blood sugar and diabetes. It leads to weakness, numbness, and pain in hands, feet, and other body parts. About 60% of all people with diabetes eventually develop this disease. To make sustainable treatment decisions and provide personalized healthcare strategies, scientists, doctors, and insurance companies use predictive modeling tools for in silico clinical trials. With these predictive modeling tools, they can forecast how a certain patient might respond to a drug and use this information to make personalized prescriptions.            Pfizer, one of the world's largest pharmaceutical companies, asked Fair Dynamics, in collaboration with Health Services Consulting Corporation, to develop a platform that would help the company’s researchers test a new drug for patients with painful diabetic peripheral neuropathy. The platform would be based on previous clinical studies and act as a decision support tool, which could assess a patient’s personal parameters, prescribe drug dosage, and predict possible outcomes. The platform also needed to be flexible and have a user-friendly interface to allow inexperienced users to work with it. To develop this platform for predictive modeling and analytics in healthcare, engineers made use of AnyLogic simulation software.   Solution   To create a predictive analytics platform, engineers needed to process raw data from different sources and categorize it. For this purpose, they integrated SAS data files and machine learning algorithm in an AnyLogic model. The algorithm grouped the data with patient profiles into six clusters with clustering variables, such as gender, age, disease duration, and others. These parameters were essential when completing patient treatment programs.   To include patients in the predictive model, engineers used an AnyLogic agent-based modeling approach, which is commonly used for simulation in healthcare. It allowed users to set up patients with predefined parameters similar to those in the clusters. The patients would then fall into one of the identified clusters depending on these parameters.  Following categorization, the treatment process of each patient was simulated in the model with several treatment scenarios. It was based on the data from the previously clustered patient profiles. To validate the model, the 4-6 weeks treatment for each patient was simulated.   Doctors were finally presented with the optimal treatment scenario and dosage for a patient. For each patient or cluster, users could export dynamically created reports.   AnyLogic capabilities for parallel computation also offered simulation of scenarios with multiple patients using the parameter variation experiment.   As the model was supposed to be used by inexperienced people, engineers used Java technologies, supported by AnyLogic, to complete the convenient interface. Outcome   In this project, AnyLogic acted as a software tool for integrating various datasets, machine learning algorithm, and simulation capabilities. Altogether, they allowed the processing of diverse historical data and its regrouping into unique clusters. With predictive modeling using AnyLogic agent-based simulation, engineers managed to complete an easily configurable predictive healthcare model and simulate personalized treatment processes with great precision. The model helped doctors make informed decisions on drug dosage for every patient and see how he or she would respond to the treatment. With Java-based design elements, the model’s interface became more intuitive and could be easily understood by new users.  Predictive modeling in healthcare project presentation by Luigi Manca, Fair Dynamics     Using Social Studies and Simulation to forecast the Human Resources Demand  Link:  Tags: Social Processes            IBS Group, the leading software developer in Central and Eastern Europe, was approached by the Ministry of Education and Science of Russia to conduct the social study, “Development of an Analytical System for Forecasting the Demand in Skilled Professionals in Russia’s Economy.” The aim of the social study was to create an analytical system, which could help the Ministry plan for the subsidization of higher education from the federal budget.   About the social study            The key to modernizing the educational system is to improve its management in several ways. This could include forecasting the demand for skilled professionals in the economy of the country, and subsidizing students’ education in the fields where specialists are in high demand.            This social study would provide employers with qualified human resources for the positions needed. To implement this strategy, and to balance supply and demand, the government needs to monitor and forecast the labor market.           Having valid forecasts of the demand for human resources in federal subjects (regions) of the country would allow the Ministry to develop plans for subsidizing students, find imbalance between the needs and resources of the regions, evaluate various scenarios, and find ways to achieve the goals, including finding the balance between supply and demand in the labor market.   The aim of the social study    The aim of the social study was to create a unified analytical system for:  Analyzing the economic needs of Russia and its regions for skilled professionals. Assessing how the current subsidizing system corresponds with the future needs. Forecasting the optimal number of university graduates, and form subsidization plans for each of the 83 regions for various scenarios.   Solution           As a part of the social study and forecasting system development project, dynamic simulation models of the whole Russian Federation and its 83 regions were created using AnyLogic software. The reason for choosing AnyLogic simulation software for the social study was because of its user-friendliness: no need for extensive coding, a lot of libraries with pre-defined objects, and an unlimited ability to widen the functions of the software by creating custom objects.           The system of the dynamic models can be found in Figure 1.    Figure 1. Social Studies Simulations Using AnyLogic            This system helped to form the subsidization plans in 2012 and 2013, and now it is being used for further forecasting.   The results of the social study simulation            The developed system will help the Ministry to:    Plan the number of university graduates and the number of subsidized students, taking into account peculiarities of each region of the country. Align the education plans for skilled professionals with the needs of the labor market. Decrease the unemployment rate. Choose optimal scenarios for higher education modernization.  The advantages of the solution  Ability to analyze different scenarios of socio-economic development of country regions. Use of a combined approach, which takes into account both the macroeconomic processes in the whole country, and the regional processes.  Planning the number of subsidized students, taking into account their specialties.  Use of web interface for simultaneous work by many users, and the transparency of the process for the participants. Automatic calculation of the required number of subsidized university students in the country.      Using simulation modeling for efficient drone application in agriculture Link:  Tags: Transportation, Business Processes  Overview  BlueKei Solutions is an Indian consulting company that transforms businesses for technology adoption and integration, through a methodological and systematic approach. They applied system engineering in order to deploy drones for their client in the agricultural sector.  Problem  Scenarios for drone applications are various: terrain mapping, rescue missions, safety missions, rations and medical supplies, as well as agriculture. The use of drones within the agricultural industry is wide and includes:   GIS mapping Weed detection Crop health monitoring Harvest planning Storage security and monitoring Spraying pesticides and fertilizers  The customer of BlueKei Solutions was a service provider. This company wanted to know how many drones they needed to deploy for spraying a field.  The drone support system included drone carriers, operators, technicians, and charging infrastructure.  The drone application contractor required more drones than the manufacturer could provide. So, it was necessary to optimize drone operation. Solution BlueKei Solutions used agent-based simulation in AnyLogic for their needs.    The model architecture (click to enlarge)    For the correct decisions to be made, the following questions were considered: How many batteries were needed? How many drone carriers were required? What type of drone was appropriate?    Decisions to be made with AnyLogic (click to enlarge)    The engineers modeled the environment, the field, drone behavior, and its control center with AnyLogic software to test different scenarios using GIS map, statecharts, and schedules.    The model of the environment    17 fields were simulated by the model developers. They modeled agents, drone operators’ shifts, and how many times the batteries should be changed.    The statechart of the drone behavior      Engineers were tracking a lot of parameters and inputs: charging and discharging rates for a drone, average flying speed, battery life, and swapping time.  Results As a result of the simulation provided by BlueKei Solutions with AnyLogic, several insights have been identified.  On the X axis there were the number of drones. On the Y axis there were the simulation time in days (blue) and the number of swapped batteries (orange). It shows that after four drones were deployed, additional drones didn't provide further substantial effectiveness. The number of times the batteries were swapped was constant. Energy consumption was similar. However, the time it took was best around 3 days if you look at the costs to the number of days.    The simulation results      The number of days for completion of spraying fields varied depending on the number of drones. Also, the number of times that the battery in the drones should be changed, varied depending on which fields the drones operated.  The outputs of the simulation modeling showed that there was no need to invest in the number of planned drones because deploying more drones didn’t help. By using AnyLogic simulation in planning, the client could reduce the costs while not wasting money on excessive drone use.  To optimize results the following parameters must be considered for the planning of drone  deployment:   Field location  Field size  Drone capacity  Working schedule of operators  Battery capacity   These factors would directly impact on the overall performance.  The simulation modeling was successfully applied in the agricultural industry, specifically for spraying a field, and could also be used for weed detection, crop health monitoring, and geofence planning.  Watch the video about the case study presented by BlueKei Solutions at the AnyLogic Conference 2021:       Walmart’s Alphabot: Designing Material Handling System with Simulation Link:  Tags: Warehouse Operations  Problem Walmart, the world’s largest retailer by revenue, was looking for an automation technology that would help complete orders faster and at a lower cost in the company’s fast-growing online grocery business. They wanted to evaluate Alert Innovation’s Goods-to-Person (GTP) concept, Alphabot®, that could automate online grocery at the store level by using autonomous mobile robots capable of operating in all three dimensions within a multilevel storage structure. Alphabot robots, or “bots”, are self-driving vehicles that can gather items in ambient, chilled, and frozen temperature zones in a high-density storage system and bring them to associates that pick individual items to build a customer’s order. Alert Innovation presented Alphabot® as a technology that would make the in-store fulfillment of online orders faster and more efficient.  The retailer wanted to evaluate the feasibility of the Alphabot® concept, and its suitability for Walmart, prior to making a financial commitment towards product development. Alert Innovation had already made some static spreadsheet calculations for the project, however, both Alert Innovation and Walmart agreed that the spreadsheets could not be relied upon due to system complexity and variability in demand and execution. Before making a financial commitment and deploying the system in a Walmart store, it was decided to task MOSIMTEC, a simulation consulting firm, with designing a material handling simulation model for an independent technology feasibility assessment.  The goal of this initial modeling assessment was to:  Understand the throughput and turnaround time capabilities of the Alphabot® system during peak customer demand periods and assess service levels. Understand any potential performance weaknesses of the Alphabot® system with realistic demand data and variability. Identify the best configuration of the racking structure, automated bots, and each-picking workstations required for a variety of store profiles. Compare the capital expenditure requirements predicted by the model vs. the business case

 

Section 59 of 60
spreadsheets. Use model throughput and turn-around-time results to compare the proposed performance of Alert Innovation’s Alphabot® with other industry leading Automated Storage and Retrieval Systems (ASRS) and Goods-to-Person (GTP) technologies.  Material handling simulation would not only help understand the true cost of rolling out the system in the real world, but also identify store-specific requirements for potential deployment of Alphabot® across Walmart’s other stores in the future.   “Alphabot® is the first and only technology designed from the ground up to meet the daunting challenge of putting automation inside a grocery store - low cost, order completion in single-digit minutes, extreme space and throughout efficiency, and the ability to operate in multiple-temperature storage zones (ambient, chill, and frozen). And finally, unlike other technologies with lifts and conveyors, the bots are the only moving parts; if one stops, customers continue to be served. That requirement is especially important to meet the high uptime requirements of online grocery fulfillment.” – John Lert, the Founder of Alert Innovation, about the micro-fulfillment technology his team has brought to market with 31 issued or pending patents.  Solution To model Alphabot®’s behavior and operations in a computer simulated environment, with real world complexity and variability, MOSIMTEC chose AnyLogic material handling simulation software for the project.  MOSIMTEC’s and AnyLogic’s abilities to dynamically build facility layouts from data inputs, without accessing the development environment for each layout change, would help cut model development time significantly and enable faster evaluation of multiple Alphabot® configurations.  AnyLogic also offered unparallel ease of deployment, so that multiple Walmart engineers could run the material handling design model, without the need to install additional software or purchase developer’s licenses.  AnyLogic was also selected because the Alphabot® system would require extensive control algorithms.  AnyLogic’s ability to integrate with Java eliminated excessive time spent translating algorithm ideas back and forth between a propriety scripting language and a format that programmers would be comfortable with.  Stage 1: Material handling simulation model animation including AGV simulation  Walmart’s initial goal was to make a go/no-go decision about the Alphabot® project launch. In seven weeks, MOSITMEC was able to learn the system, design initial control algorithms for bot decision making, build the material handling simulation model, analyze results, and present findings to Walmart leadership. In the final delivered model, Walmart managers could specify different inputs, like number of bots, their length, width, acceleration, and speed in different areas.  Physical racking configurations, like number of aisles, levels, space between levels, and number of workstation tiers, along with other physical components of this system, were all configurable via model input parameters.  Control logic parameters, including selecting from various work assignment approaches or setting various thresholds, were also exposed and available for Walmart to run their own analysis.  Stage 2: Material handling simulation model animation including AGV simulation   Model input and output statistics were integrated in an Excel front-end so the users could easily configure and run the model. At this concept evaluation stage, MOSIMTEC incorporated basic 3D model animation that scaled based on the layout defined by the user within Excel. The output results in Excel included a summary report with key metrics, log files, scenario comparisons, charts, and graphs. After completing the independent analysis of system capabilities, MOSIMTEC transitioned the AnyLogic material handling design model to Alert Innovation for long-term use in fine-tuning software control algorithms for eventual production deployment.  Based on the outcome of Phase 1, Walmart proceeded with the Alphabot® product development.  Alert Innovation engineers used the Phase 1 model as the foundation for Phase 2 by increasing the level of detail and testing various control algorithms to simulate different system design alternatives.  Model enhancements incorporated included:   More bot-specific movement logic: bot paths, reservations, collision avoidance logic, and movement profiles, using AnyLogic agent-based modeling and Java capabilities. Logic for separate temperature zones for shelf-stable, refrigerated, and frozen items in the storage system. Inventory tracking functionality within the racks to track every item in the system. Bot sequencing logic to help simulate robots sorting items on the desk.  With the updated material handling simulation model, engineers were able to validate the original system design assumptions and provide feedback to Alphabot® product development teams.  The Alphabot system model including AGV simulation  Outcome By estimating equipment requirements needed to meet various turn-around-time thresholds, the outputs from the initial material handling design model informed the business case for deploying Alphabot® more widely. The simulation model quantified system performance capability under unconstrained demand conditions to benchmark its limits.  The model showed that Alphabot® would be able to pick 95% of online grocery orders in less than eight minutes, with an average pick time to be under five minutes. The initial model was later updated and expanded to understand the impact of various detailed design alternatives.  The model helped Alert Innovation determine which design alternatives would result in the largest ROI, along with better sizing out the system for future deployments. Walmart and Alert Innovation launched a proof-of-concept pilot of Alphabot® at a Walmart supercenter in Salem, New Hampshire in March 2019.   Watch the video of Amy Brown Greer, Dr. Christian Hammel and John Lert presenting this case study at The AnyLogic Conference, or download presentation.       Warehouse Cluster Pick Optimization Link:  Tags: Warehouse Operations   The picking methods employed in warehouses vary depending on warehouse size. Small storage facilities will often pick orders individually while large multi-zone facilities will gather orders in batches. In between, there is a gray area, where orders are complex and warehouses neither large nor small. In these cases, what are the optimal order picking methods?   DHL Supply Chain conducted research and developed an optimization tool for order picking in medium-size warehouses. Here, learn about their research, solution, and case study results.    DHL Supply Chain  is a division of Deutsche Post DHL Group with a global network and extensive logistics portfolio, including warehousing, transport, and value-added services.   Problem   Cluster picking is a method for collecting stock keeping items (SKU) for multiple orders in a single assignment. It is commonly implemented with a first come first served (FCFS) wave strategy.   The cluster pick method is used in medium size warehouses instead of picking orders individually, as practiced in small warehouses, or batch picking, which is used in large warehouses. Cluster pick is an attempt at maintaining a good level of throughput while dealing with many orders. At maximum capacity, congestion and inefficiency can become challenges.  Read more about warehouse throughput in the  DHL Supply Chain case study.   Order picking methods (click to enlarge)  The Operations Science Team at DHL Supply Chain wanted to improve on basic cluster picking by reducing congestion and improving efficiency. Their solution would be a software tool that could be deployed at medium size warehouses.   Solution   Engineers developed a simulation model of cluster picking in a warehouse. The model was used to replicate the cluster picking method and compare it against alternatives. In this way, the engineers could resolve congestion and other bottlenecks in the order picking processes.   The model accounted for warehouse shifts, the number of pickers in each shift, and gave the possibility to compare alternative waving strategies against FCFS.  Metric collection included aisle and cart congestion, cart journey completion time, and waiting times.    Cart movement simulation: orders are grouped and assigned to a cart which is then seized. The cart then spends time picking until all lines are picked and returns home before being released, ready to go again (click to enlarge)  Testing showed that the optimal cluster picking solution should focus on reducing picking cart travel distance. DHL’s engineers found that rather than have a cart visit many or all aisles in a warehouse on a single assignment, the optimal method was to arrange orders for collection in a way that would mean visiting as few aisles as possible.    Comparison of picking methods for a medium size warehouse (click to enlarge)  The DHL team further developed their order grouping algorithm beyond minimizing travel distance to include minimization of stops and to balance work across zones when applicable. The result increased the number of units processed per hour, minimized order cycle time, and reduced congestion.   To help apply cluster picking analysis and order grouping methods across the company, a micro-service plug-in was developed for DHL’s warehouse management systems. The system was called IDEA (Instantly Discover Efficient Activities).   Results   DHL Supply Chain’s Operations Science Team validated IDEA, showing an overall increase in productivity of 14% and a decrease in cart congestion of 35% when compared to FCFS. As a result of these efficiency findings, it would be possible to reduce the number of pickers by 12.5%.  The efficiency of IDEA, when compared to FCFS, resulted from improvements in picking time and cart idle time. In the test scenarios there was a 12% reduction in the time required for a cart to complete a picking assignment (cart completion time) and a 36 % reduction in the time carts were waiting for slots to become free.    Histograms showing a comparison of cart completion times and waiting times for the IDEA method versus the FCFS method (click to enlarge)  The comparison of cart congestion for IDEA and FCFS illustrates the reduction in time carts were waiting for slots to become free. Noticeably, IDEA reduces the time more than four carts are congested from 28% of the time to 18% of the time.   Charts showing a reduction in congestion when using IDEA for cluster picking order assignment versus FCFS. Carts and aisles are considered congested when there are two or more carts in an aisle (click to enlarge)  Overall, the IDEA tool is an effective way to reduce operating costs by reducing the number of pickers needed to maintain warehouse throughput. The tool is easily integrated with DHL’s warehouse management system as a plug-in and can be deployed wherever needed. It is an example of how simulation modeling with AnyLogic can deliver powerful results in a way that integrates with existing systems. Learn more about  warehouse optimization  using AnyLogic.   This case study is from a presentation given by Vijay Sharma, of DHL Supply Chain, at the  AnyLogic Conference 2021. His presentation with follow-up question and answer session:       Warehouse Simulation for Choosing Optimal Picking Algorithm Link:  Tags: Warehouse Operations  Kuehne+Nagel, a leading global provider of logistics solutions, was involved in planning a new warehouse for one of their clients. The warehouse would process 13K order lines or 750 picking cartons per day. The project included the development of the best algorithm for multi-order picking. It was planned that the orders in the warehouse would be served by workers with trolleys (or fangos). Workers with trolleys would pick the goods and put them in cartons by order. Kuehne+Nagel experts used AnyLogic simulation to choose the right algorithm for building optimal picking tours. Problem  Warehouse Trolley  Trolleys planned for usage in this warehouse (see the picture) can carry up to 8 cartons at a time, 4 of which are positioned on the trolley’s weighing scales. Weighing scales are used to increase picking accuracy by issuing an alarm signal when the weight of the picked goods does not match that in the master data.  The operator is able to fill only those cartons positioned on the scales. When a carton on the scales becomes full, he swaps it with the next empty one that he carries. So, only 4 cartons are always available for simultaneous filling. In addition, the articles for one carton can be stored at any location along the operator’s route.  These were the reasons the warehouse needed a strict algorithm for building optimal picking tours to service incoming orders. Solution The Kuehne+Nagel experts came up with the required algorithm. Their idea was that an operator’s picking route would always be straight, so that the operator would never have to return back after swapping cartons. This means that the maximum number of cartons (8) cannot always be assigned to one tour. For example, one carton can contain articles from both the first and last locations of the route, so it cannot be swapped until it is full.  The experts built an AnyLogic simulation model of the warehouse to test and validate the suggested algorithm using real historical data. The detailed model reflected the physical layout of the warehouse, articles’ places of storage, movements of trolley operators, incoming orders, trolley occupation, and service level. The operators moved and picked goods according to the suggested algorithm.    Warehouse Layout and 3D Animation The experts optimized operators’ routes by two criteria:   Maximizing average cartons quantity per tour.  Maximizing article overlap in each tour (picking the same article for multiple cartons in one tour is preferable).  The modelers uploaded an Excel file containing 260K of real order data from March 2014 to the model, and then ran the model using this as input data. Carton building (assigning different order lines to different cartons according to picking sequence) was done in the corporate warehouse management system.  The output statistics included the average number of cartons filled per tour, total duration of serving orders, total tours’ distance, average trolley utilization, and mean time of a tour. Results The statistics received from the model were then compared to the March 2014 statistics from the old warehouse. The result of the warehouse operation’s simulation showed that with the suggested layout configuration, equipment, and movement algorithm, the trolley utilization rate would rise from 58% to 94%.  These results will be used by Kuehne+Nagel to prove the investment efficiency for the client.  Also, the model will be used for choosing the right warehouse layout and article distribution among the

 

Section 60 of 60
warehouse. The developers will also vary trolley numbers to find the best balance between service level and staff workload.    Wastewater Treatment Simulation for a Major UK Utilities Provider Link:  Tags: Supply Chains   Quote reference: HM Government (2018), Our Waste, Our Resources: A Strategy for England  Problem and context: How can utility businesses improve the efficacy of their wastewater systems?       Wastewater and sewage in the UK water sector is valued at £8.7 billion and employs 42,000 people. Every day it deals with over 16 billion tons of wastewater. And, increasingly, some of the wastewater is transformed into bio-resources that can be used for energy or in other industries. Presently, deregulation and competition are increasing in the wastewater sector, creating opportunities for supply chain innovations at different stages of the water treatment cycle. Utility companies are now competing against each other to manage bio-resources, making wastewater supply chains, and the efficiency of their logistics networks a massively profitable industry.   Opportunities for supply chain and process control management in the UK water sector industry  Wastewater treatment process optimization: systems diagram with model scope in red as DES  For water utility providers, it can be difficult to develop the resource efficiency of supply chains and processes, especially when considering them in the context of industrial sustainability. Configuring and testing new designs for wastewater logistics networks that transform bio-resources into useful bi-products like energy is one option, which makes economic and environmental sense; especially as it is a key resource and a contributor to maintaining and potentially improving society’s quality of life. However, achieving resource efficiency in the wastewater treatment sector requires novel models and new ways of thinking. That is why, when a UK wastewater treatment company decided to optimize the logistics infrastructure of their facility network, they sought the expertise of decisionLab, a London consulting company that specializes in creating decision-making tools using simulation, optimization, and machine learning. decisionLab engineers needed to simulate and test the wastewater treatment supply chain in a risk-free environment, with a specific focus on novel processes, before any capital investments were made. With a wastewater treatment simulation model, they would be able to assess the utilization of different types of settlement and anaerobic digestion facilities, and optimize their quantity based on return on energy investments.    The optimized configuration would lead to:   Refined industrial ecology for the settlement, caking, and digestive processes that act as a measure of waste “upcycling loops”. Increased collaboration with neighboring water utilities based on anaerobic digestion capacity. Optimized logistics routing during the transportation of caked product, ensuring competitive advantage. A proof of operational sustainability for industry regulators and investors.  The key to delivering an effective wastewater treatment simulation model, in this case, was understanding the sustainability of the operation and ensuring that there was visibility of any returns on logistics and energy investments. Both would lead to a ‘net-positive’ impact. To determine this, the wastewater supply chain treatment simulation was used to forecast if the system was agile enough to minimize capital expenses and enable further infrastructure investment. Solution To model a network of wastewater treatment facilities, the decisionLab team applied AnyLogic simulation capabilities. This platform was a natural fit for modeling such a complex environment, with its flexible modeling tools, enabling a combination of discrete event and agent-based approaches, so the developed model could be optimized for minimum cost and maximum energy return. AnyLogic also provided excellent visualization capabilities and enabled the engineers to use GIS functionality to better display the logistics network and make the data visually compelling. The decisionLab consultants decided to focus on: understanding the sustainability of the operation, ensuring that there was a return on wastewater treatment logistics costs, and that the energy return on investment was positive. To support the client, the consultants simulated four scenarios:  ‘As-is’ logistics routing of bio-resource production Industrial ecology of settlement and caking processes with fewer centralized digestion sites Distributed digestion sites and how these impacted the cost of logistics Advanced ‘to-be’ anaerobic digestion sites vs. current ‘as-is’ technologies  Wastewater treatment simulation model showing treatment (blue), caking (yellow), and digestion (red) sites and routing  Wastewater treatment simulation results benchmarking optimal logistics routing in four different scenarios   Outcome As a result of the work, a wastewater treatment simulation model for the utility provider’s industrial ecology was developed. This model can be used to support network planning and prove various assumptions. It was specifically used to benchmark the following key performance indicators using the scenarios described above: Optimal logistic routing over a year of bio-resource production, considering seasonality The industrial ecology of settlement and caking processes at per-quarter granularity Anaerobic digestion site utilization to provide a maximum energy return on investment  The first insight was that the best-performing anaerobic digestion (AD) facilities were those with a 5-10 million-liter capacity spread out across the supply chain. This was a surprising result, as the engineering team had originally assumed, from a static analysis, that centralized larger facilities (<10 and <20-30 million-liter capacities) were more productive as they could potentially digest more, and therefore should be able to serve a higher percentage of the population. This was proven wrong using the simulation model. The second finding was that the medium size anaerobic digestion facilities were much better in terms of energy return on investment and the amount of population that the reprocessed water was serving.  Anaerobic digestion capacity optimization for system flow and population served  Energy return on investment in anaerobic digester sites in 2018 and 2019   DecisionLab’s work in AnyLogic fulfilled the client’s requirements. It allowed them to get a better understanding of their processes and of alternative approaches for optimizing their current supply chain with novel infrastructure and logistics methods that could maximize returns economically and ecologically.  Watch the video of Dr. Aanand Davé, presenting this case study at The AnyLogic Conference, or download his presentation.




Links to supporting detail:

https://anylogic.help/anylogic/introduction/anylogic-professional.html#anylogic-professional

https://anylogic.help/anylogic/ui/support-services.html#anylogic-support-services
https://anylogic.help/anylogic/ui/system-requirements.html#system-requirements

https://anylogic.help/cloud/private-cloud.html#enterprise (ONLY focus on the enterprise content + underlying included feature/benefits)

https://www.anylogic.com/use-of-simulation/

https://www.anylogic.com/features/digital-twin/


My price book details: 				
AnyLogic Professional						AnyLogic Professional				
Product	Class	Type	Part	Price		Product	Class	Term	Part	Price
AnyLogic Professional - 1 License	Software	Perpetual	7741-0001	$21,000.00 		Maintenance - Professional AnyLogic (1 License)	Maintenance Agreement	12 Months	4741-0001	$6,500.00 
AnyLogic Professional - 2 Licenses	Software	Perpetual	7741-0002	$34,000.00 		Maintenance - Professional AnyLogic (2 License)	Maintenance Agreement	12 Months	4741-0002	$10,800.00 
AnyLogic Professional - 3 Licenses	Software	Perpetual	7741-0003	$45,000.00 		Maintenance - Professional AnyLogic (3 License)	Maintenance Agreement	12 Months	4741-0003	$14,100.00 
AnyLogic Professional - 4 Licenses	Software	Perpetual	7741-0004	$54,000.00 		Maintenance - Professional AnyLogic (4 License)	Maintenance Agreement	12 Months	4741-0004	$17,200.00 
Lease - Professional + maint - 1 Seat	Software	Term-based (12 Months)	7741-0011	$9,900.00 		Maintenance Early Renewal - Professional AnyLogic License	Maintenance Agreement	12 Months	4741-0011	$4,200.00 
Lease - Professional + maint - 2 Seats	Software	Term-based (12 Months)	7741-0012	$16,200.00 		Maintenance Early Renewal - Professional AnyLogic License 2 Seats	Maintenance Agreement	12 Months	4741-0012	$7,000.00 
Lease - Professional + maint - 3 Seats	Software	Term-based (12 Months)	7741-0013	$21,000.00 		Maintenance Early Renewal - Professional AnyLogic License 3 Seats	Maintenance Agreement	12 Months	4741-0013	$9,000.00 
Lease - Professional + maint - 4 Seats	Software	Term-based (12 Months)	7741-0014	$25,000.00 		Maintenance Early Renewal - Professional AnyLogic License 4 Seats	Maintenance Agreement	12 Months	4741-0014	$10,800.00 
Lease - Professional + maint - 5 Seats	Software	Term-based (12 Months)	7741-0015	$29,000.00 		Maintenance Early Renewal - Professional AnyLogic License 5 Seats	Maintenance Agreement	12 Months	7741-0015	$5,800.00 
										
Team License						Team License				
Product	Class	Type	Part	Price		Product	Class	Term	Part	Price
Professional AnyLogic Team License initial package (1 user)	Software	Perpetual	7742-0001	$31,350.00 		Maintenance - Team License (Includes 1 Licenses or Users)	Maintenance Agreement	12 Months	4742-0001	$8,600.00 
Professional AnyLogic Team License initial package (2 users)	Software	Perpetual	7742-0002	$42,900.00 		Maintenance - Team License (Includes 2 Licenses or Users)	Maintenance Agreement	12 Months	4742-0002	$11,750.00 
Professional AnyLogic Team License initial package (3 users)	Software	Perpetual	7742-0003	$53,350.00 		Maintenance - Team License (Includes 3 Licenses or Users)	Maintenance Agreement	12 Months	4742-0003	$14,600.00 
Professional AnyLogic Team License initial package (4 users)	Software	Perpetual	7742-0004	$63,000.00 		Maintenance - Team License (Includes 4 Licenses or Users)	Maintenance Agreement	12 Months	4742-0004	$17,300.00 
Professional AnyLogic Team License initial package (1 user) LEASE	Software	Term-based (12 Months)	7742-0011	$14,500.00 		Maintenance - Team License Additional User	Maintenance Agreement	12 Months	4742-0201	$4,300.00 
Professional AnyLogic Team License initial package (2 users) LEASE	Software	Term-based (12 Months)	7742-0012	$19,800.00 		Maintenance Early Renewal - Team License (Includes 1 Licenses)	Maintenance Agreement	12 Months	4742-0011	$5,350.00 
Professional AnyLogic Team License initial package (3 users) LEASE	Software	Term-based (12 Months)	7742-0013	$24,650.00 		Maintenance Early Renewal - Team License (Includes 2 Licenses)	Maintenance Agreement	12 Months	4742-0012	$7,300.00 
Professional AnyLogic Team License initial package (4 users) LEASE	Software	Term-based (12 Months)	7742-0014	$29,000.00 		Maintenance Early Renewal - Team License (Includes 3 Licenses)	Maintenance Agreement	12 Months	4742-0013	$9,100.00 
Additional seat for AnyLogic Professional Team License	Software	Perpetual	7742-0201	$12,700.00 		Maintenance Early Renewal - Team License (Includes 4 Licenses)	Maintenance Agreement	12 Months	4742-0014	$11,300.00 
						Maintenance Early Renewal - Team License Additional User	Maintenance Agreement	12 Months	4742-0211	$2,500.00 
										
Team License (Global)						Team License (Global)				
Product	Class	Type	Part	Price		Product	Class	Term	Part	Price
Professional AnyLogic GLOBAL Team License initial package (2 users)	Software	Perpetual	7744-0002	$64,150.00 		Maintenance - GLOBAL Team License (Includes 2 Licenses or Users)	Maintenance Agreement	12 Months	4744-0002	$16,050.00 
Professional AnyLogic GLOBAL Team License initial package (3 users)	Software	Perpetual	7744-0003	$80,250.00 		Maintenance - GLOBAL Team License (Includes 3 Licenses or Users)	Maintenance Agreement	12 Months	4744-0003	$22,100.00 
Professional AnyLogic GLOBAL Team License initial package (4 users)	Software	Perpetual	7744-0004	$95,000.00 		Maintenance - GLOBAL Team License (Includes 4 Licenses or Users)	Maintenance Agreement	12 Months	4744-0004	$26,000.00 
Professional AnyLogic GLOBAL Team License initial package (5 users)	Software	Perpetual	7744-0005	$113,500.00 		Maintenance - GLOBAL Team License (Includes 5 Licenses or Users)	Maintenance Agreement	12 Months	4744-0005	$31,500.00 
Additional user for AnyLogic Professional GLOBAL Team License	Software	Perpetual	7744-0201	$19,200.00 		Maintenance - GLOBAL Team License (Additional User)	Maintenance Agreement	12 Months	4744-0006	$3,840.00 
										
AnyLogic Enterprise						AnyLogic Enterprise				
Product	Class	Type	Part	Price		Product	Class	Term	Part	Price
Professional AnyLogic Enterprise Developer License (100 Users)	Software	Perpetual	7743-2001	$250,000.00 		Maintenance - Enterprise Developer License	Maintenance Agreement	12 Months	4743-0001	$50,000.00 
AnyLogic Model Manager - Professional Edition	Software	Perpetual	7642-0001	$50,000.00 		Maintenance - AnyLogic Model Manager - Professional Edition	Maintenance Agreement	12 Months	4743-0002	$10,000.00 
AnyLogic Model Manager - Enterprise Edition	Software	Perpetual	7643-0001	$200,000.00 		Maintenance - AnyLogic Model Manager - Enterprise Edition	Maintenance Agreement	12 Months	4743-0003	$40,000.00 

Services				
Product	Class	Type	Part	Price
Training AnyLogic Fundamentals Course-GSA	Services	One-Time	341-1000-GSA	$1,257.43 
Training AnyLogic Fundamentals Course (educational customer)	Services	One-Time	3711-0001	$800.00 
Training AnyLogic Fundamentals Course (early purchase - educational customer)	Services	One-Time	3711-0101	$650.00 
Training AnyLogic Fundamentals Course (Research Customer)	Services	One-Time	3721-0001	$800.00 
Training AnyLogic Fundamentals Course (early purchase - Research Customer)	Services	One-Time	3721-0101	$650.00 
Training AnyLogic Fundamentals Course (Commercial)	Services	One-Time	3731-0001	$1,750.00 
Training AnyLogic - 3 Day - ON SITE	Services	One-Time	3731-0002	$15,000.00 
Training AnyLogic - 5 Day - ON SITE	Services	One-Time	3731-0003	$25,000.00 
Training AnyLogic Fundamentals Course (Professional license) Bundled	Services	One-Time	3731-0010	$0.00 
Training AnyLogic Fundamentals Course (early purchase - Commercial)	Services	One-Time	3731-0101	$1,600.00 
Project Based Consulting Services	Services	One-Time	3740-0000	$0.00 
Consulting and Modeling Services (Hourly Rate)	Services	One-Time	3740-0001	$250.00 
Consulting and Modeling Services - On-site (Daily Rate)	Services	One-Time	3740-0002	$2,400.00 
Enterprise Simulation Workshop	Services	One-Time	3740-0010	$25,000.00 
